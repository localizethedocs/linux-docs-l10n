# SOME DESCRIPTIVE TITLE.
# Copyright (C) The kernel development community
# This file is distributed under the same license as the The Linux Kernel package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: The Linux Kernel master\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-09-29 08:26+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../gpu/drm-compute.rst:3
msgid "Long running workloads and compute"
msgstr ""

#: ../../../gpu/drm-compute.rst:5
msgid ""
"Long running workloads (compute) are workloads that will not complete in 10 "
"seconds. (The time let the user wait before he reaches for the power "
"button). This means that other techniques need to be used to manage those "
"workloads, that cannot use fences."
msgstr ""

#: ../../../gpu/drm-compute.rst:10
msgid ""
"Some hardware may schedule compute jobs, and have no way to pre-empt them, "
"or have their memory swapped out from them. Or they simply want their "
"workload not to be preempted or swapped out at all."
msgstr ""

#: ../../../gpu/drm-compute.rst:14
msgid ""
"This means that it differs from what is described in driver-api/dma-buf.rst."
msgstr ""

#: ../../../gpu/drm-compute.rst:16
msgid ""
"As with normal compute jobs, dma-fence may not be used at all. In this case, "
"not even to force preemption. The driver with is simply forced to unmap a BO "
"from the long compute job's address space on unbind immediately, not even "
"waiting for the workload to complete. Effectively this terminates the "
"workload when there is no hardware support to recover."
msgstr ""

#: ../../../gpu/drm-compute.rst:22
msgid ""
"Since this is undesirable, there need to be mitigations to prevent a "
"workload from being terminated. There are several possible approach, all "
"with their advantages and drawbacks."
msgstr ""

#: ../../../gpu/drm-compute.rst:26
msgid ""
"The first approach you will likely try is to pin all buffers used by "
"compute. This guarantees that the job will run uninterrupted, but also "
"allows a very denial of service attack by pinning as much memory as "
"possible, hogging the all GPU memory, and possibly a huge chunk of CPU "
"memory."
msgstr ""

#: ../../../gpu/drm-compute.rst:31
msgid ""
"A second approach that will work slightly better on its own is adding an "
"option not to evict when creating a new job (any kind). If all of userspace "
"opts in to this flag, it would prevent cooperating userspace from forced "
"terminating older compute jobs to start a new one."
msgstr ""

#: ../../../gpu/drm-compute.rst:36
msgid ""
"If job preemption and recoverable pagefaults are not available, those are "
"the only approaches possible. So even with those, you want a separate way of "
"controlling resources. The standard kernel way of doing so is cgroups."
msgstr ""

#: ../../../gpu/drm-compute.rst:40
msgid ""
"This creates a third option, using cgroups to prevent eviction. Both GPU and "
"driver-allocated CPU memory would be accounted to the correct cgroup, and "
"eviction would be made cgroup aware. This allows the GPU to be partitioned "
"into cgroups, that will allow jobs to run next to each other without "
"interference."
msgstr ""

#: ../../../gpu/drm-compute.rst:46
msgid ""
"The interface to the cgroup would be similar to the current CPU memory "
"interface, with similar semantics for min/low/high/max, if eviction can be "
"made cgroup aware."
msgstr ""

#: ../../../gpu/drm-compute.rst:50
msgid ""
"What should be noted is that each memory region (tiled memory for example) "
"should have its own accounting."
msgstr ""

#: ../../../gpu/drm-compute.rst:53
msgid ""
"The key is set to the regionid set by the driver, for example \"tile0\". For "
"the value of $card, we use drmGetUnique()."
msgstr ""
