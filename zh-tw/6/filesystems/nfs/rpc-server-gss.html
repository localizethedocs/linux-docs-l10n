<!DOCTYPE html>

<html lang="zh-TW" data-content_root="../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>rpcsec_gss support for kernel RPC servers &#8212; The Linux Kernel unknown version 說明文件</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/basic.css?v=b08954a9" />
    <link rel="stylesheet" type="text/css" href="../../_static/alabaster.css?v=adfc0c0d" />
    <script src="../../_static/documentation_options.js?v=b446b479"></script>
    <script src="../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/translations.js?v=cbf116e0"></script>
    <link rel="canonical" href="https://projects.localizethedocs.org/linux-docs-l10n/filesystems/nfs/rpc-server-gss.html" />
    <link rel="index" title="索引" href="../../genindex.html" />
    <link rel="search" title="搜尋" href="../../search.html" />
    <link rel="next" title="NFSv4.1 Server Implementation" href="nfs41-server.html" />
    <link rel="prev" title="RPC Cache" href="rpc-cache.html" />

   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  

  
  

<script type="text/javascript" src="../../ltd-provenance.js"></script>
<script type="text/javascript" src="../../ltd-current.js"></script>
<script type="text/javascript" src="../../../../ltd-config.js"></script>
<script type="text/javascript" src="../../../../ltd-flyout.js"></script>

  </head><body>
  <div class="document">
    
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../../index.html">
              <img class="logo" src="../../_static/logo.svg" alt="Logo of The Linux Kernel"/>
            </a></p>
<h1 class="logo"><a href="../../index.html">The Linux Kernel</a></h1>



<p class="blurb">6.19.0</p>







<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">快速搜尋</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="前往" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>


<p>
<h3 class="kernel-toc-contents">Contents</h3>
<input type="checkbox" class="kernel-toc-toggle" id = "kernel-toc-toggle" checked>
<label class="kernel-toc-title" for="kernel-toc-toggle"></label>

<div class="kerneltoc" id="kerneltoc">
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../process/development-process.html">Development process</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../process/submitting-patches.html">提交補釘</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../process/code-of-conduct.html">行為守則</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../maintainer/index.html">維護者手冊</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../process/index.html">All development-process docs</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../core-api/index.html">核心 API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../driver-api/index.html">Driver APIs</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../subsystem-apis.html">子系統</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../subsystem-apis.html#core-subsystems">Core subsystems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../subsystem-apis.html#human-interfaces">Human interfaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../subsystem-apis.html#networking-interfaces">Networking interfaces</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../../subsystem-apis.html#storage-interfaces">Storage interfaces</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="../index.html">Linux 內核中的檔案系統</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../block/index.html">Block</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../cdrom/index.html">CD-ROM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../scsi/index.html">SCSI Subsystem</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../target/index.html">TCM 虛擬裝置</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../nvme/index.html">NVMe Subsystem</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../subsystem-apis.html#other-subsystems">其他子系統</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../locking/index.html">Locking</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../process/license-rules.html">Licensing rules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../doc-guide/index.html">撰寫文件</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dev-tools/index.html">開發工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dev-tools/testing-overview.html">測試指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../kernel-hacking/index.html">Hacking guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../trace/index.html">Tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fault-injection/index.html">Fault injection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../livepatch/index.html">即時補釘</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rust/index.html">Rust</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../admin-guide/index.html">Administration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../kbuild/index.html">建置系統</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../admin-guide/reporting-issues.html">回報議題</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tools/index.html">使用者空間工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../userspace-api/index.html">使用者空間 API</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../firmware-guide/index.html">韌體</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../devicetree/index.html">韌體與裝置樹</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../arch/index.html">CPU 架構</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../staging/index.html">未排序的文件</a></li>
</ul>

</div>

<script type="text/javascript"> <!--
  var sbar = document.getElementsByClassName("sphinxsidebar")[0];
  let currents = document.getElementsByClassName("current")
  if (currents.length) {
    sbar.scrollTop = currents[currents.length - 1].offsetTop;
  }
  --> </script>
  <div role="note" aria-label="source link">
    <h3>本頁</h3>
    <ul class="this-page-menu">
      <li><a href="../../_sources/filesystems/nfs/rpc-server-gss.rst.txt"
            rel="nofollow">顯示原始碼</a></li>
    </ul>
   </div>
        </div>
      </div>
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="rpcsec-gss-support-for-kernel-rpc-servers">
<h1>rpcsec_gss support for kernel RPC servers<a class="headerlink" href="#rpcsec-gss-support-for-kernel-rpc-servers" title="連結到這個標頭">¶</a></h1>
<p>This document gives references to the standards and protocols used to
implement RPCGSS authentication in kernel RPC servers such as the NFS
server and the NFS client's NFSv4.0 callback server.  (But note that
NFSv4.1 and higher don't require the client to act as a server for the
purposes of authentication.)</p>
<p>RPCGSS is specified in a few IETF documents:</p>
<blockquote>
<div><ul class="simple">
<li><p>RFC2203 v1: <a class="reference external" href="https://tools.ietf.org/rfc/rfc2203.txt">https://tools.ietf.org/rfc/rfc2203.txt</a></p></li>
<li><p>RFC5403 v2: <a class="reference external" href="https://tools.ietf.org/rfc/rfc5403.txt">https://tools.ietf.org/rfc/rfc5403.txt</a></p></li>
</ul>
</div></blockquote>
<p>There is a third version that we don't currently implement:</p>
<blockquote>
<div><ul class="simple">
<li><p>RFC7861 v3: <a class="reference external" href="https://tools.ietf.org/rfc/rfc7861.txt">https://tools.ietf.org/rfc/rfc7861.txt</a></p></li>
</ul>
</div></blockquote>
<section id="background">
<h2>Background<a class="headerlink" href="#background" title="連結到這個標頭">¶</a></h2>
<p>The RPCGSS Authentication method describes a way to perform GSSAPI
Authentication for NFS.  Although GSSAPI is itself completely mechanism
agnostic, in many cases only the KRB5 mechanism is supported by NFS
implementations.</p>
<p>The Linux kernel, at the moment, supports only the KRB5 mechanism, and
depends on GSSAPI extensions that are KRB5 specific.</p>
<p>GSSAPI is a complex library, and implementing it completely in kernel is
unwarranted. However GSSAPI operations are fundamentally separable in 2
parts:</p>
<ul class="simple">
<li><p>initial context establishment</p></li>
<li><p>integrity/privacy protection (signing and encrypting of individual
packets)</p></li>
</ul>
<p>The former is more complex and policy-independent, but less
performance-sensitive.  The latter is simpler and needs to be very fast.</p>
<p>Therefore, we perform per-packet integrity and privacy protection in the
kernel, but leave the initial context establishment to userspace.  We
need upcalls to request userspace to perform context establishment.</p>
</section>
<section id="nfs-server-legacy-upcall-mechanism">
<h2>NFS Server Legacy Upcall Mechanism<a class="headerlink" href="#nfs-server-legacy-upcall-mechanism" title="連結到這個標頭">¶</a></h2>
<p>The classic upcall mechanism uses a custom text based upcall mechanism
to talk to a custom daemon called rpc.svcgssd that is provide by the
nfs-utils package.</p>
<p>This upcall mechanism has 2 limitations:</p>
<ol class="upperalpha simple">
<li><p>It can handle tokens that are no bigger than 2KiB</p></li>
</ol>
<p>In some Kerberos deployment GSSAPI tokens can be quite big, up and
beyond 64KiB in size due to various authorization extensions attacked to
the Kerberos tickets, that needs to be sent through the GSS layer in
order to perform context establishment.</p>
<p>B) It does not properly handle creds where the user is member of more
than a few thousand groups (the current hard limit in the kernel is 65K
groups) due to limitation on the size of the buffer that can be send
back to the kernel (4KiB).</p>
</section>
<section id="nfs-server-new-rpc-upcall-mechanism">
<h2>NFS Server New RPC Upcall Mechanism<a class="headerlink" href="#nfs-server-new-rpc-upcall-mechanism" title="連結到這個標頭">¶</a></h2>
<p>The newer upcall mechanism uses RPC over a unix socket to a daemon
called gss-proxy, implemented by a userspace program called Gssproxy.</p>
<p>The gss_proxy RPC protocol is currently documented <a class="reference external" href="https://fedorahosted.org/gss-proxy/wiki/ProtocolDocumentation">here</a>.</p>
<p>This upcall mechanism uses the kernel rpc client and connects to the gssproxy
userspace program over a regular unix socket. The gssproxy protocol does not
suffer from the size limitations of the legacy protocol.</p>
</section>
<section id="negotiating-upcall-mechanisms">
<h2>Negotiating Upcall Mechanisms<a class="headerlink" href="#negotiating-upcall-mechanisms" title="連結到這個標頭">¶</a></h2>
<p>To provide backward compatibility, the kernel defaults to using the
legacy mechanism.  To switch to the new mechanism, gss-proxy must bind
to /var/run/gssproxy.sock and then write &quot;1&quot; to
/proc/net/rpc/use-gss-proxy.  If gss-proxy dies, it must repeat both
steps.</p>
<p>Once the upcall mechanism is chosen, it cannot be changed.  To prevent
locking into the legacy mechanisms, the above steps must be performed
before starting nfsd.  Whoever starts nfsd can guarantee this by reading
from /proc/net/rpc/use-gss-proxy and checking that it contains a
&quot;1&quot;--the read will block until gss-proxy has done its write to the file.</p>
</section>
</section>


          </div>
          
        </div>
      </div>
    <div class="clearer"></div>
  </div>
    <div class="footer">
      &#169;The kernel development community.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.2.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="../../_sources/filesystems/nfs/rpc-server-gss.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>