# SOME DESCRIPTIVE TITLE.
# Copyright (C) The kernel development community
# This file is distributed under the same license as the The Linux Kernel package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: The Linux Kernel 6\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-09-30 08:40+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../core-api/workqueue.rst:3
msgid "Workqueue"
msgstr ""

#: ../../../core-api/workqueue.rst:0
msgid "Date"
msgstr ""

#: ../../../core-api/workqueue.rst:5
msgid "September, 2010"
msgstr ""

#: ../../../core-api/workqueue.rst:0
msgid "Author"
msgstr ""

#: ../../../core-api/workqueue.rst:6
msgid "Tejun Heo <tj@kernel.org>"
msgstr ""

#: ../../../core-api/workqueue.rst:7
msgid "Florian Mickler <florian@mickler.org>"
msgstr ""

#: ../../../core-api/workqueue.rst:11
msgid "Introduction"
msgstr ""

#: ../../../core-api/workqueue.rst:13
msgid ""
"There are many cases where an asynchronous process execution context is "
"needed and the workqueue (wq) API is the most commonly used mechanism for "
"such cases."
msgstr ""

#: ../../../core-api/workqueue.rst:17
msgid ""
"When such an asynchronous execution context is needed, a work item "
"describing which function to execute is put on a queue.  An independent "
"thread serves as the asynchronous execution context.  The queue is called "
"workqueue and the thread is called worker."
msgstr ""

#: ../../../core-api/workqueue.rst:22
msgid ""
"While there are work items on the workqueue the worker executes the "
"functions associated with the work items one after the other.  When there is "
"no work item left on the workqueue the worker becomes idle. When a new work "
"item gets queued, the worker begins executing again."
msgstr ""

#: ../../../core-api/workqueue.rst:29
msgid "Why Concurrency Managed Workqueue?"
msgstr ""

#: ../../../core-api/workqueue.rst:31
msgid ""
"In the original wq implementation, a multi threaded (MT) wq had one worker "
"thread per CPU and a single threaded (ST) wq had one worker thread system-"
"wide.  A single MT wq needed to keep around the same number of workers as "
"the number of CPUs.  The kernel grew a lot of MT wq users over the years and "
"with the number of CPU cores continuously rising, some systems saturated the "
"default 32k PID space just booting up."
msgstr ""

#: ../../../core-api/workqueue.rst:39
msgid ""
"Although MT wq wasted a lot of resource, the level of concurrency provided "
"was unsatisfactory.  The limitation was common to both ST and MT wq albeit "
"less severe on MT.  Each wq maintained its own separate worker pool.  An MT "
"wq could provide only one execution context per CPU while an ST wq one for "
"the whole system.  Work items had to compete for those very limited "
"execution contexts leading to various problems including proneness to "
"deadlocks around the single execution context."
msgstr ""

#: ../../../core-api/workqueue.rst:47
msgid ""
"The tension between the provided level of concurrency and resource usage "
"also forced its users to make unnecessary tradeoffs like libata choosing to "
"use ST wq for polling PIOs and accepting an unnecessary limitation that no "
"two polling PIOs can progress at the same time.  As MT wq don't provide much "
"better concurrency, users which require higher level of concurrency, like "
"async or fscache, had to implement their own thread pool."
msgstr ""

#: ../../../core-api/workqueue.rst:55
msgid ""
"Concurrency Managed Workqueue (cmwq) is a reimplementation of wq with focus "
"on the following goals."
msgstr ""

#: ../../../core-api/workqueue.rst:58
msgid "Maintain compatibility with the original workqueue API."
msgstr ""

#: ../../../core-api/workqueue.rst:60
msgid ""
"Use per-CPU unified worker pools shared by all wq to provide flexible level "
"of concurrency on demand without wasting a lot of resource."
msgstr ""

#: ../../../core-api/workqueue.rst:64
msgid ""
"Automatically regulate worker pool and level of concurrency so that the API "
"users don't need to worry about such details."
msgstr ""

#: ../../../core-api/workqueue.rst:69
msgid "The Design"
msgstr ""

#: ../../../core-api/workqueue.rst:71
msgid ""
"In order to ease the asynchronous execution of functions a new abstraction, "
"the work item, is introduced."
msgstr ""

#: ../../../core-api/workqueue.rst:74
msgid ""
"A work item is a simple struct that holds a pointer to the function that is "
"to be executed asynchronously.  Whenever a driver or subsystem wants a "
"function to be executed asynchronously it has to set up a work item pointing "
"to that function and queue that work item on a workqueue."
msgstr ""

#: ../../../core-api/workqueue.rst:80
msgid ""
"A work item can be executed in either a thread or the BH (softirq) context."
msgstr ""

#: ../../../core-api/workqueue.rst:82
msgid ""
"For threaded workqueues, special purpose threads, called [k]workers, execute "
"the functions off of the queue, one after the other. If no work is queued, "
"the worker threads become idle. These worker threads are managed in worker-"
"pools."
msgstr ""

#: ../../../core-api/workqueue.rst:87
msgid ""
"The cmwq design differentiates between the user-facing workqueues that "
"subsystems and drivers queue work items on and the backend mechanism which "
"manages worker-pools and processes the queued work items."
msgstr ""

#: ../../../core-api/workqueue.rst:91
msgid ""
"There are two worker-pools, one for normal work items and the other for high "
"priority ones, for each possible CPU and some extra worker-pools to serve "
"work items queued on unbound workqueues - the number of these backing pools "
"is dynamic."
msgstr ""

#: ../../../core-api/workqueue.rst:96
msgid ""
"BH workqueues use the same framework. However, as there can only be one "
"concurrent execution context, there's no need to worry about concurrency. "
"Each per-CPU BH worker pool contains only one pseudo worker which represents "
"the BH execution context. A BH workqueue can be considered a convenience "
"interface to softirq."
msgstr ""

#: ../../../core-api/workqueue.rst:102
msgid ""
"Subsystems and drivers can create and queue work items through special "
"workqueue API functions as they see fit. They can influence some aspects of "
"the way the work items are executed by setting flags on the workqueue they "
"are putting the work item on. These flags include things like CPU locality, "
"concurrency limits, priority and more.  To get a detailed overview refer to "
"the API description of ``alloc_workqueue()`` below."
msgstr ""

#: ../../../core-api/workqueue.rst:110
msgid ""
"When a work item is queued to a workqueue, the target worker-pool is "
"determined according to the queue parameters and workqueue attributes and "
"appended on the shared worklist of the worker-pool.  For example, unless "
"specifically overridden, a work item of a bound workqueue will be queued on "
"the worklist of either normal or highpri worker-pool that is associated to "
"the CPU the issuer is running on."
msgstr ""

#: ../../../core-api/workqueue.rst:117
msgid ""
"For any thread pool implementation, managing the concurrency level (how many "
"execution contexts are active) is an important issue.  cmwq tries to keep "
"the concurrency at a minimal but sufficient level. Minimal to save resources "
"and sufficient in that the system is used at its full capacity."
msgstr ""

#: ../../../core-api/workqueue.rst:123
msgid ""
"Each worker-pool bound to an actual CPU implements concurrency management by "
"hooking into the scheduler.  The worker-pool is notified whenever an active "
"worker wakes up or sleeps and keeps track of the number of the currently "
"runnable workers.  Generally, work items are not expected to hog a CPU and "
"consume many cycles.  That means maintaining just enough concurrency to "
"prevent work processing from stalling should be optimal.  As long as there "
"are one or more runnable workers on the CPU, the worker-pool doesn't start "
"execution of a new work, but, when the last running worker goes to sleep, it "
"immediately schedules a new worker so that the CPU doesn't sit idle while "
"there are pending work items.  This allows using a minimal number of workers "
"without losing execution bandwidth."
msgstr ""

#: ../../../core-api/workqueue.rst:136
msgid ""
"Keeping idle workers around doesn't cost other than the memory space for "
"kthreads, so cmwq holds onto idle ones for a while before killing them."
msgstr ""

#: ../../../core-api/workqueue.rst:140
msgid ""
"For unbound workqueues, the number of backing pools is dynamic. Unbound "
"workqueue can be assigned custom attributes using "
"``apply_workqueue_attrs()`` and workqueue will automatically create backing "
"worker pools matching the attributes.  The responsibility of regulating "
"concurrency level is on the users.  There is also a flag to mark a bound wq "
"to ignore the concurrency management.  Please refer to the API section for "
"details."
msgstr ""

#: ../../../core-api/workqueue.rst:148
msgid ""
"Forward progress guarantee relies on that workers can be created when more "
"execution contexts are necessary, which in turn is guaranteed through the "
"use of rescue workers.  All work items which might be used on code paths "
"that handle memory reclaim are required to be queued on wq's that have a "
"rescue-worker reserved for execution under memory pressure.  Else it is "
"possible that the worker-pool deadlocks waiting for execution contexts to "
"free up."
msgstr ""

#: ../../../core-api/workqueue.rst:158
msgid "Application Programming Interface (API)"
msgstr ""

#: ../../../core-api/workqueue.rst:160
msgid ""
"``alloc_workqueue()`` allocates a wq.  The original ``create_*workqueue()`` "
"functions are deprecated and scheduled for removal.  ``alloc_workqueue()`` "
"takes three arguments - ``@name``, ``@flags`` and ``@max_active``.  "
"``@name`` is the name of the wq and also used as the name of the rescuer "
"thread if there is one."
msgstr ""

#: ../../../core-api/workqueue.rst:166
msgid ""
"A wq no longer manages execution resources but serves as a domain for "
"forward progress guarantee, flush and work item attributes. ``@flags`` and "
"``@max_active`` control how work items are assigned execution resources, "
"scheduled and executed."
msgstr ""

#: ../../../core-api/workqueue.rst:173 ../../../core-api/workqueue:787:
#: include/linux/workqueue.h:541 include/linux/workqueue.h:560
msgid "``flags``"
msgstr ""

#: ../../../core-api/workqueue.rst:175
msgid "``WQ_BH``"
msgstr ""

#: ../../../core-api/workqueue.rst:176
msgid ""
"BH workqueues can be considered a convenience interface to softirq. BH "
"workqueues are always per-CPU and all BH work items are executed in the "
"queueing CPU's softirq context in the queueing order."
msgstr ""

#: ../../../core-api/workqueue.rst:180
msgid ""
"All BH workqueues must have 0 ``max_active`` and ``WQ_HIGHPRI`` is the only "
"allowed additional flag."
msgstr ""

#: ../../../core-api/workqueue.rst:183
msgid ""
"BH work items cannot sleep. All other features such as delayed queueing, "
"flushing and canceling are supported."
msgstr ""

#: ../../../core-api/workqueue.rst:186
msgid "``WQ_PERCPU``"
msgstr ""

#: ../../../core-api/workqueue.rst:187
msgid ""
"Work items queued to a per-cpu wq are bound to a specific CPU. This flag is "
"the right choice when cpu locality is important."
msgstr ""

#: ../../../core-api/workqueue.rst:190
msgid "This flag is the complement of ``WQ_UNBOUND``."
msgstr ""

#: ../../../core-api/workqueue.rst:192
msgid "``WQ_UNBOUND``"
msgstr ""

#: ../../../core-api/workqueue.rst:193
msgid ""
"Work items queued to an unbound wq are served by the special worker-pools "
"which host workers which are not bound to any specific CPU.  This makes the "
"wq behave as a simple execution context provider without concurrency "
"management.  The unbound worker-pools try to start execution of work items "
"as soon as possible.  Unbound wq sacrifices locality but is useful for the "
"following cases."
msgstr ""

#: ../../../core-api/workqueue.rst:201
msgid ""
"Wide fluctuation in the concurrency level requirement is expected and using "
"bound wq may end up creating large number of mostly unused workers across "
"different CPUs as the issuer hops through different CPUs."
msgstr ""

#: ../../../core-api/workqueue.rst:206
msgid ""
"Long running CPU intensive workloads which can be better managed by the "
"system scheduler."
msgstr ""

#: ../../../core-api/workqueue.rst:209
msgid "``WQ_FREEZABLE``"
msgstr ""

#: ../../../core-api/workqueue.rst:210
msgid ""
"A freezable wq participates in the freeze phase of the system suspend "
"operations.  Work items on the wq are drained and no new work item starts "
"execution until thawed."
msgstr ""

#: ../../../core-api/workqueue.rst:214
msgid "``WQ_MEM_RECLAIM``"
msgstr ""

#: ../../../core-api/workqueue.rst:215
msgid ""
"All wq which might be used in the memory reclaim paths **MUST** have this "
"flag set.  The wq is guaranteed to have at least one execution context "
"regardless of memory pressure."
msgstr ""

#: ../../../core-api/workqueue.rst:219
msgid "``WQ_HIGHPRI``"
msgstr ""

#: ../../../core-api/workqueue.rst:220
msgid ""
"Work items of a highpri wq are queued to the highpri worker-pool of the "
"target cpu.  Highpri worker-pools are served by worker threads with elevated "
"nice level."
msgstr ""

#: ../../../core-api/workqueue.rst:224
msgid ""
"Note that normal and highpri worker-pools don't interact with each other.  "
"Each maintains its separate pool of workers and implements concurrency "
"management among its workers."
msgstr ""

#: ../../../core-api/workqueue.rst:228
msgid "``WQ_CPU_INTENSIVE``"
msgstr ""

#: ../../../core-api/workqueue.rst:229
msgid ""
"Work items of a CPU intensive wq do not contribute to the concurrency "
"level.  In other words, runnable CPU intensive work items will not prevent "
"other work items in the same worker-pool from starting execution.  This is "
"useful for bound work items which are expected to hog CPU cycles so that "
"their execution is regulated by the system scheduler."
msgstr ""

#: ../../../core-api/workqueue.rst:236
msgid ""
"Although CPU intensive work items don't contribute to the concurrency level, "
"start of their executions is still regulated by the concurrency management "
"and runnable non-CPU-intensive work items can delay execution of CPU "
"intensive work items."
msgstr ""

#: ../../../core-api/workqueue.rst:242
msgid "This flag is meaningless for unbound wq."
msgstr ""

#: ../../../core-api/workqueue.rst:246
msgid "``max_active``"
msgstr ""

#: ../../../core-api/workqueue.rst:248
msgid ""
"``@max_active`` determines the maximum number of execution contexts per CPU "
"which can be assigned to the work items of a wq. For example, with "
"``@max_active`` of 16, at most 16 work items of the wq can be executing at "
"the same time per CPU. This is always a per-CPU attribute, even for unbound "
"workqueues."
msgstr ""

#: ../../../core-api/workqueue.rst:254
msgid ""
"The maximum limit for ``@max_active`` is 2048 and the default value used "
"when 0 is specified is 1024. These values are chosen sufficiently high such "
"that they are not the limiting factor while providing protection in runaway "
"cases."
msgstr ""

#: ../../../core-api/workqueue.rst:259
msgid ""
"The number of active work items of a wq is usually regulated by the users of "
"the wq, more specifically, by how many work items the users may queue at the "
"same time.  Unless there is a specific need for throttling the number of "
"active work items, specifying '0' is recommended."
msgstr ""

#: ../../../core-api/workqueue.rst:265
msgid ""
"Some users depend on strict execution ordering where only one work item is "
"in flight at any given time and the work items are processed in queueing "
"order. While the combination of ``@max_active`` of 1 and ``WQ_UNBOUND`` used "
"to achieve this behavior, this is no longer the case. Use "
"alloc_ordered_workqueue() instead."
msgstr ""

#: ../../../core-api/workqueue.rst:273
msgid "Example Execution Scenarios"
msgstr ""

#: ../../../core-api/workqueue.rst:275
msgid ""
"The following example execution scenarios try to illustrate how cmwq behave "
"under different configurations."
msgstr ""

#: ../../../core-api/workqueue.rst:278
msgid ""
"Work items w0, w1, w2 are queued to a bound wq q0 on the same CPU. w0 burns "
"CPU for 5ms then sleeps for 10ms then burns CPU for 5ms again before "
"finishing.  w1 and w2 burn CPU for 5ms then sleep for 10ms."
msgstr ""

#: ../../../core-api/workqueue.rst:283
msgid ""
"Ignoring all other tasks, works and processing overhead, and assuming simple "
"FIFO scheduling, the following is one highly simplified version of possible "
"sequences of events with the original wq. ::"
msgstr ""

#: ../../../core-api/workqueue.rst:299
msgid "And with cmwq with ``@max_active`` >= 3, ::"
msgstr ""

#: ../../../core-api/workqueue.rst:313
msgid "If ``@max_active`` == 2, ::"
msgstr ""

#: ../../../core-api/workqueue.rst:327
msgid ""
"Now, let's assume w1 and w2 are queued to a different wq q1 which has "
"``WQ_CPU_INTENSIVE`` set, ::"
msgstr ""

#: ../../../core-api/workqueue.rst:343
msgid "Guidelines"
msgstr ""

#: ../../../core-api/workqueue.rst:345
msgid ""
"Do not forget to use ``WQ_MEM_RECLAIM`` if a wq may process work items which "
"are used during memory reclaim.  Each wq with ``WQ_MEM_RECLAIM`` set has an "
"execution context reserved for it.  If there is dependency among multiple "
"work items used during memory reclaim, they should be queued to separate wq "
"each with ``WQ_MEM_RECLAIM``."
msgstr ""

#: ../../../core-api/workqueue.rst:352
msgid "Unless strict ordering is required, there is no need to use ST wq."
msgstr ""

#: ../../../core-api/workqueue.rst:354
msgid ""
"Unless there is a specific need, using 0 for @max_active is recommended.  In "
"most use cases, concurrency level usually stays well under the default limit."
msgstr ""

#: ../../../core-api/workqueue.rst:358
msgid ""
"A wq serves as a domain for forward progress guarantee (``WQ_MEM_RECLAIM``, "
"flush and work item attributes.  Work items which are not involved in memory "
"reclaim and don't need to be flushed as a part of a group of work items, and "
"don't require any special attribute, can use one of the system wq.  There is "
"no difference in execution characteristics between using a dedicated wq and "
"a system wq."
msgstr ""

#: ../../../core-api/workqueue.rst:366
msgid ""
"Note: If something may generate more than @max_active outstanding work items "
"(do stress test your producers), it may saturate a system wq and potentially "
"lead to deadlock. It should utilize its own dedicated workqueue rather than "
"the system wq."
msgstr ""

#: ../../../core-api/workqueue.rst:371
msgid ""
"Unless work items are expected to consume a huge amount of CPU cycles, using "
"a bound wq is usually beneficial due to the increased level of locality in "
"wq operations and work item execution."
msgstr ""

#: ../../../core-api/workqueue.rst:377
msgid "Affinity Scopes"
msgstr ""

#: ../../../core-api/workqueue.rst:379
msgid ""
"An unbound workqueue groups CPUs according to its affinity scope to improve "
"cache locality. For example, if a workqueue is using the default affinity "
"scope of \"cache\", it will group CPUs according to last level cache "
"boundaries. A work item queued on the workqueue will be assigned to a worker "
"on one of the CPUs which share the last level cache with the issuing CPU. "
"Once started, the worker may or may not be allowed to move outside the scope "
"depending on the ``affinity_strict`` setting of the scope."
msgstr ""

#: ../../../core-api/workqueue.rst:387
msgid "Workqueue currently supports the following affinity scopes."
msgstr ""

#: ../../../core-api/workqueue.rst:389
msgid "``default``"
msgstr ""

#: ../../../core-api/workqueue.rst:390
msgid ""
"Use the scope in module parameter ``workqueue.default_affinity_scope`` which "
"is always set to one of the scopes below."
msgstr ""

#: ../../../core-api/workqueue.rst:393
msgid "``cpu``"
msgstr ""

#: ../../../core-api/workqueue.rst:394
msgid ""
"CPUs are not grouped. A work item issued on one CPU is processed by a worker "
"on the same CPU. This makes unbound workqueues behave as per-cpu workqueues "
"without concurrency management."
msgstr ""

#: ../../../core-api/workqueue.rst:398
msgid "``smt``"
msgstr ""

#: ../../../core-api/workqueue.rst:399
msgid ""
"CPUs are grouped according to SMT boundaries. This usually means that the "
"logical threads of each physical CPU core are grouped together."
msgstr ""

#: ../../../core-api/workqueue.rst:402
msgid "``cache``"
msgstr ""

#: ../../../core-api/workqueue.rst:403
msgid ""
"CPUs are grouped according to cache boundaries. Which specific cache "
"boundary is used is determined by the arch code. L3 is used in a lot of "
"cases. This is the default affinity scope."
msgstr ""

#: ../../../core-api/workqueue.rst:407
msgid "``numa``"
msgstr ""

#: ../../../core-api/workqueue.rst:408
msgid "CPUs are grouped according to NUMA boundaries."
msgstr ""

#: ../../../core-api/workqueue.rst:410
msgid "``system``"
msgstr ""

#: ../../../core-api/workqueue.rst:411
msgid ""
"All CPUs are put in the same group. Workqueue makes no effort to process a "
"work item on a CPU close to the issuing CPU."
msgstr ""

#: ../../../core-api/workqueue.rst:414
msgid ""
"The default affinity scope can be changed with the module parameter "
"``workqueue.default_affinity_scope`` and a specific workqueue's affinity "
"scope can be changed using ``apply_workqueue_attrs()``."
msgstr ""

#: ../../../core-api/workqueue.rst:418
msgid ""
"If ``WQ_SYSFS`` is set, the workqueue will have the following affinity scope "
"related interface files under its ``/sys/devices/virtual/workqueue/WQ_NAME/"
"`` directory."
msgstr ""

#: ../../../core-api/workqueue.rst:422
msgid "``affinity_scope``"
msgstr ""

#: ../../../core-api/workqueue.rst:423
msgid "Read to see the current affinity scope. Write to change."
msgstr ""

#: ../../../core-api/workqueue.rst:425
msgid ""
"When default is the current scope, reading this file will also show the "
"current effective scope in parentheses, for example, ``default (cache)``."
msgstr ""

#: ../../../core-api/workqueue.rst:428
msgid "``affinity_strict``"
msgstr ""

#: ../../../core-api/workqueue.rst:429
msgid ""
"0 by default indicating that affinity scopes are not strict. When a work "
"item starts execution, workqueue makes a best-effort attempt to ensure that "
"the worker is inside its affinity scope, which is called repatriation. Once "
"started, the scheduler is free to move the worker anywhere in the system as "
"it sees fit. This enables benefiting from scope locality while still being "
"able to utilize other CPUs if necessary and available."
msgstr ""

#: ../../../core-api/workqueue.rst:437
msgid ""
"If set to 1, all workers of the scope are guaranteed always to be in the "
"scope. This may be useful when crossing affinity scopes has other "
"implications, for example, in terms of power consumption or workload "
"isolation. Strict NUMA scope can also be used to match the workqueue "
"behavior of older kernels."
msgstr ""

#: ../../../core-api/workqueue.rst:445
msgid "Affinity Scopes and Performance"
msgstr ""

#: ../../../core-api/workqueue.rst:447
msgid ""
"It'd be ideal if an unbound workqueue's behavior is optimal for vast "
"majority of use cases without further tuning. Unfortunately, in the current "
"kernel, there exists a pronounced trade-off between locality and utilization "
"necessitating explicit configurations when workqueues are heavily used."
msgstr ""

#: ../../../core-api/workqueue.rst:452
msgid ""
"Higher locality leads to higher efficiency where more work is performed for "
"the same number of consumed CPU cycles. However, higher locality may also "
"cause lower overall system utilization if the work items are not spread "
"enough across the affinity scopes by the issuers. The following performance "
"testing with dm-crypt clearly illustrates this trade-off."
msgstr ""

#: ../../../core-api/workqueue.rst:458
msgid ""
"The tests are run on a CPU with 12-cores/24-threads split across four L3 "
"caches (AMD Ryzen 9 3900x). CPU clock boost is turned off for consistency. "
"``/dev/dm-0`` is a dm-crypt device created on NVME SSD (Samsung 990 PRO) and "
"opened with ``cryptsetup`` with default settings."
msgstr ""

#: ../../../core-api/workqueue.rst:465
msgid "Scenario 1: Enough issuers and work spread across the machine"
msgstr ""

#: ../../../core-api/workqueue.rst:467 ../../../core-api/workqueue.rst:509
#: ../../../core-api/workqueue.rst:552
msgid "The command used: ::"
msgstr ""

#: ../../../core-api/workqueue.rst:473
msgid ""
"There are 24 issuers, each issuing 64 IOs concurrently. ``--verify=sha512`` "
"makes ``fio`` generate and read back the content each time which makes "
"execution locality matter between the issuer and ``kcryptd``. The following "
"are the read bandwidths and CPU utilizations depending on different affinity "
"scope settings on ``kcryptd`` measured over five runs. Bandwidths are in "
"MiBps, and CPU util in percents."
msgstr ""

#: ../../../core-api/workqueue.rst:484 ../../../core-api/workqueue.rst:523
#: ../../../core-api/workqueue.rst:566
msgid "Affinity"
msgstr ""

#: ../../../core-api/workqueue.rst:485 ../../../core-api/workqueue.rst:524
#: ../../../core-api/workqueue.rst:567
msgid "Bandwidth (MiBps)"
msgstr ""

#: ../../../core-api/workqueue.rst:486 ../../../core-api/workqueue.rst:525
#: ../../../core-api/workqueue.rst:568
msgid "CPU util (%)"
msgstr ""

#: ../../../core-api/workqueue.rst:488 ../../../core-api/workqueue.rst:527
#: ../../../core-api/workqueue.rst:570
msgid "system"
msgstr ""

#: ../../../core-api/workqueue.rst:489
msgid "1159.40 ±1.34"
msgstr ""

#: ../../../core-api/workqueue.rst:490
msgid "99.31 ±0.02"
msgstr ""

#: ../../../core-api/workqueue.rst:492 ../../../core-api/workqueue.rst:531
#: ../../../core-api/workqueue.rst:574
msgid "cache"
msgstr ""

#: ../../../core-api/workqueue.rst:493
msgid "1166.40 ±0.89"
msgstr ""

#: ../../../core-api/workqueue.rst:494
msgid "99.34 ±0.01"
msgstr ""

#: ../../../core-api/workqueue.rst:496 ../../../core-api/workqueue.rst:535
#: ../../../core-api/workqueue.rst:578
msgid "cache (strict)"
msgstr ""

#: ../../../core-api/workqueue.rst:497
msgid "1166.00 ±0.71"
msgstr ""

#: ../../../core-api/workqueue.rst:498
msgid "99.35 ±0.01"
msgstr ""

#: ../../../core-api/workqueue.rst:500
msgid ""
"With enough issuers spread across the system, there is no downside to "
"\"cache\", strict or otherwise. All three configurations saturate the whole "
"machine but the cache-affine ones outperform by 0.6% thanks to improved "
"locality."
msgstr ""

#: ../../../core-api/workqueue.rst:507
msgid "Scenario 2: Fewer issuers, enough work for saturation"
msgstr ""

#: ../../../core-api/workqueue.rst:515
msgid ""
"The only difference from the previous scenario is ``--numjobs=8``. There are "
"a third of the issuers but is still enough total work to saturate the system."
msgstr ""

#: ../../../core-api/workqueue.rst:528
msgid "1155.40 ±0.89"
msgstr ""

#: ../../../core-api/workqueue.rst:529
msgid "97.41 ±0.05"
msgstr ""

#: ../../../core-api/workqueue.rst:532
msgid "1154.40 ±1.14"
msgstr ""

#: ../../../core-api/workqueue.rst:533
msgid "96.15 ±0.09"
msgstr ""

#: ../../../core-api/workqueue.rst:536
msgid "1112.00 ±4.64"
msgstr ""

#: ../../../core-api/workqueue.rst:537
msgid "93.26 ±0.35"
msgstr ""

#: ../../../core-api/workqueue.rst:539
msgid ""
"This is more than enough work to saturate the system. Both \"system\" and "
"\"cache\" are nearly saturating the machine but not fully. \"cache\" is "
"using less CPU but the better efficiency puts it at the same bandwidth as "
"\"system\"."
msgstr ""

#: ../../../core-api/workqueue.rst:544
msgid ""
"Eight issuers moving around over four L3 cache scope still allow \"cache "
"(strict)\" to mostly saturate the machine but the loss of work conservation "
"is now starting to hurt with 3.7% bandwidth loss."
msgstr ""

#: ../../../core-api/workqueue.rst:550
msgid "Scenario 3: Even fewer issuers, not enough work to saturate"
msgstr ""

#: ../../../core-api/workqueue.rst:558
msgid ""
"Again, the only difference is ``--numjobs=4``. With the number of issuers "
"reduced to four, there now isn't enough work to saturate the whole system "
"and the bandwidth becomes dependent on completion latencies."
msgstr ""

#: ../../../core-api/workqueue.rst:571
msgid "993.60 ±1.82"
msgstr ""

#: ../../../core-api/workqueue.rst:572
msgid "75.49 ±0.06"
msgstr ""

#: ../../../core-api/workqueue.rst:575
msgid "973.40 ±1.52"
msgstr ""

#: ../../../core-api/workqueue.rst:576
msgid "74.90 ±0.07"
msgstr ""

#: ../../../core-api/workqueue.rst:579
msgid "828.20 ±4.49"
msgstr ""

#: ../../../core-api/workqueue.rst:580
msgid "66.84 ±0.29"
msgstr ""

#: ../../../core-api/workqueue.rst:582
msgid ""
"Now, the tradeoff between locality and utilization is clearer. \"cache\" "
"shows 2% bandwidth loss compared to \"system\" and \"cache (struct)\" "
"whopping 20%."
msgstr ""

#: ../../../core-api/workqueue.rst:587
msgid "Conclusion and Recommendations"
msgstr ""

#: ../../../core-api/workqueue.rst:589
msgid ""
"In the above experiments, the efficiency advantage of the \"cache\" affinity "
"scope over \"system\" is, while consistent and noticeable, small. However, "
"the impact is dependent on the distances between the scopes and may be more "
"pronounced in processors with more complex topologies."
msgstr ""

#: ../../../core-api/workqueue.rst:594
msgid ""
"While the loss of work-conservation in certain scenarios hurts, it is a lot "
"better than \"cache (strict)\" and maximizing workqueue utilization is "
"unlikely to be the common case anyway. As such, \"cache\" is the default "
"affinity scope for unbound pools."
msgstr ""

#: ../../../core-api/workqueue.rst:599
msgid ""
"As there is no one option which is great for most cases, workqueue usages "
"that may consume a significant amount of CPU are recommended to configure "
"the workqueues using ``apply_workqueue_attrs()`` and/or enable ``WQ_SYSFS``."
msgstr ""

#: ../../../core-api/workqueue.rst:604
msgid ""
"An unbound workqueue with strict \"cpu\" affinity scope behaves the same as "
"``WQ_CPU_INTENSIVE`` per-cpu workqueue. There is no real advanage to the "
"latter and an unbound workqueue provides a lot more flexibility."
msgstr ""

#: ../../../core-api/workqueue.rst:608
msgid ""
"Affinity scopes are introduced in Linux v6.5. To emulate the previous "
"behavior, use strict \"numa\" affinity scope."
msgstr ""

#: ../../../core-api/workqueue.rst:611
msgid ""
"The loss of work-conservation in non-strict affinity scopes is likely "
"originating from the scheduler. There is no theoretical reason why the "
"kernel wouldn't be able to do the right thing and maintain work-conservation "
"in most cases. As such, it is possible that future scheduler improvements "
"may make most of these tunables unnecessary."
msgstr ""

#: ../../../core-api/workqueue.rst:619
msgid "Examining Configuration"
msgstr ""

#: ../../../core-api/workqueue.rst:621
msgid ""
"Use tools/workqueue/wq_dump.py to examine unbound CPU affinity "
"configuration, worker pools and how workqueues map to the pools: ::"
msgstr ""

#: ../../../core-api/workqueue.rst:692 ../../../core-api/workqueue.rst:723
msgid "See the command's help message for more info."
msgstr ""

#: ../../../core-api/workqueue.rst:696
msgid "Monitoring"
msgstr ""

#: ../../../core-api/workqueue.rst:698
msgid "Use tools/workqueue/wq_monitor.py to monitor workqueue operations: ::"
msgstr ""

#: ../../../core-api/workqueue.rst:727
msgid "Debugging"
msgstr ""

#: ../../../core-api/workqueue.rst:729
msgid ""
"Because the work functions are executed by generic worker threads there are "
"a few tricks needed to shed some light on misbehaving workqueue users."
msgstr ""

#: ../../../core-api/workqueue.rst:733
msgid "Worker threads show up in the process list as: ::"
msgstr ""

#: ../../../core-api/workqueue.rst:740
msgid ""
"If kworkers are going crazy (using too much cpu), there are two types of "
"possible problems:"
msgstr ""

#: ../../../core-api/workqueue.rst:743
msgid "Something being scheduled in rapid succession"
msgstr ""

#: ../../../core-api/workqueue.rst:744
msgid "A single work item that consumes lots of cpu cycles"
msgstr ""

#: ../../../core-api/workqueue.rst:746
msgid "The first one can be tracked using tracing: ::"
msgstr ""

#: ../../../core-api/workqueue.rst:753
msgid ""
"If something is busy looping on work queueing, it would be dominating the "
"output and the offender can be determined with the work item function."
msgstr ""

#: ../../../core-api/workqueue.rst:757
msgid ""
"For the second type of problems it should be possible to just check the "
"stack trace of the offending worker thread. ::"
msgstr ""

#: ../../../core-api/workqueue.rst:762
msgid ""
"The work item's function should be trivially visible in the stack trace."
msgstr ""

#: ../../../core-api/workqueue.rst:767
msgid "Non-reentrance Conditions"
msgstr ""

#: ../../../core-api/workqueue.rst:769
msgid ""
"Workqueue guarantees that a work item cannot be re-entrant if the following "
"conditions hold after a work item gets queued:"
msgstr ""

#: ../../../core-api/workqueue.rst:772
msgid "The work function hasn't been changed."
msgstr ""

#: ../../../core-api/workqueue.rst:773
msgid "No one queues the work item to another workqueue."
msgstr ""

#: ../../../core-api/workqueue.rst:774
msgid "The work item hasn't been reinitiated."
msgstr ""

#: ../../../core-api/workqueue.rst:776
msgid ""
"In other words, if the above conditions hold, the work item is guaranteed to "
"be executed by at most one worker system-wide at any given time."
msgstr ""

#: ../../../core-api/workqueue.rst:779
msgid ""
"Note that requeuing the work item (to the same queue) in the self function "
"doesn't break these conditions, so it's safe to do. Otherwise, caution is "
"required when breaking the conditions inside a work function."
msgstr ""

#: ../../../core-api/workqueue.rst:785
msgid "Kernel Inline Documentations Reference"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:143
msgid "A struct for workqueue attributes."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:147
msgid "**Definition**::"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:158
msgid "**Members**"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:149
msgid "``nice``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:150
msgid "nice level"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:154
msgid "``cpumask``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:155
msgid "allowed CPUs"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:157
msgid ""
"Work items in this workqueue are affine to these CPUs and not allowed to "
"execute on other CPUs. A pool serving a workqueue must have the same "
"**cpumask**."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:163
msgid "``__pod_cpumask``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:164
msgid "internal attribute used to create per-pod pools"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:166
msgid "Internal use only."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:168
msgid ""
"Per-pod unbound worker pools are used to improve locality. Always a subset "
"of ->cpumask. A workqueue can be associated with multiple worker pools with "
"disjoint **__pod_cpumask**'s. Whether the enforcement of a pool's "
"**__pod_cpumask** is strict depends on **affn_strict**."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:175
msgid "``affn_strict``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:176
msgid "affinity scope is strict"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:178
msgid ""
"If clear, workqueue will make a best-effort attempt at starting the worker "
"inside **__pod_cpumask** but the scheduler is free to migrate it outside."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:182
msgid "If set, workers are only allowed to run inside **__pod_cpumask**."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:195
msgid "``affn_scope``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:196
msgid "unbound CPU affinity scope"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:198
msgid ""
"CPU pods are used to improve execution locality of unbound work items. There "
"are multiple pod types, one for each wq_affn_scope, and every CPU in the "
"system belongs to one pod in every pod type. CPUs that belong to the same "
"pod share the worker pool. For example, selecting ``WQ_AFFN_NUMA`` makes the "
"workqueue use a separate worker pool for each NUMA node."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:207
msgid "``ordered``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:208
msgid "work items must be executed one by one in queueing order"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:211
#: include/linux/workqueue.h:482 include/linux/workqueue.h:524
#: include/linux/workqueue.h:544 include/linux/workqueue.h:562
#: include/linux/workqueue.h:648 include/linux/workqueue.h:678
#: include/linux/workqueue.h:693 include/linux/workqueue.h:707
#: include/linux/workqueue.h:718 include/linux/workqueue.h:738
#: include/linux/workqueue.h:804 include/linux/workqueue.h:818
#: ../../../core-api/workqueue:789: kernel/workqueue.c:3 kernel/workqueue.c:563
#: kernel/workqueue.c:580 kernel/workqueue.c:595 kernel/workqueue.c:707
#: kernel/workqueue.c:746 kernel/workqueue.c:872 kernel/workqueue.c:974
#: kernel/workqueue.c:996 kernel/workqueue.c:1030 kernel/workqueue.c:1064
#: kernel/workqueue.c:1085 kernel/workqueue.c:1133 kernel/workqueue.c:1171
#: kernel/workqueue.c:1234 kernel/workqueue.c:1387 kernel/workqueue.c:1420
#: kernel/workqueue.c:1466 kernel/workqueue.c:1516 kernel/workqueue.c:1548
#: kernel/workqueue.c:1573 kernel/workqueue.c:1624 kernel/workqueue.c:1638
#: kernel/workqueue.c:1657 kernel/workqueue.c:1708 kernel/workqueue.c:1783
#: kernel/workqueue.c:1807 kernel/workqueue.c:1848 kernel/workqueue.c:1928
#: kernel/workqueue.c:1979 kernel/workqueue.c:2026 kernel/workqueue.c:2143
#: kernel/workqueue.c:2172 kernel/workqueue.c:2372 kernel/workqueue.c:2403
#: kernel/workqueue.c:2434 kernel/workqueue.c:2537 kernel/workqueue.c:2575
#: kernel/workqueue.c:2669 kernel/workqueue.c:2723 kernel/workqueue.c:2767
#: kernel/workqueue.c:2868 kernel/workqueue.c:2902 kernel/workqueue.c:2940
#: kernel/workqueue.c:3036 kernel/workqueue.c:3085 kernel/workqueue.c:3126
#: kernel/workqueue.c:3300 kernel/workqueue.c:3337 kernel/workqueue.c:3423
#: kernel/workqueue.c:3694 kernel/workqueue.c:3743 kernel/workqueue.c:3819
#: kernel/workqueue.c:3934 kernel/workqueue.c:4091 kernel/workqueue.c:4255
#: kernel/workqueue.c:4273 kernel/workqueue.c:4385 kernel/workqueue.c:4409
#: kernel/workqueue.c:4431 kernel/workqueue.c:4446 kernel/workqueue.c:4464
#: kernel/workqueue.c:4483 kernel/workqueue.c:4510 kernel/workqueue.c:4523
#: kernel/workqueue.c:4536 kernel/workqueue.c:4548 kernel/workqueue.c:4587
#: kernel/workqueue.c:4611 kernel/workqueue.c:4757 kernel/workqueue.c:4920
#: kernel/workqueue.c:4999 kernel/workqueue.c:5192 kernel/workqueue.c:5380
#: kernel/workqueue.c:5407 kernel/workqueue.c:5598 kernel/workqueue.c:5839
#: kernel/workqueue.c:5938 kernel/workqueue.c:5972 kernel/workqueue.c:6030
#: kernel/workqueue.c:6069 kernel/workqueue.c:6104 kernel/workqueue.c:6127
#: kernel/workqueue.c:6561 kernel/workqueue.c:6618 kernel/workqueue.c:6754
#: kernel/workqueue.c:6921 kernel/workqueue.c:7251 kernel/workqueue.c:7356
#: kernel/workqueue.c:7423
msgid "**Description**"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:144
msgid "This can be used to change attributes of an unbound workqueue."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:354
msgid "``work_pending (work)``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:352
msgid "Find out whether a work item is currently pending"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:356
#: include/linux/workqueue.h:363 include/linux/workqueue.h:480
#: include/linux/workqueue.h:521 include/linux/workqueue.h:540
#: include/linux/workqueue.h:561 include/linux/workqueue.h:648
#: include/linux/workqueue.h:677 include/linux/workqueue.h:692
#: include/linux/workqueue.h:707 include/linux/workqueue.h:719
#: include/linux/workqueue.h:738 include/linux/workqueue.h:803
#: include/linux/workqueue.h:818 ../../../core-api/workqueue:789:
#: kernel/workqueue.c:563 kernel/workqueue.c:580 kernel/workqueue.c:595
#: kernel/workqueue.c:708 kernel/workqueue.c:747 kernel/workqueue.c:873
#: kernel/workqueue.c:974 kernel/workqueue.c:996 kernel/workqueue.c:1031
#: kernel/workqueue.c:1065 kernel/workqueue.c:1085 kernel/workqueue.c:1132
#: kernel/workqueue.c:1170 kernel/workqueue.c:1235 kernel/workqueue.c:1388
#: kernel/workqueue.c:1421 kernel/workqueue.c:1467 kernel/workqueue.c:1517
#: kernel/workqueue.c:1548 kernel/workqueue.c:1573 kernel/workqueue.c:1625
#: kernel/workqueue.c:1639 kernel/workqueue.c:1658 kernel/workqueue.c:1708
#: kernel/workqueue.c:1783 kernel/workqueue.c:1808 kernel/workqueue.c:1848
#: kernel/workqueue.c:1929 kernel/workqueue.c:1979 kernel/workqueue.c:2025
#: kernel/workqueue.c:2142 kernel/workqueue.c:2170 kernel/workqueue.c:2371
#: kernel/workqueue.c:2404 kernel/workqueue.c:2433 kernel/workqueue.c:2535
#: kernel/workqueue.c:2573 kernel/workqueue.c:2617 kernel/workqueue.c:2669
#: kernel/workqueue.c:2724 kernel/workqueue.c:2768 kernel/workqueue.c:2868
#: kernel/workqueue.c:2903 kernel/workqueue.c:2941 kernel/workqueue.c:3037
#: kernel/workqueue.c:3086 kernel/workqueue.c:3126 kernel/workqueue.c:3301
#: kernel/workqueue.c:3338 kernel/workqueue.c:3424 kernel/workqueue.c:3693
#: kernel/workqueue.c:3741 kernel/workqueue.c:3818 kernel/workqueue.c:3935
#: kernel/workqueue.c:4092 kernel/workqueue.c:4256 kernel/workqueue.c:4274
#: kernel/workqueue.c:4296 kernel/workqueue.c:4386 kernel/workqueue.c:4410
#: kernel/workqueue.c:4432 kernel/workqueue.c:4447 kernel/workqueue.c:4465
#: kernel/workqueue.c:4484 kernel/workqueue.c:4511 kernel/workqueue.c:4524
#: kernel/workqueue.c:4537 kernel/workqueue.c:4549 kernel/workqueue.c:4586
#: kernel/workqueue.c:4612 kernel/workqueue.c:4627 kernel/workqueue.c:4758
#: kernel/workqueue.c:4921 kernel/workqueue.c:5000 kernel/workqueue.c:5192
#: kernel/workqueue.c:5380 kernel/workqueue.c:5407 kernel/workqueue.c:5599
#: kernel/workqueue.c:5840 kernel/workqueue.c:5938 kernel/workqueue.c:5972
#: kernel/workqueue.c:5999 kernel/workqueue.c:6015 kernel/workqueue.c:6030
#: kernel/workqueue.c:6070 kernel/workqueue.c:6104 kernel/workqueue.c:6127
#: kernel/workqueue.c:6320 kernel/workqueue.c:6364 kernel/workqueue.c:6413
#: kernel/workqueue.c:6437 kernel/workqueue.c:6562 kernel/workqueue.c:6618
#: kernel/workqueue.c:6752 kernel/workqueue.c:6780 kernel/workqueue.c:6808
#: kernel/workqueue.c:6854 kernel/workqueue.c:6922 kernel/workqueue.c:7252
#: kernel/workqueue.c:7357 kernel/workqueue.c:7424 kernel/workqueue.c:7715
#: kernel/workqueue.c:7872 kernel/workqueue.c:7996
msgid "**Parameters**"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:358
msgid "``work``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:353
#: include/linux/workqueue.h:361
msgid "The work item in question"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:361
msgid "``delayed_work_pending (w)``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:359
msgid "Find out whether a delayable work item is currently pending"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:365
msgid "``w``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:476
msgid "allocate a workqueue"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:482
#: include/linux/workqueue.h:523 ../../../core-api/workqueue:789:
#: kernel/workqueue.c:6106
msgid "``const char *fmt``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:477
#: include/linux/workqueue.h:518 include/linux/workqueue.h:539
#: include/linux/workqueue.h:558
msgid "printf format for the name of the workqueue"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:479
#: include/linux/workqueue.h:520 ../../../core-api/workqueue:789:
#: kernel/workqueue.c:973 kernel/workqueue.c:995
msgid "``unsigned int flags``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:478
#: include/linux/workqueue.h:519
msgid "WQ_* flags"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:480
#: include/linux/workqueue.h:521 ../../../core-api/workqueue:789:
#: kernel/workqueue.c:5937
msgid "``int max_active``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:479
#: include/linux/workqueue.h:520
msgid "max in-flight work items, 0 for default"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:481
#: include/linux/workqueue.h:523 ../../../core-api/workqueue:789:
#: kernel/workqueue.c:6103
msgid "``...``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:480
#: include/linux/workqueue.h:522 include/linux/workqueue.h:542
#: include/linux/workqueue.h:560
msgid "args for **fmt**"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:481
msgid ""
"For a per-cpu workqueue, **max_active** limits the number of in-flight work "
"items for each CPU. e.g. **max_active** of 1 indicates that each CPU can be "
"executing at most one work item for the workqueue."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:485
msgid ""
"For unbound workqueues, **max_active** limits the number of in-flight work "
"items for the whole system. e.g. **max_active** of 16 indicates that there "
"can be at most 16 work items executing for the workqueue in the whole system."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:489
msgid ""
"As sharing the same active counter for an unbound workqueue across multiple "
"NUMA nodes can be expensive, **max_active** is distributed to each NUMA node "
"according to the proportion of the number of online CPUs and enforced "
"independently."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:494
msgid ""
"Depending on online CPU distribution, a node may end up with per-node "
"max_active which is significantly lower than **max_active**, which can lead "
"to deadlocks if the per-node concurrency limit is lower than the maximum "
"number of interdependent work items for the workqueue."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:499
msgid ""
"To guarantee forward progress regardless of online CPU distribution, the "
"concurrency limit on every node is guaranteed to be equal to or greater than "
"min_active which is set to min(**max_active**, ``WQ_DFL_MIN_ACTIVE``). This "
"means that the sum of per-node max_active's may be larger than "
"**max_active**."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:504
msgid ""
"For detailed information on ``WQ_*`` flags, please refer to Documentation/"
"core-api/workqueue.rst."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:507
#: include/linux/workqueue.h:527 include/linux/workqueue.h:547
#: include/linux/workqueue.h:565 ../../../core-api/workqueue:789:
#: kernel/workqueue.c:880 kernel/workqueue.c:1108 kernel/workqueue.c:1521
#: kernel/workqueue.c:2377 kernel/workqueue.c:2445 kernel/workqueue.c:2542
#: kernel/workqueue.c:2583 kernel/workqueue.c:2617 kernel/workqueue.c:2771
#: kernel/workqueue.c:3096 kernel/workqueue.c:3342 kernel/workqueue.c:3437
#: kernel/workqueue.c:3839 kernel/workqueue.c:4257 kernel/workqueue.c:4276
#: kernel/workqueue.c:4295 kernel/workqueue.c:4410 kernel/workqueue.c:4432
#: kernel/workqueue.c:4551 kernel/workqueue.c:4589 kernel/workqueue.c:4627
#: kernel/workqueue.c:4758 kernel/workqueue.c:5005 kernel/workqueue.c:5387
#: kernel/workqueue.c:5999 kernel/workqueue.c:6015 kernel/workqueue.c:6040
#: kernel/workqueue.c:6072 kernel/workqueue.c:6756 kernel/workqueue.c:6811
#: kernel/workqueue.c:7254 kernel/workqueue.c:7364
msgid "**Return**"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:508
#: include/linux/workqueue.h:528 include/linux/workqueue.h:548
#: include/linux/workqueue.h:566
msgid "Pointer to the allocated workqueue on success, ``NULL`` on failure."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:517
msgid "allocate a workqueue with user-defined lockdep_map"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:522
msgid "``struct lockdep_map *lockdep_map``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:521
#: include/linux/workqueue.h:541
msgid "user-defined lockdep_map"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:523
msgid ""
"Same as alloc_workqueue but with the a user-define lockdep_map. Useful for "
"workqueues created with the same purpose and to avoid leaking a lockdep_map "
"on each workqueue creation."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:538
msgid ""
"``alloc_ordered_workqueue_lockdep_map (fmt, flags, lockdep_map, args...)``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:536
msgid "allocate an ordered workqueue with user-defined lockdep_map"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:542
#: include/linux/workqueue.h:563
msgid "``fmt``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:540
#: include/linux/workqueue.h:559
msgid "WQ_* flags (only WQ_FREEZABLE and WQ_MEM_RECLAIM are meaningful)"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:542
msgid "``lockdep_map``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:543
#: include/linux/workqueue.h:561
msgid "``args...``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:543
msgid ""
"Same as alloc_ordered_workqueue but with the a user-define lockdep_map. "
"Useful for workqueues created with the same purpose and to avoid leaking a "
"lockdep_map on each workqueue creation."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:559
msgid "``alloc_ordered_workqueue (fmt, flags, args...)``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:557
msgid "allocate an ordered workqueue"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:561
msgid ""
"Allocate an ordered workqueue.  An ordered workqueue executes at most one "
"work item at any given time in the queued order.  They are implemented as "
"unbound workqueues with **max_active** of one."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:644
msgid "queue work on a workqueue"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:650
#: include/linux/workqueue.h:679 include/linux/workqueue.h:694
#: include/linux/workqueue.h:740 ../../../core-api/workqueue:789:
#: kernel/workqueue.c:749 kernel/workqueue.c:1550 kernel/workqueue.c:1575
#: kernel/workqueue.c:1810 kernel/workqueue.c:2370 kernel/workqueue.c:2432
#: kernel/workqueue.c:2534 kernel/workqueue.c:2572 kernel/workqueue.c:2619
#: kernel/workqueue.c:3820 kernel/workqueue.c:3937 kernel/workqueue.c:4094
#: kernel/workqueue.c:5382 kernel/workqueue.c:5409 kernel/workqueue.c:5601
#: kernel/workqueue.c:5842 kernel/workqueue.c:5940 kernel/workqueue.c:5974
#: kernel/workqueue.c:6029 kernel/workqueue.c:6322 kernel/workqueue.c:7359
#: kernel/workqueue.c:7426
msgid "``struct workqueue_struct *wq``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:645
#: include/linux/workqueue.h:674 include/linux/workqueue.h:689
#: ../../../core-api/workqueue:789: kernel/workqueue.c:2369
#: kernel/workqueue.c:2431 kernel/workqueue.c:2533 kernel/workqueue.c:2571
#: kernel/workqueue.c:2614
msgid "workqueue to use"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:647
#: include/linux/workqueue.h:706 include/linux/workqueue.h:721
#: include/linux/workqueue.h:737 ../../../core-api/workqueue:789:
#: kernel/workqueue.c:875 kernel/workqueue.c:1084 kernel/workqueue.c:1134
#: kernel/workqueue.c:1172 kernel/workqueue.c:2027 kernel/workqueue.c:2144
#: kernel/workqueue.c:2169 kernel/workqueue.c:2371 kernel/workqueue.c:2433
#: kernel/workqueue.c:2943 kernel/workqueue.c:3125 kernel/workqueue.c:4258
#: kernel/workqueue.c:4388 kernel/workqueue.c:4449 kernel/workqueue.c:4467
#: kernel/workqueue.c:4486 kernel/workqueue.c:6072
msgid "``struct work_struct *work``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:646
#: include/linux/workqueue.h:690 ../../../core-api/workqueue:789:
#: kernel/workqueue.c:2370 kernel/workqueue.c:2432 kernel/workqueue.c:2534
#: kernel/workqueue.c:2572 kernel/workqueue.c:2615
msgid "work to queue"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:647
msgid ""
"Returns ``false`` if **work** was already on a queue, ``true`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:649
msgid ""
"We queue the work to the CPU on which it was submitted, but if the CPU dies "
"it can be processed by another CPU."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:652
msgid ""
"Memory-ordering properties:  If it returns ``true``, guarantees that all "
"stores preceding the call to queue_work() in the program order will be "
"visible from the CPU which will execute **work** by the time such work "
"executes, e.g.,"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:656
msgid "{ x is initially 0 }"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:658
msgid "CPU0                               CPU1"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:660
msgid ""
"WRITE_ONCE(x, 1);                  [ **work** is being executed ] r0 = "
"queue_work(wq, work);           r1 = READ_ONCE(x);"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:663
msgid "Forbids: r0 == true && r1 == 0"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:673
msgid "queue work on a workqueue after delay"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:676
#: include/linux/workqueue.h:691 include/linux/workqueue.h:802
#: include/linux/workqueue.h:820 ../../../core-api/workqueue:789:
#: kernel/workqueue.c:2535 kernel/workqueue.c:2573 kernel/workqueue.c:4276
#: kernel/workqueue.c:4412 kernel/workqueue.c:4434 kernel/workqueue.c:4513
#: kernel/workqueue.c:4526 kernel/workqueue.c:4539
msgid "``struct delayed_work *dwork``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:675
msgid "delayable work to queue"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:677
#: include/linux/workqueue.h:692 include/linux/workqueue.h:803
#: include/linux/workqueue.h:817 ../../../core-api/workqueue:789:
#: kernel/workqueue.c:2536 kernel/workqueue.c:2574
msgid "``unsigned long delay``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:676
#: include/linux/workqueue.h:691 ../../../core-api/workqueue:789:
#: kernel/workqueue.c:2535 kernel/workqueue.c:2573
msgid "number of jiffies to wait before queueing"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:677
msgid "Equivalent to queue_delayed_work_on() but tries to use the local CPU."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:688
msgid "modify delay of or queue a delayed work"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:692
msgid "mod_delayed_work_on() on local CPU."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:703
msgid "put work task on a specific cpu"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:709
#: include/linux/workqueue.h:805 ../../../core-api/workqueue:789:
#: kernel/workqueue.c:2373 kernel/workqueue.c:2537 kernel/workqueue.c:2575
#: kernel/workqueue.c:5191 kernel/workqueue.c:5406 kernel/workqueue.c:6032
#: kernel/workqueue.c:6617 kernel/workqueue.c:6754
msgid "``int cpu``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:704
msgid "cpu to put the work task on"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:705
#: include/linux/workqueue.h:716 include/linux/workqueue.h:801
#: include/linux/workqueue.h:815
msgid "job to be done"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:706
msgid "This puts a job on a specific cpu"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:715
msgid "put work task in global workqueue"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:717
msgid ""
"Returns ``false`` if **work** was already on the kernel-global workqueue and "
"``true`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:720
msgid ""
"This puts a job in the kernel-global workqueue if it was not already queued "
"and leaves it in the same position on the kernel-global workqueue otherwise."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:724
msgid ""
"Shares the same memory-ordering properties of queue_work(), cf. the DocBook "
"header of queue_work()."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:734
msgid "Enable and queue a work item on a specific workqueue"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:735
msgid "The target workqueue"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:736
msgid "The work item to be enabled and queued"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:737
msgid ""
"This function combines the operations of enable_work() and queue_work(), "
"providing a convenient way to enable and queue a work item in a single call. "
"It invokes enable_work() on **work** and then queues it if the disable depth "
"reached 0. Returns ``true`` if the disable depth reached 0 and **work** is "
"queued, and ``false`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:743
msgid ""
"Note that **work** is always queued when disable depth reaches zero. If the "
"desired behavior is queueing only if certain events took place while "
"**work** is disabled, the user should implement the necessary state tracking "
"and perform explicit conditional queueing after enable_work()."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:799
msgid "queue work in global workqueue on CPU after delay"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:800
msgid "cpu to use"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:802
msgid "number of jiffies to wait"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:803
msgid ""
"After waiting for a given time this puts a job in the kernel-global "
"workqueue on the specified CPU."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:814
msgid "put work task in global workqueue after delay"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:816
msgid "number of jiffies to wait or 0 for immediate execution"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:817
msgid ""
"After waiting for a given time this puts a job in the kernel-global "
"workqueue."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:561
msgid "``for_each_pool (pool, pi)``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:559
msgid "iterate through all worker_pools in the system"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:565
#: kernel/workqueue.c:579
msgid "``pool``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:560
#: kernel/workqueue.c:577 kernel/workqueue.c:592
msgid "iteration cursor"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:562
msgid "``pi``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:561
msgid "integer used for iteration"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:562
msgid ""
"This must be called either with wq_pool_mutex held or RCU read locked.  If "
"the pool needs to be used beyond the locking in effect, the caller is "
"responsible for guaranteeing that the pool stays online."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:566
#: kernel/workqueue.c:581 kernel/workqueue.c:598
msgid ""
"The if/else clause exists only for the lockdep assertion and can be ignored."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:578
msgid "``for_each_pool_worker (worker, pool)``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:576
msgid "iterate through all workers of a worker_pool"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:582
msgid "``worker``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:578
msgid "worker_pool to iterate workers of"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:579
msgid "This must be called with wq_pool_attach_mutex."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:593
msgid "``for_each_pwq (pwq, wq)``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:591
msgid "iterate through all pool_workqueues of the specified workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:597
msgid "``pwq``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:594
msgid "``wq``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:593
#: kernel/workqueue.c:5377 kernel/workqueue.c:5404
msgid "the target workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:594
msgid ""
"This must be called either with wq->mutex held or RCU read locked. If the "
"pwq needs to be used beyond the locking in effect, the caller is responsible "
"for guaranteeing that the pwq stays online."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:704
msgid "allocate ID and assign it to **pool**"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:710
#: kernel/workqueue.c:1087 kernel/workqueue.c:1237 kernel/workqueue.c:2668
#: kernel/workqueue.c:2770 kernel/workqueue.c:3039 kernel/workqueue.c:4760
#: kernel/workqueue.c:4923 kernel/workqueue.c:6366 kernel/workqueue.c:6564
#: kernel/workqueue.c:6620
msgid "``struct worker_pool *pool``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:705
msgid "the pool pointer of interest"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:706
msgid ""
"Returns 0 if ID in [0, WORK_OFFQ_POOL_NONE) is allocated and assigned "
"successfully, -errno on failure."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:743
msgid "effective cpumask of an unbound workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:744
#: kernel/workqueue.c:1545
msgid "workqueue of interest"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:745
msgid ""
"**wq->unbound_attrs->cpumask** contains the cpumask requested by the user "
"which is masked with wq_unbound_cpumask to determine the effective cpumask. "
"The default pwq is always mapped to the pool with the current effective "
"cpumask."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:869
msgid "return the worker_pool a given work was associated with"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:870
msgid "the work item of interest"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:871
msgid ""
"Pools are created and destroyed under wq_pool_mutex, and allows read access "
"under RCU read lock.  As such, this function should be called under "
"wq_pool_mutex or inside of a rcu_read_lock() region."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:875
msgid ""
"All fields of the returned pool are accessible as long as the above "
"mentioned locking is in effect.  If the returned pool needs to be used "
"beyond the critical section, the caller is responsible for ensuring the "
"returned pool is and stays online."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:881
msgid "The worker_pool **work** was last associated with.  ``NULL`` if none."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:970
msgid "set worker flags and adjust nr_running accordingly"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:976
#: kernel/workqueue.c:998 kernel/workqueue.c:1033 kernel/workqueue.c:1067
#: kernel/workqueue.c:1169 kernel/workqueue.c:2671 kernel/workqueue.c:2726
#: kernel/workqueue.c:2870 kernel/workqueue.c:3088 kernel/workqueue.c:3128
#: kernel/workqueue.c:3303 kernel/workqueue.c:3742
msgid "``struct worker *worker``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:971
#: kernel/workqueue.c:993 kernel/workqueue.c:3083 kernel/workqueue.c:3123
#: kernel/workqueue.c:3298 kernel/workqueue.c:3335 kernel/workqueue.c:3421
msgid "self"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:972
msgid "flags to set"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:973
msgid "Set **flags** in **worker->flags** and adjust nr_running accordingly."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:992
msgid "clear worker flags and adjust nr_running accordingly"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:994
msgid "flags to clear"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:995
msgid "Clear **flags** in **worker->flags** and adjust nr_running accordingly."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1027
msgid "enter idle state"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1028
msgid "worker which is entering idle state"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1029
msgid ""
"**worker** is entering idle state.  Update stats and idle timer if necessary."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1032
#: kernel/workqueue.c:1065
msgid "LOCKING: raw_spin_lock_irq(pool->lock)."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1061
msgid "leave idle state"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1062
msgid "worker which is leaving idle state"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1063
msgid "**worker** is leaving idle state.  Update stats."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1081
msgid "find worker which is executing a work"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1082
#: kernel/workqueue.c:6559
msgid "pool of interest"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1083
msgid "work to find worker for"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1084
msgid ""
"Find a worker which is executing **work** on **pool** by searching **pool-"
">busy_hash** which is keyed by the address of **work**.  For a worker to "
"match, its current execution should match the address of **work** and its "
"work function.  This is to avoid unwanted dependency between unrelated work "
"executions through a work item being recycled while still being executed."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1091
msgid ""
"This is a bit tricky.  A work item may be freed once its execution starts "
"and nothing prevents the freed area from being recycled for another work "
"item.  If the same work item address ends up being reused before the "
"original execution finishes, workqueue will identify the recycled work item "
"as currently executing and make it wait until the current execution "
"finishes, introducing an unwanted dependency."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1098
msgid ""
"This function checks the work item address and work function to avoid false "
"positives.  Note that this isn't complete as one may construct a work "
"function which can introduce dependency onto itself through a recycled work "
"item.  Well, if somebody wants to shoot oneself in the foot that badly, "
"there's only so much we can do, and if such deadlock actually occurs, it "
"should be easy to locate the culprit work function."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1105
#: kernel/workqueue.c:1137 kernel/workqueue.c:1529 kernel/workqueue.c:1986
#: kernel/workqueue.c:2174 kernel/workqueue.c:2768 kernel/workqueue.c:2870
#: kernel/workqueue.c:3092 kernel/workqueue.c:3131 kernel/workqueue.c:3303
#: kernel/workqueue.c:3756 kernel/workqueue.c:3836 kernel/workqueue.c:5940
#: kernel/workqueue.c:6781 kernel/workqueue.c:6808 kernel/workqueue.c:6854
msgid "**Context**"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1106
#: kernel/workqueue.c:1138 kernel/workqueue.c:1987 kernel/workqueue.c:2175
#: kernel/workqueue.c:2871 kernel/workqueue.c:3757
msgid "raw_spin_lock_irq(pool->lock)."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1109
msgid ""
"Pointer to worker which is executing **work** if found, ``NULL`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1128
msgid "move linked works to a list"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1129
msgid "start of series of works to be scheduled"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1131
#: kernel/workqueue.c:2170
msgid "``struct list_head *head``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1130
msgid "target list to append **work** to"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1132
#: kernel/workqueue.c:1170
msgid "``struct work_struct **nextp``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1131
#: kernel/workqueue.c:1169
msgid "out parameter for nested worklist walking"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1132
msgid ""
"Schedule linked works starting from **work** to **head**. Work series to be "
"scheduled starts at **work** and includes any consecutive work with "
"WORK_STRUCT_LINKED set in its predecessor. See assign_work() for details on "
"**nextp**."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1166
msgid "assign a work item and its linked work items to a worker"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1167
msgid "work to assign"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1168
msgid "worker to assign to"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1170
msgid ""
"Assign **work** and its linked work items to **worker**. If **work** is "
"already being executed by another worker in the same pool, it'll be punted "
"there."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1173
msgid ""
"If **nextp** is not NULL, it's updated to point to the next work of the last "
"scheduled work. This allows assign_work() to be nested inside "
"list_for_each_entry_safe()."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1177
msgid ""
"Returns ``true`` if **work** was successfully assigned to **worker**. "
"``false`` if **work** was punted to another worker already executing it."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1231
msgid "wake up an idle worker if necessary"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1232
msgid "pool to kick"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1233
msgid ""
"**pool** may have pending work items. Wake up worker if necessary. Returns "
"whether a worker was woken up."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1384
msgid "a worker is running again"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1390
#: kernel/workqueue.c:1423 kernel/workqueue.c:1469 kernel/workqueue.c:1519
#: kernel/workqueue.c:6126
msgid "``struct task_struct *task``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1385
msgid "task waking up"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1386
msgid "This function is called when a worker returns from schedule()"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1417
msgid "a worker is going to sleep"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1418
msgid "task going to sleep"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1419
msgid ""
"This function is called from schedule() when a busy worker is going to sleep."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1463
msgid "a scheduler tick occurred while a kworker is running"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1464
msgid "task currently running"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1465
msgid ""
"Called from sched_tick(). We're in the IRQ context and the current worker's "
"fields which follow the 'K' locking rule can be accessed safely."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1513
msgid "retrieve worker's last work function"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1514
msgid "Task to retrieve last work function of."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1515
msgid ""
"Determine the last function a worker executed. This is called from the "
"scheduler to get a worker's last known identity."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1519
msgid ""
"This function is called during schedule() when a kworker is going to sleep. "
"It's used by psi to identify aggregation workers during dequeuing, to allow "
"periodic aggregation to shut-off when that worker is the last task in the "
"system or cgroup to go to sleep."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1524
msgid ""
"As this function doesn't involve any workqueue-related locking, it only "
"returns stable values when called from inside the scheduler's queuing and "
"dequeuing paths, when **task**, which must be a kworker, is guaranteed to "
"not be processing any works."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1519
msgid "raw_spin_lock_irq(rq->lock)"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1532
msgid ""
"The last work function ``current`` executed as a worker, NULL if it hasn't "
"executed any work yet."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1544
msgid "Determine wq_node_nr_active to use"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1547
#: kernel/workqueue.c:2406 kernel/workqueue.c:2435
msgid "``int node``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1546
msgid "NUMA node, can be ``NUMA_NO_NODE``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1547
msgid "Determine wq_node_nr_active to use for **wq** on **node**. Returns:"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1549
msgid ""
"``NULL`` for per-cpu workqueues as they don't need to use shared nr_active."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1551
msgid "node_nr_active[nr_node_ids] if **node** is ``NUMA_NO_NODE``."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1553
msgid "Otherwise, node_nr_active[**node**]."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1569
msgid "Update per-node max_actives to use"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1570
msgid "workqueue to update"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1572
msgid "``int off_cpu``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1571
msgid "CPU that's going down, -1 if a CPU is not going down"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1572
msgid ""
"Update **wq->node_nr_active**[]->max. **wq** must be unbound. max_active is "
"distributed among nodes according to the proportions of numbers of online "
"cpus. The result is always between **wq->min_active** and max_active."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1621
msgid "get an extra reference on the specified pool_workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1627
#: kernel/workqueue.c:1641 kernel/workqueue.c:1660 kernel/workqueue.c:1710
#: kernel/workqueue.c:1785 kernel/workqueue.c:1931 kernel/workqueue.c:1981
#: kernel/workqueue.c:2172 kernel/workqueue.c:3743
msgid "``struct pool_workqueue *pwq``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1622
msgid "pool_workqueue to get"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1623
msgid ""
"Obtain an extra reference on **pwq**.  The caller should guarantee that "
"**pwq** has positive refcnt and be holding the matching pool->lock."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1635
msgid "put a pool_workqueue reference"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1636
msgid "pool_workqueue to put"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1637
msgid ""
"Drop a reference of **pwq**.  If its refcnt reaches zero, schedule its "
"destruction.  The caller should be holding the matching pool->lock."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1654
msgid "put_pwq() with surrounding pool lock/unlock"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1655
msgid "pool_workqueue to put (can be ``NULL``)"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1656
msgid "put_pwq() with locking.  This function also allows ``NULL`` **pwq**."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1704
msgid "Try to increment nr_active for a pwq"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1705
#: kernel/workqueue.c:1780 kernel/workqueue.c:1926
msgid "pool_workqueue of interest"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1707
#: kernel/workqueue.c:1782
msgid "``bool fill``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1706
#: kernel/workqueue.c:1781
msgid "max_active may have increased, try to increase concurrency level"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1707
msgid ""
"Try to increment nr_active for **pwq**. Returns ``true`` if an nr_active "
"count is successfully obtained. ``false`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1779
msgid "Activate the first inactive work item on a pwq"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1782
msgid ""
"Activate the first inactive work item of **pwq** if available and allowed by "
"max_active limit."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1785
msgid ""
"Returns ``true`` if an inactive work item has been activated. ``false`` if "
"no inactive work item is found or max_active limit is reached."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1804
msgid "unplug the oldest pool_workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1805
msgid "workqueue_struct where its oldest pwq is to be unplugged"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1806
msgid ""
"This function should only be called for ordered workqueues where only the "
"oldest pwq is unplugged, the others are plugged to suspend execution to "
"ensure proper work item ordering::"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1819
msgid ""
"When the oldest pwq is drained and removed, this function should be called "
"to unplug the next oldest one to start its work item execution. Note that "
"pwq's are linked into wq->pwqs with the oldest first, so the first one in "
"the list is the oldest."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1844
msgid "Activate a pending pwq on a wq_node_nr_active"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1850
msgid "``struct wq_node_nr_active *nna``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1845
msgid "wq_node_nr_active to activate a pending pwq for"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1847
msgid "``struct worker_pool *caller_pool``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1846
msgid "worker_pool the caller is locking"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1847
msgid ""
"Activate a pwq in **nna->pending_pwqs**. Called with **caller_pool** locked. "
"**caller_pool** may be unlocked and relocked to lock other worker_pools."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1925
msgid "Retire an active count"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1927
msgid ""
"Decrement **pwq**'s nr_active and try to activate the first inactive work "
"item. For unbound workqueues, this function may temporarily drop **pwq->pool-"
">lock**."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1975
msgid "decrement pwq's nr_in_flight"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1976
msgid "pwq of interest"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1978
msgid "``unsigned long work_data``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1977
msgid "work_data of work which left the queue"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1978
msgid ""
"A work either has completed or is removed from pending queue, decrement "
"nr_in_flight of its pwq and handle workqueue flushing."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1981
msgid "**NOTE**"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1982
msgid ""
"For unbound workqueues, this function may temporarily drop **pwq->pool-"
">lock** and thus should be called after all other state updates for the in-"
"flight work item is complete."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2021
#: kernel/workqueue.c:2138
msgid "steal work item from worklist and disable irq"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2022
#: kernel/workqueue.c:2139
msgid "work item to steal"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2024
#: kernel/workqueue.c:2141
msgid "``u32 cflags``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2023
#: kernel/workqueue.c:2140
msgid "``WORK_CANCEL_`` flags"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2025
#: kernel/workqueue.c:2142
msgid "``unsigned long *irq_flags``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2024
msgid "place to store irq state"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2025
msgid ""
"Try to grab PENDING bit of **work**.  This function can handle **work** in "
"any stable state - idle, on timer or on worklist."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2030
msgid "1"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2030
msgid "if **work** was pending and we successfully stole PENDING"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2031
#: kernel/workqueue.c:3343 kernel/workqueue.c:3438
msgid "0"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2031
msgid "if **work** was idle and we claimed PENDING"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2032
msgid "-EAGAIN"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2032
msgid "if PENDING couldn't be grabbed at the moment, safe to busy-retry"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2035
#: kernel/workqueue.c:4414
msgid "**Note**"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2037
msgid ""
"On >= 0 return, the caller owns **work**'s PENDING bit.  To avoid getting "
"interrupted while holding PENDING and **work** off queue, irq must be "
"disabled on entry.  This, combined with delayed_work->timer being irqsafe, "
"ensures that we return -EAGAIN for finite short period of time."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2042
msgid ""
"On successful return, >= 0, irq is disabled and the caller is responsible "
"for releasing it using local_irq_restore(***irq_flags**)."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2045
#: kernel/workqueue.c:4418
msgid "This function is safe to call from any context including IRQ handler."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2141
msgid "place to store IRQ state"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2142
msgid ""
"Grab PENDING bit of **work**. **work** can be in any stable state - idle, on "
"timer or on worklist."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2145
msgid ""
"Can be called from any context. IRQ is disabled on return with IRQ state "
"stored in ***irq_flags**. The caller is responsible for re-enabling it using "
"local_irq_restore()."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2149
msgid "Returns ``true`` if **work** was pending. ``false`` if idle."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2166
msgid "insert a work into a pool"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2167
msgid "pwq **work** belongs to"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2168
msgid "work to insert"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2169
msgid "insertion point"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2171
msgid "``unsigned int extra_flags``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2170
msgid "extra WORK_STRUCT_* flags to set"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2171
msgid ""
"Insert **work** which belongs to **pwq** after **head**.  **extra_flags** is "
"or'd to work_struct flags."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2367
msgid "queue work on specific cpu"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2368
#: kernel/workqueue.c:2532 kernel/workqueue.c:2570
msgid "CPU number to execute work on"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2371
msgid ""
"We queue the work to a specific CPU, the caller must ensure it can't go "
"away.  Callers that fail to ensure that the specified CPU cannot go away "
"will execute on a randomly chosen CPU. But note well that callers specifying "
"a CPU that never has been online will get a splat."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2378
#: kernel/workqueue.c:2446
msgid "``false`` if **work** was already on a queue, ``true`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2400
msgid "Select a CPU based on NUMA node"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2401
msgid "NUMA node ID that we want to select a CPU from"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2402
msgid ""
"This function will attempt to find a \"random\" cpu available on a given "
"node. If there are no CPUs available on the given node it will return "
"WORK_CPU_UNBOUND indicating that we should just schedule to any available "
"CPU if we need to schedule this work."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2429
msgid "queue work on a \"random\" cpu for a given NUMA node"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2430
msgid "NUMA node that we are targeting the work for"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2433
msgid ""
"We queue the work to a \"random\" CPU within a given NUMA node. The basic "
"idea here is to provide a way to somehow associate work with a given NUMA "
"node."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2437
msgid ""
"This function will only make a best effort attempt at getting this onto the "
"right NUMA node. If no node is requested or the requested node is offline "
"then we just fall back to standard queue_work behavior."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2441
msgid ""
"Currently the \"random\" CPU ends up being the first available CPU in the "
"intersection of cpu_online_mask and the cpumask of the node, unless we are "
"running on the node. In that case we just use the current CPU."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2531
msgid "queue work on specific CPU after delay"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2536
msgid ""
"We queue the delayed_work to a specific CPU, for non-zero delays the caller "
"must ensure it is online and can't go away. Callers that fail to ensure "
"this, may get **dwork->timer** queued to an offlined CPU and this will "
"prevent queueing of **dwork->work** unless the offlined CPU becomes online "
"again."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2543
msgid ""
"``false`` if **work** was already on a queue, ``true`` otherwise.  If "
"**delay** is zero and **dwork** is idle, it will be scheduled for immediate "
"execution."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2569
msgid "modify delay of or queue a delayed work on specific CPU"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2574
msgid ""
"If **dwork** is idle, equivalent to queue_delayed_work_on(); otherwise, "
"modify **dwork**'s timer so that it expires after **delay**.  If **delay** "
"is zero, **work** is guaranteed to be scheduled immediately regardless of "
"its current state."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2580
msgid ""
"This function is safe to call from any context including IRQ handler. See "
"try_to_grab_pending() for details."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2580
msgid ""
"``false`` if **dwork** was idle and queued, ``true`` if **dwork** was "
"pending and its timer was modified."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2613
msgid "queue work after a RCU grace period"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2616
#: kernel/workqueue.c:4298
msgid "``struct rcu_work *rwork``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2617
msgid ""
"``false`` if **rwork** was already pending, ``true`` otherwise.  Note that a "
"full RCU grace period is guaranteed only after a ``true`` return. While "
"**rwork** is guaranteed to be executed after a ``false`` return, the "
"execution may happen before a full RCU grace period has passed."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2665
msgid "attach a worker to a pool"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2666
msgid "worker to be attached"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2667
msgid "the target pool"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2668
msgid ""
"Attach **worker** to **pool**.  Once attached, the ``WORKER_UNBOUND`` flag "
"and cpu-binding of **worker** are kept coordinated with the pool across cpu-"
"[un]hotplugs."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2720
msgid "detach a worker from its pool"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2721
msgid "worker which is attached to its pool"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2722
msgid ""
"Undo the attaching which had been done in worker_attach_to_pool().  The "
"caller worker shouldn't access to the pool after detached except it has "
"other reference to the pool."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2764
msgid "create a new workqueue worker"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2765
msgid "pool the new worker will belong to"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2766
msgid "Create and start a new worker which is attached to **pool**."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2769
msgid "Might sleep.  Does GFP_KERNEL allocations."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2772
msgid "Pointer to the newly created worker."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2864
msgid "Tag a worker for destruction"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2865
msgid "worker to be destroyed"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2867
msgid "``struct list_head *list``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2866
msgid "transfer worker away from its pool->idle_list and into list"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2867
msgid ""
"Tag **worker** for destruction and adjust **pool** stats accordingly.  The "
"worker should be idle."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2899
msgid "check if some idle workers can now be deleted."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2905
msgid "``struct timer_list *t``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2900
msgid "The pool's idle_timer that just expired"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2901
msgid ""
"The timer is armed in worker_enter_idle(). Note that it isn't disarmed in "
"worker_leave_idle(), as a worker flicking between idle and active while its "
"pool is at the too_many_workers() tipping point would cause too much timer "
"housekeeping overhead. Since IDLE_WORKER_TIMEOUT is long enough, we just let "
"it expire and re-evaluate things from there."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2937
msgid "cull workers that have been idle for too long."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2938
msgid "the pool's work for handling these idle workers"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2939
msgid ""
"This goes through a pool's idle workers and gets rid of those that have been "
"idle for at least IDLE_WORKER_TIMEOUT seconds."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2942
msgid ""
"We don't want to disturb isolated CPUs because of a pcpu kworker being "
"culled, so this also resets worker affinity. This requires a sleepable "
"context, hence the split between timer callback and work item."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3033
msgid "create a new worker if necessary"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3034
msgid "pool to create a new worker for"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3035
msgid ""
"Create a new worker for **pool** if necessary.  **pool** is guaranteed to "
"have at least one idle worker on return from this function.  If creating a "
"new worker takes longer than MAYDAY_INTERVAL, mayday is sent to all rescuers "
"with works scheduled on **pool** to resolve possible allocation deadlock."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3041
msgid ""
"On return, need_to_create_worker() is guaranteed to be ``false`` and "
"may_start_working() ``true``."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3044
msgid ""
"LOCKING: raw_spin_lock_irq(pool->lock) which may be released and regrabbed "
"multiple times.  Does GFP_KERNEL allocations.  Called only from manager."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3082
msgid "manage worker pool"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3084
msgid ""
"Assume the manager role and manage the worker pool **worker** belongs to.  "
"At any given time, there can be only zero or one manager per pool.  The "
"exclusion is handled automatically by this function."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3088
msgid ""
"The caller can safely start processing works on false return.  On true "
"return, it's guaranteed that need_to_create_worker() is false and "
"may_start_working() is true."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3093
msgid ""
"raw_spin_lock_irq(pool->lock) which may be released and regrabbed multiple "
"times.  Does GFP_KERNEL allocations."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3097
msgid ""
"``false`` if the pool doesn't need management and the caller can safely "
"start processing works, ``true`` if management function was performed and "
"the conditions that the caller verified before calling the function may no "
"longer be true."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3122
msgid "process single work"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3124
msgid "work to process"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3125
msgid ""
"Process **work**.  This function contains all the logics necessary to "
"process a single work including synchronization against and interaction with "
"other workers on the same cpu, queueing and flushing.  As long as context "
"requirement is met, any worker can call this function to process a work."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3132
msgid "raw_spin_lock_irq(pool->lock) which is released and regrabbed."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3297
msgid "process scheduled works"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3299
msgid ""
"Process all scheduled works.  Please note that the scheduled list may change "
"while processing a work, so this function repeatedly fetches a work from the "
"top and executes it."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3304
msgid ""
"raw_spin_lock_irq(pool->lock) which may be released and regrabbed multiple "
"times."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3334
msgid "the worker thread function"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3340
msgid "``void *__worker``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3336
msgid ""
"The worker thread function.  All workers belong to a worker_pool - either a "
"per-cpu one or dynamic unbound one.  These workers process all work items "
"regardless of their specific target workqueue.  The only exception is work "
"items which belong to workqueues with a rescuer which will be explained in "
"rescuer_thread()."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3420
msgid "the rescuer thread function"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3426
msgid "``void *__rescuer``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3422
msgid ""
"Workqueue rescuer thread function.  There's one rescuer for each workqueue "
"which has WQ_MEM_RECLAIM set."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3425
msgid ""
"Regular work processing on a pool may block trying to create a new worker "
"which uses GFP_KERNEL allocation which has slight chance of developing into "
"deadlock if some works currently on the same queue need to be processed to "
"satisfy the GFP_KERNEL allocation.  This is the problem rescuer solves."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3431
msgid ""
"When such condition is possible, the pool summons rescuers of all workqueues "
"which have works queued on the pool and let them process those works so that "
"forward progress can be guaranteed."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3435
msgid "This should happen rarely."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3689
msgid "check for flush dependency sanity"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3695
msgid "``struct workqueue_struct *target_wq``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3690
#: kernel/workqueue.c:3815
msgid "workqueue being flushed"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3692
msgid "``struct work_struct *target_work``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3691
msgid "work item being flushed (NULL for workqueue flushes)"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3693
msgid "``bool from_cancel``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3692
msgid "are we called from the work cancel path"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3693
msgid ""
"``current`` is trying to flush the whole **target_wq** or **target_work** on "
"it. If this is not the cancel path (which implies work being flushed is "
"either already running, or will not be at all), check if **target_wq** "
"doesn't have ``WQ_MEM_RECLAIM`` and verify that ``current`` is not "
"reclaiming memory or running on a workqueue which doesn't have "
"``WQ_MEM_RECLAIM`` as that can break forward- progress guarantee leading to "
"a deadlock."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3737
msgid "insert a barrier work"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3738
msgid "pwq to insert barrier into"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3740
msgid "``struct wq_barrier *barr``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3739
msgid "wq_barrier to insert"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3741
msgid "``struct work_struct *target``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3740
msgid "target work to attach **barr** to"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3741
msgid ""
"worker currently executing **target**, NULL if **target** is not executing"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3742
msgid ""
"**barr** is linked to **target** such that **barr** is completed only after "
"**target** finishes execution.  Please note that the ordering guarantee is "
"observed only with respect to **target** and on the local cpu."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3747
msgid ""
"Currently, a queued barrier can't be canceled.  This is because "
"try_to_grab_pending() can't determine whether the work to be grabbed is at "
"the head of the queue and thus can't clear LINKED flag of the previous work "
"while there must be a valid next work after a work with LINKED flag set."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3753
msgid ""
"Note that when **worker** is non-NULL, **target** may be modified underneath "
"us, so we can't reliably determine pwq from **target**."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3814
msgid "prepare pwqs for workqueue flushing"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3817
msgid "``int flush_color``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3816
msgid "new flush color, < 0 for no-op"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3818
msgid "``int work_color``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3817
msgid "new work color, < 0 for no-op"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3818
msgid "Prepare pwqs for workqueue flushing."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3820
msgid ""
"If **flush_color** is non-negative, flush_color on all pwqs should be -1.  "
"If no pwq has in-flight commands at the specified color, all pwq-"
">flush_color's stay at -1 and ``false`` is returned.  If any pwq has in "
"flight commands, its pwq->flush_color is set to **flush_color**, **wq-"
">nr_pwqs_to_flush** is updated accordingly, pwq wakeup logic is armed and "
"``true`` is returned."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3827
msgid ""
"The caller should have initialized **wq->first_flusher** prior to calling "
"this function with non-negative **flush_color**.  If **flush_color** is "
"negative, no flush color update is done and ``false`` is returned."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3832
msgid ""
"If **work_color** is non-negative, all pwqs should have the same work_color "
"which is previous to **work_color** and all will be advanced to "
"**work_color**."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3837
msgid "mutex_lock(wq->mutex)."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3840
msgid ""
"``true`` if **flush_color** >= 0 and there's something to flush.  ``false`` "
"otherwise."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3931
msgid "ensure that any scheduled work has run to completion."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3932
msgid "workqueue to flush"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3933
msgid ""
"This function sleeps until all work items which were queued on entry have "
"finished execution, but it is not livelocked by new incoming ones."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4088
msgid "drain a workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4089
msgid "workqueue to drain"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4090
msgid ""
"Wait until the workqueue becomes empty.  While draining is in progress, only "
"chain queueing is allowed.  IOW, only currently pending or running work "
"items on **wq** can queue further work items on it.  **wq** is flushed "
"repeatedly until it becomes empty.  The number of flushing is determined by "
"the depth of chaining and should be relatively short.  Whine if it takes too "
"long."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4252
msgid "wait for a work to finish executing the last queueing instance"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4253
msgid "the work to flush"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4254
msgid ""
"Wait until **work** has finished execution.  **work** is guaranteed to be "
"idle on return if it hasn't been requeued since flush started."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4258
#: kernel/workqueue.c:4277
msgid ""
"``true`` if flush_work() waited for the work to finish execution, ``false`` "
"if it was already idle."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4270
msgid "wait for a dwork to finish executing the last queueing"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4271
msgid "the delayed work to flush"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4272
msgid ""
"Delayed timer is cancelled and the pending work is queued for immediate "
"execution.  Like flush_work(), this function only considers the last "
"queueing instance of **dwork**."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4292
msgid "wait for a rwork to finish executing the last queueing"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4293
msgid "the rcu work to flush"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4295
msgid ""
"``true`` if flush_rcu_work() waited for the work to finish execution, "
"``false`` if it was already idle."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4382
msgid "cancel a work and wait for it to finish"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4383
msgid "the work to cancel"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4384
msgid ""
"Cancel **work** and wait for its execution to finish. This function can be "
"used even if the work re-queues itself or migrates to another workqueue. On "
"return from this function, **work** is guaranteed to be not pending or "
"executing on any CPU as long as there aren't racing enqueues."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4389
msgid ""
"cancel_work_sync(:c:type:`delayed_work->work <delayed_work>`) must not be "
"used for delayed_work's. Use cancel_delayed_work_sync() instead."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4392
#: kernel/workqueue.c:4466
msgid ""
"Must be called from a sleepable context if **work** was last queued on a non-"
"BH workqueue. Can also be called from non-hardirq atomic contexts including "
"BH if **work** was last queued on a BH workqueue."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4396
#: kernel/workqueue.c:4470
msgid "Returns ``true`` if **work** was pending, ``false`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4406
msgid "cancel a delayed work"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4407
msgid "delayed_work to cancel"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4408
msgid "Kill off a pending delayed_work."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4411
msgid ""
"``true`` if **dwork** was pending and canceled; ``false`` if it wasn't "
"pending."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4414
msgid ""
"The work callback function may still be running on return, unless it returns "
"``true`` and the work doesn't re-arm itself.  Explicitly flush or use "
"cancel_delayed_work_sync() to wait on it."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4428
msgid "cancel a delayed work and wait for it to finish"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4429
msgid "the delayed work cancel"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4430
msgid "This is cancel_work_sync() for delayed works."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4433
msgid "``true`` if **dwork** was pending, ``false`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4443
msgid "Disable and cancel a work item"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4444
#: kernel/workqueue.c:4462
msgid "work item to disable"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4445
msgid ""
"Disable **work** by incrementing its disable count and cancel it if "
"currently pending. As long as the disable count is non-zero, any attempt to "
"queue **work** will fail and return ``false``. The maximum supported disable "
"depth is 2 to the power of ``WORK_OFFQ_DISABLE_BITS``, currently 65536."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4450
msgid ""
"Can be called from any context. Returns ``true`` if **work** was pending, "
"``false`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4461
msgid "Disable, cancel and drain a work item"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4463
msgid ""
"Similar to disable_work() but also wait for **work** to finish if currently "
"executing."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4480
msgid "Enable a work item"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4481
msgid "work item to enable"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4482
msgid ""
"Undo disable_work[_sync]() by decrementing **work**'s disable count. "
"**work** can only be queued if its disable count is 0."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4485
msgid ""
"Can be called from any context. Returns ``true`` if the disable count "
"reached 0. Otherwise, ``false``."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4507
msgid "Disable and cancel a delayed work item"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4508
#: kernel/workqueue.c:4521
msgid "delayed work item to disable"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4509
msgid "disable_work() for delayed work items."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4520
msgid "Disable, cancel and drain a delayed work item"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4522
msgid "disable_work_sync() for delayed work items."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4533
msgid "Enable a delayed work item"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4534
msgid "delayed work item to enable"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4535
msgid "enable_work() for delayed work items."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4545
msgid "execute a function synchronously on each online CPU"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4551
msgid "``work_func_t func``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4546
msgid "the function to call"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4547
msgid ""
"schedule_on_each_cpu() executes **func** on each online CPU using the system "
"workqueue and blocks until all CPUs have completed. schedule_on_each_cpu() "
"is very slow."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4552
#: kernel/workqueue.c:7365
msgid "0 on success, -errno on failure."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4582
msgid "reliably execute the routine with user context"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4588
msgid "``work_func_t fn``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4583
msgid "the function to execute"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4585
msgid "``struct execute_work *ew``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4584
msgid ""
"guaranteed storage for the execute work structure (must be available when "
"the work executes)"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4586
msgid ""
"Executes the function immediately if process context is available, otherwise "
"schedules the function for delayed execution."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4590
msgid "0 - function was executed 1 - function was scheduled for execution"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4608
msgid "free a workqueue_attrs"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4614
#: kernel/workqueue.c:5194
msgid "``struct workqueue_attrs *attrs``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4609
msgid "workqueue_attrs to free"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4610
msgid "Undo alloc_workqueue_attrs()."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4623
msgid "allocate a workqueue_attrs"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4629
#: kernel/workqueue.c:6001 kernel/workqueue.c:6017 kernel/workqueue.c:6415
#: kernel/workqueue.c:6439 kernel/workqueue.c:6782 kernel/workqueue.c:6810
#: kernel/workqueue.c:6856 kernel/workqueue.c:7717 kernel/workqueue.c:7874
#: kernel/workqueue.c:7998
msgid "``void``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1
msgid "no arguments"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4624
msgid ""
"Allocate a new workqueue_attrs, initialize with default settings and return "
"it."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4628
msgid "The allocated new workqueue_attr on success. ``NULL`` on failure."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4754
msgid "initialize a newly zalloc'd worker_pool"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4755
msgid "worker_pool to initialize"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4756
msgid ""
"Initialize a newly zalloc'd **pool**.  It also allocates **pool->attrs**."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4759
msgid ""
"0 on success, -errno on failure.  Even on failure, all fields inside "
"**pool** proper are initialized and put_unbound_pool() can be called on "
"**pool** safely to release it."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4917
msgid "put a worker_pool"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4918
msgid "worker_pool to put"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4919
msgid ""
"Put **pool**.  If its refcnt reaches zero, it gets destroyed in RCU safe "
"manner.  get_unbound_pool() calls this function on its failure path and this "
"function should be able to release pools which went through, successfully or "
"not, init_worker_pool()."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4924
#: kernel/workqueue.c:5003
msgid "Should be called with wq_pool_mutex held."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4996
msgid "get a worker_pool with the specified attributes"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5002
#: kernel/workqueue.c:5379
msgid "``const struct workqueue_attrs *attrs``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4997
msgid "the attributes of the worker_pool to get"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4998
msgid ""
"Obtain a worker_pool which has the same attributes as **attrs**, bump the "
"reference count and return it.  If there already is a matching worker_pool, "
"it will be used; otherwise, this function attempts to create a new one."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5006
msgid ""
"On success, a worker_pool with the same attributes as **attrs**. On failure, "
"``NULL``."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5188
msgid "calculate a wq_attrs' cpumask for a pod"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5189
msgid "the wq_attrs of the default pwq of the target workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5190
msgid "the target CPU"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5191
msgid ""
"Calculate the cpumask a workqueue with **attrs** should use on **pod**. The "
"result is stored in **attrs->__pod_cpumask**."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5194
msgid ""
"If pod affinity is not enabled, **attrs->cpumask** is always used. If "
"enabled and **pod** has online CPUs requested by **attrs**, the returned "
"cpumask is the intersection of the possible CPUs of **pod** and **attrs-"
">cpumask**."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5198
msgid ""
"The caller is responsible for ensuring that the cpumask of **pod** stays "
"stable."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5376
msgid "apply new workqueue_attrs to an unbound workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5378
msgid "the workqueue_attrs to apply, allocated with alloc_workqueue_attrs()"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5379
msgid ""
"Apply **attrs** to an unbound workqueue **wq**. Unless disabled, this "
"function maps a separate pwq to each CPU pod with possibles CPUs in **attrs-"
">cpumask** so that work items are affine to the pod it was issued on. Older "
"pwqs are released as in-flight work items finish. Note that a work item "
"which repeatedly requeues itself back-to-back will stay on its current pwq."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5385
msgid "Performs GFP_KERNEL allocations."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5388
msgid "0 on success and -errno on failure."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5403
msgid "update a pwq slot for CPU hot[un]plug"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5405
msgid "the CPU to update the pwq slot for"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5406
msgid ""
"This function is to be called from ``CPU_DOWN_PREPARE``, ``CPU_ONLINE`` and "
"``CPU_DOWN_FAILED``.  **cpu** is in the same pod of the CPU being "
"hot[un]plugged."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5410
msgid ""
"If pod affinity can't be adjusted due to memory allocation failure, it falls "
"back to **wq->dfl_pwq** which may not be optimal but is always correct."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5413
msgid ""
"Note that when the last allowed CPU of a pod goes offline for a workqueue "
"with a cpumask spanning multiple pods, the workers which were already "
"executing the work items for the workqueue will lose their CPU affinity and "
"may execute on any CPU. This is similar to how per-cpu workqueues behave on "
"CPU_DOWN. If a workqueue user wants strict affinity, it's the user's "
"responsibility to flush the work item from CPU_DOWN_PREPARE."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5595
msgid "update a wq's max_active to the current setting"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5596
#: kernel/workqueue.c:5837 kernel/workqueue.c:5935 kernel/workqueue.c:6028
msgid "target workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5597
msgid ""
"If **wq** isn't freezing, set **wq->max_active** to the saved_max_active and "
"activate inactive work items accordingly. If **wq** is freezing, clear **wq-"
">max_active** to zero."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5836
msgid "safely terminate a workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5838
msgid ""
"Safely destroy a workqueue. All work currently pending will be done first."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5840
msgid ""
"This function does NOT guarantee that non-pending work that has been "
"submitted with queue_delayed_work() and similar functions will be done "
"before destroying the workqueue. The fundamental problem is that, currently, "
"the workqueue has no way of accessing non-pending delayed_work. delayed_work "
"is only linked on the timer-side. All delayed_work must, therefore, be "
"canceled before calling this function."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5847
msgid ""
"TODO: It would be better if the problem described above wouldn't exist and "
"destroy_workqueue() would cleanly cancel all pending and non-pending "
"delayed_work."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5934
msgid "adjust max_active of a workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5936
msgid "new max_active value."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5937
msgid ""
"Set max_active of **wq** to **max_active**. See the alloc_workqueue() "
"function comment."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5941
msgid "Don't call from IRQ context."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5968
msgid "adjust min_active of an unbound workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5969
msgid "target unbound workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5971
msgid "``int min_active``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5970
msgid "new min_active value"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5971
msgid ""
"Set min_active of an unbound workqueue. Unlike other types of workqueues, an "
"unbound workqueue is not guaranteed to be able to process max_active "
"interdependent work items. Instead, an unbound workqueue is guaranteed to be "
"able to process min_active number of interdependent work items which is "
"``WQ_DFL_MIN_ACTIVE`` by default."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5977
msgid ""
"Use this function to adjust the min_active value between 0 and the current "
"max_active."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5995
msgid "retrieve ``current`` task's work struct"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5996
msgid ""
"Determine if ``current`` task is a workqueue worker and what it's working "
"on. Useful to find out the context that the ``current`` task is running in."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6000
msgid ""
"work struct if ``current`` task is a workqueue worker, ``NULL`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6011
msgid "is ``current`` workqueue rescuer?"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6012
msgid ""
"Determine whether ``current`` is a workqueue rescuer.  Can be used from work "
"functions to determine whether it's being run off the rescuer task."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6016
msgid "``true`` if ``current`` is a workqueue rescuer. ``false`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6026
msgid "test whether a workqueue is congested"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6027
msgid "CPU in question"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6029
msgid ""
"Test whether **wq**'s cpu workqueue for **cpu** is congested.  There is no "
"synchronization around this function and the test result is unreliable and "
"only useful as advisory hints or for debugging."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6033
msgid "If **cpu** is WORK_CPU_UNBOUND, the test is performed on the local CPU."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6035
msgid ""
"With the exception of ordered workqueues, all workqueues have per-cpu "
"pool_workqueues, each with its own congested state. A workqueue being "
"congested on one CPU doesn't mean that the workqueue is contested on any "
"other CPUs."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6041
msgid "``true`` if congested, ``false`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6066
msgid "test whether a work is currently pending or running"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6067
msgid "the work to be tested"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6068
msgid ""
"Test whether **work** is currently pending or running.  There is no "
"synchronization around this function and the test result is unreliable and "
"only useful as advisory hints or for debugging."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6073
msgid "OR'd bitmask of WORK_BUSY_* bits."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6100
msgid "set description for the current work item"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6101
msgid "printf-style format string"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6102
msgid "arguments for the format string"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6103
msgid ""
"This function can be called by a running work function to describe what the "
"work item is about.  If the worker task gets dumped, this information will "
"be printed out together to help debugging.  The description can be at most "
"WORKER_DESC_LEN including the trailing '\\0'."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6123
msgid "print out worker information and description"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6129
msgid "``const char *log_lvl``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6124
msgid "the log level to use when printing"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6125
msgid "target task"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6126
msgid ""
"If **task** is a worker and currently executing a work item, print out the "
"name of the workqueue being serviced and worker description set with "
"set_worker_desc() by the currently executing work item."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6130
msgid ""
"This function can be safely called on any task as long as the task_struct "
"itself is accessible.  While safe, this function isn't synchronized and may "
"print out mixups or garbages of limited length."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6316
msgid "dump state of specified workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6317
msgid "workqueue whose state will be printed"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6360
msgid "dump state of specified worker pool"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6361
msgid "worker pool whose state will be printed"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6409
msgid "dump workqueue state"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6410
msgid ""
"Called from a sysrq handler and prints out all busy workqueues and pools."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6433
msgid "dump freezable workqueue state"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6434
msgid ""
"Called from try_to_freeze_tasks() and prints out all freezable workqueues "
"still busy."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6558
msgid "rebind all workers of a pool to the associated CPU"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6560
msgid "**pool->cpu** is coming online.  Rebind all workers to the CPU."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6614
msgid "restore cpumask of unbound workers"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6615
msgid "unbound pool of interest"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6616
msgid "the CPU which is coming up"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6617
msgid ""
"An unbound pool may end up with a cpumask which doesn't have any online "
"CPUs.  When a worker of such pool get scheduled, the scheduler resets its "
"cpus_allowed.  If **cpu** is in **pool**'s cpumask which didn't have any "
"online CPU before, cpus_allowed of all its workers should be restored."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6748
msgid "run a function in thread context on a particular cpu"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6749
msgid "the cpu to run on"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6751
msgid "``long (*fn)(void *)``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6750
msgid "the function to run"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6752
msgid "``void *arg``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6751
msgid "the function arg"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6753
msgid "``struct lock_class_key *key``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6752
msgid "The lock class key for lock debugging purposes"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6753
msgid ""
"It is up to the caller to ensure that the cpu doesn't go offline. The caller "
"must not hold any locks which would prevent **fn** from completing."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6757
msgid "The value **fn** returns."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6776
msgid "begin freezing workqueues"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6777
msgid ""
"Start freezing workqueues.  After this function returns, all freezable "
"workqueues will queue new works to their inactive_works list instead of pool-"
">worklist."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6782
#: kernel/workqueue.c:6855
msgid "Grabs and releases wq_pool_mutex, wq->mutex and pool->lock's."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6804
msgid "are freezable workqueues still busy?"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6805
msgid ""
"Check whether freezing is complete.  This function must be called between "
"freeze_workqueues_begin() and thaw_workqueues()."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6809
msgid "Grabs and releases wq_pool_mutex."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6812
msgid ""
"``true`` if some freezable workqueues are still busy.  ``false`` if freezing "
"is complete."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6850
msgid "thaw workqueues"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6851
msgid ""
"Thaw workqueues.  Normal queueing is restored and all collected frozen works "
"are transferred to their respective pool worklists."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6918
msgid "Exclude given CPUs from unbound cpumask"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6924
msgid "``cpumask_var_t exclude_cpumask``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6919
msgid "the cpumask to be excluded from wq_unbound_cpumask"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6920
msgid ""
"This function can be called from cpuset code to provide a set of isolated "
"CPUs that should be excluded from wq_unbound_cpumask."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:7248
msgid "Set the low-level unbound cpumask"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:7254
msgid "``cpumask_var_t cpumask``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:7249
msgid "the cpumask to set"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:7250
msgid ""
"The low-level workqueues cpumask is a global cpumask that limits the "
"affinity of all unbound workqueues.  This function check the **cpumask** and "
"apply it to all unbound workqueues and updates all pwqs of them."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:7255
msgid ""
"0       - Success -EINVAL - Invalid **cpumask** -ENOMEM - Failed to allocate "
"memory for attrs or pwqs."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:7353
msgid "make a workqueue visible in sysfs"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:7354
msgid "the workqueue to register"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:7355
msgid ""
"Expose **wq** in sysfs under /sys/bus/workqueue/devices. alloc_workqueue*() "
"automatically calls this function if WQ_SYSFS is set which is the preferred "
"method."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:7359
msgid ""
"Workqueue user should use this function directly iff it wants to apply "
"workqueue_attrs before making the workqueue visible in sysfs; otherwise, "
"apply_workqueue_attrs() may race against userland updating the attributes."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:7420
msgid "undo workqueue_sysfs_register()"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:7421
msgid "the workqueue to unregister"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:7422
msgid ""
"If **wq** is registered to sysfs by workqueue_sysfs_register(), unregister."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:7711
msgid "early init for workqueue subsystem"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:7712
msgid ""
"This is the first step of three-staged workqueue subsystem initialization "
"and invoked as soon as the bare basics - memory allocation, cpumasks and idr "
"are up. It sets up all the data structures and system workqueues and allows "
"early boot code to create workqueues and queue/cancel work items. Actual "
"work item execution starts only after kthreads can be created and scheduled "
"right before early initcalls."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:7868
msgid "bring workqueue subsystem fully online"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:7869
msgid ""
"This is the second step of three-staged workqueue subsystem initialization "
"and invoked as soon as kthreads can be created and scheduled. Workqueues "
"have been created and work items queued on them, but there are no kworkers "
"executing the work items yet. Populate the worker pools with the initial "
"workers and enable future kworker creations."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:7992
msgid "initialize CPU pods for unbound workqueues"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:7993
msgid ""
"This is the third step of three-staged workqueue subsystem initialization "
"and invoked after SMP and topology information are fully initialized. It "
"initializes the unbound CPU pods accordingly."
msgstr ""
