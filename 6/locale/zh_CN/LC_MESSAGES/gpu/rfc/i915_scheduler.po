# SOME DESCRIPTIVE TITLE.
# Copyright (C) The kernel development community
# This file is distributed under the same license as the The Linux Kernel package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: The Linux Kernel 6\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-10-28 09:03+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../gpu/rfc/i915_scheduler.rst:3
msgid "I915 GuC Submission/DRM Scheduler Section"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:6
msgid "Upstream plan"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:7
msgid ""
"For upstream the overall plan for landing GuC submission and integrating the "
"i915 with the DRM scheduler is:"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:10
msgid "Merge basic GuC submission"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:11
msgid "Basic submission support for all gen11+ platforms"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:12
msgid ""
"Not enabled by default on any current platforms but can be enabled via "
"modparam enable_guc"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:14
msgid ""
"Lots of rework will need to be done to integrate with DRM scheduler so no "
"need to nit pick everything in the code, it just should be functional, no "
"major coding style / layering errors, and not regress execlists"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:18
msgid "Update IGTs / selftests as needed to work with GuC submission"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:19
msgid "Enable CI on supported platforms for a baseline"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:20
msgid "Rework / get CI heathly for GuC submission in place as needed"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:21
msgid "Merge new parallel submission uAPI"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:22
msgid ""
"Bonding uAPI completely incompatible with GuC submission, plus it has severe "
"design issues in general, which is why we want to retire it no matter what"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:25
msgid ""
"New uAPI adds I915_CONTEXT_ENGINES_EXT_PARALLEL context setup step which "
"configures a slot with N contexts"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:27
msgid ""
"After I915_CONTEXT_ENGINES_EXT_PARALLEL a user can submit N batches to a "
"slot in a single execbuf IOCTL and the batches run on the GPU in parallel"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:30
msgid ""
"Initially only for GuC submission but execlists can be supported if needed"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:32
msgid "Convert the i915 to use the DRM scheduler"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:33
msgid "GuC submission backend fully integrated with DRM scheduler"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:34
msgid ""
"All request queues removed from backend (e.g. all backpressure handled in "
"DRM scheduler)"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:36
msgid "Resets / cancels hook in DRM scheduler"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:37
msgid "Watchdog hooks into DRM scheduler"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:38
msgid ""
"Lots of complexity of the GuC backend can be pulled out once integrated with "
"DRM scheduler (e.g. state machine gets simpler, locking gets simpler, etc...)"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:41
msgid "Execlists backend will minimum required to hook in the DRM scheduler"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:42
msgid "Legacy interface"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:43
msgid ""
"Features like timeslicing / preemption / virtual engines would be difficult "
"to integrate with the DRM scheduler and these features are not required for "
"GuC submission as the GuC does these things for us"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:47
msgid "ROI low on fully integrating into DRM scheduler"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:48
msgid "Fully integrating would add lots of complexity to DRM scheduler"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:50
msgid "Port i915 priority inheritance / boosting feature in DRM scheduler"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:51
msgid "Used for i915 page flip, may be useful to other DRM drivers as well"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:53
msgid "Will be an optional feature in the DRM scheduler"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:54
msgid "Remove in-order completion assumptions from DRM scheduler"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:55
msgid ""
"Even when using the DRM scheduler the backends will handle preemption, "
"timeslicing, etc... so it is possible for jobs to finish out of order"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:58
msgid "Pull out i915 priority levels and use DRM priority levels"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:59
msgid "Optimize DRM scheduler as needed"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:62
msgid "TODOs for GuC submission upstream"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:64
msgid "Need an update to GuC firmware / i915 to enable error state capture"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:65
msgid "Open source tool to decode GuC logs"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:66
msgid "Public GuC spec"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:69
msgid "New uAPI for basic GuC submission"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:70
msgid ""
"No major changes are required to the uAPI for basic GuC submission. The only "
"change is a new scheduler attribute: I915_SCHEDULER_CAP_STATIC_PRIORITY_MAP. "
"This attribute indicates the 2k i915 user priority levels are statically "
"mapped into 3 levels as follows:"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:75
msgid "-1k to -1 Low priority"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:76
msgid "0 Medium priority"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:77
msgid "1 to 1k High priority"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:79
msgid ""
"This is needed because the GuC only has 4 priority bands. The highest "
"priority band is reserved with the kernel. This aligns with the DRM "
"scheduler priority levels too."
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:84
msgid "Spec references:"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:85
msgid ""
"https://www.khronos.org/registry/EGL/extensions/IMG/EGL_IMG_context_priority."
"txt"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:86
msgid ""
"https://www.khronos.org/registry/vulkan/specs/1.2-extensions/html/chap5."
"html#devsandqueues-priority"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:87
msgid ""
"https://spec.oneapi.com/level-zero/latest/core/api.html#ze-command-queue-"
"priority-t"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:90
msgid "New parallel submission uAPI"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:91
msgid ""
"The existing bonding uAPI is completely broken with GuC submission because "
"whether a submission is a single context submit or parallel submit isn't "
"known until execbuf time activated via the I915_SUBMIT_FENCE. To submit "
"multiple contexts in parallel with the GuC the context must be explicitly "
"registered with N contexts and all N contexts must be submitted in a single "
"command to the GuC. The GuC interfaces do not support dynamically changing "
"between N contexts as the bonding uAPI does. Hence the need for a new "
"parallel submission interface. Also the legacy bonding uAPI is quite "
"confusing and not intuitive at all. Furthermore I915_SUBMIT_FENCE is by "
"design a future fence, so not really something we should continue to support."
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:102
msgid "The new parallel submission uAPI consists of 3 parts:"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:104
#: ../../../gpu/rfc/i915_scheduler.rst:110
msgid "Export engines logical mapping"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:105
#: ../../../gpu/rfc/i915_scheduler.rst:126
msgid ""
"A 'set_parallel' extension to configure contexts for parallel submission"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:107
#: ../../../gpu/rfc/i915_scheduler.rst:146
msgid "Extend execbuf2 IOCTL to support submitting N BBs in a single IOCTL"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:111
msgid ""
"Certain use cases require BBs to be placed on engine instances in logical "
"order (e.g. split-frame on gen11+). The logical mapping of engine instances "
"can change based on fusing. Rather than making UMDs be aware of fusing, "
"simply expose the logical mapping with the existing query engine info IOCTL. "
"Also the GuC submission interface currently only supports submitting "
"multiple contexts to engines in logical order which is a new requirement "
"compared to execlists. Lastly, all current platforms have at most 2 engine "
"instances and the logical order is the same as uAPI order. This will change "
"on platforms with more than 2 engine instances."
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:121
msgid ""
"A single bit will be added to drm_i915_engine_info.flags indicating that the "
"logical instance has been returned and a new field, drm_i915_engine_info."
"logical_instance, returns the logical instance."
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:127
msgid ""
"The 'set_parallel' extension configures a slot for parallel submission of N "
"BBs. It is a setup step that must be called before using any of the "
"contexts. See I915_CONTEXT_ENGINES_EXT_LOAD_BALANCE or "
"I915_CONTEXT_ENGINES_EXT_BOND for similar existing examples. Once a slot is "
"configured for parallel submission the execbuf2 IOCTL can be called "
"submitting N BBs in a single IOCTL. Initially only supports GuC submission. "
"Execlists supports can be added later if needed."
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:134
msgid ""
"Add I915_CONTEXT_ENGINES_EXT_PARALLEL_SUBMIT and "
"drm_i915_context_engines_parallel_submit to the uAPI to implement this "
"extension."
msgstr ""

#: ../../../gpu/rfc/i915_scheduler:140: include/uapi/drm/i915_drm.h:2380
msgid "Configure engine for parallel submission."
msgstr ""

#: ../../../gpu/rfc/i915_scheduler:140: include/uapi/drm/i915_drm.h:2384
msgid "**Definition**::"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler:140: include/uapi/drm/i915_drm.h:2397
msgid "**Members**"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler:140: include/uapi/drm/i915_drm.h:2451
msgid "``base``"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler:140: include/uapi/drm/i915_drm.h:2452
msgid "base user extension."
msgstr ""

#: ../../../gpu/rfc/i915_scheduler:140: include/uapi/drm/i915_drm.h:2456
msgid "``engine_index``"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler:140: include/uapi/drm/i915_drm.h:2457
msgid "slot for parallel engine"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler:140: include/uapi/drm/i915_drm.h:2461
msgid "``width``"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler:140: include/uapi/drm/i915_drm.h:2462
msgid ""
"number of contexts per parallel engine or in other words the number of "
"batches in each submission"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler:140: include/uapi/drm/i915_drm.h:2467
msgid "``num_siblings``"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler:140: include/uapi/drm/i915_drm.h:2468
msgid ""
"number of siblings per context or in other words the number of possible "
"placements for each submission"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler:140: include/uapi/drm/i915_drm.h:2473
msgid "``mbz16``"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler:140: include/uapi/drm/i915_drm.h:2474
#: include/uapi/drm/i915_drm.h:2484
msgid "reserved for future use; must be zero"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler:140: include/uapi/drm/i915_drm.h:2478
msgid "``flags``"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler:140: include/uapi/drm/i915_drm.h:2479
msgid "all undefined flags must be zero, currently not defined flags"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler:140: include/uapi/drm/i915_drm.h:2483
msgid "``mbz64``"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler:140: include/uapi/drm/i915_drm.h:2488
msgid "``engines``"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler:140: include/uapi/drm/i915_drm.h:2489
msgid "2-d array of engine instances to configure parallel engine"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler:140: include/uapi/drm/i915_drm.h:2491
msgid "length = width (i) * num_siblings (j) index = j + i * num_siblings"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler:140: include/uapi/drm/i915_drm.h:2495
msgid "**Description**"
msgstr ""

#: ../../../gpu/rfc/i915_scheduler:140: include/uapi/drm/i915_drm.h:2381
msgid ""
"Setup a slot in the context engine map to allow multiple BBs to be submitted "
"in a single execbuf IOCTL. Those BBs will then be scheduled to run on the "
"GPU in parallel. Multiple hardware contexts are created internally in the "
"i915 to run these BBs. Once a slot is configured for N BBs only N BBs can be "
"submitted in each execbuf IOCTL and this is implicit behavior e.g. The user "
"doesn't tell the execbuf IOCTL there are N BBs, the execbuf IOCTL knows how "
"many BBs there are based on the slot's configuration. The N BBs are the last "
"N buffer objects or first N if I915_EXEC_BATCH_FIRST is set."
msgstr ""

#: ../../../gpu/rfc/i915_scheduler:140: include/uapi/drm/i915_drm.h:2390
msgid ""
"The default placement behavior is to create implicit bonds between each "
"context if each context maps to more than 1 physical engine (e.g. context is "
"a virtual engine). Also we only allow contexts of same engine class and "
"these contexts must be in logically contiguous order. Examples of the "
"placement behavior are described below. Lastly, the default is to not allow "
"BBs to be preempted mid-batch. Rather insert coordinated preemption points "
"on all hardware contexts between each set of BBs. Flags could be added in "
"the future to change both of these default behaviors."
msgstr ""

#: ../../../gpu/rfc/i915_scheduler:140: include/uapi/drm/i915_drm.h:2399
msgid ""
"Returns -EINVAL if hardware context placement configuration is invalid or if "
"the placement configuration isn't supported on the platform / submission "
"interface. Returns -ENODEV if extension isn't supported on the platform / "
"submission interface."
msgstr ""

#: ../../../gpu/rfc/i915_scheduler.rst:147
msgid ""
"Contexts that have been configured with the 'set_parallel' extension can "
"only submit N BBs in a single execbuf2 IOCTL. The BBs are either the last N "
"objects in the drm_i915_gem_exec_object2 list or the first N if "
"I915_EXEC_BATCH_FIRST is set. The number of BBs is implicit based on the "
"slot submitted and how it has been configured by 'set_parallel' or other "
"extensions. No uAPI changes are required to the execbuf2 IOCTL."
msgstr ""
