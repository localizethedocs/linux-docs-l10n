# SOME DESCRIPTIVE TITLE.
# Copyright (C) The kernel development community
# This file is distributed under the same license as the The Linux Kernel package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: The Linux Kernel master\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-02-16 08:44+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../core-api/workqueue.rst:3
msgid "Workqueue"
msgstr ""

#: ../../../core-api/workqueue.rst:0
msgid "Date"
msgstr ""

#: ../../../core-api/workqueue.rst:5
msgid "September, 2010"
msgstr ""

#: ../../../core-api/workqueue.rst:0
msgid "Author"
msgstr ""

#: ../../../core-api/workqueue.rst:6
msgid "Tejun Heo <tj@kernel.org>"
msgstr ""

#: ../../../core-api/workqueue.rst:7
msgid "Florian Mickler <florian@mickler.org>"
msgstr ""

#: ../../../core-api/workqueue.rst:11
msgid "Introduction"
msgstr ""

#: ../../../core-api/workqueue.rst:13
msgid ""
"There are many cases where an asynchronous process execution context is "
"needed and the workqueue (wq) API is the most commonly used mechanism for "
"such cases."
msgstr ""

#: ../../../core-api/workqueue.rst:17
msgid ""
"When such an asynchronous execution context is needed, a work item "
"describing which function to execute is put on a queue.  An independent "
"thread serves as the asynchronous execution context.  The queue is called "
"workqueue and the thread is called worker."
msgstr ""

#: ../../../core-api/workqueue.rst:22
msgid ""
"While there are work items on the workqueue the worker executes the "
"functions associated with the work items one after the other.  When there is "
"no work item left on the workqueue the worker becomes idle. When a new work "
"item gets queued, the worker begins executing again."
msgstr ""

#: ../../../core-api/workqueue.rst:29
msgid "Why Concurrency Managed Workqueue?"
msgstr ""

#: ../../../core-api/workqueue.rst:31
msgid ""
"In the original wq implementation, a multi threaded (MT) wq had one worker "
"thread per CPU and a single threaded (ST) wq had one worker thread system-"
"wide.  A single MT wq needed to keep around the same number of workers as "
"the number of CPUs.  The kernel grew a lot of MT wq users over the years and "
"with the number of CPU cores continuously rising, some systems saturated the "
"default 32k PID space just booting up."
msgstr ""

#: ../../../core-api/workqueue.rst:39
msgid ""
"Although MT wq wasted a lot of resource, the level of concurrency provided "
"was unsatisfactory.  The limitation was common to both ST and MT wq albeit "
"less severe on MT.  Each wq maintained its own separate worker pool.  An MT "
"wq could provide only one execution context per CPU while an ST wq one for "
"the whole system.  Work items had to compete for those very limited "
"execution contexts leading to various problems including proneness to "
"deadlocks around the single execution context."
msgstr ""

#: ../../../core-api/workqueue.rst:47
msgid ""
"The tension between the provided level of concurrency and resource usage "
"also forced its users to make unnecessary tradeoffs like libata choosing to "
"use ST wq for polling PIOs and accepting an unnecessary limitation that no "
"two polling PIOs can progress at the same time.  As MT wq don't provide much "
"better concurrency, users which require higher level of concurrency, like "
"async or fscache, had to implement their own thread pool."
msgstr ""

#: ../../../core-api/workqueue.rst:55
msgid ""
"Concurrency Managed Workqueue (cmwq) is a reimplementation of wq with focus "
"on the following goals."
msgstr ""

#: ../../../core-api/workqueue.rst:58
msgid "Maintain compatibility with the original workqueue API."
msgstr ""

#: ../../../core-api/workqueue.rst:60
msgid ""
"Use per-CPU unified worker pools shared by all wq to provide flexible level "
"of concurrency on demand without wasting a lot of resource."
msgstr ""

#: ../../../core-api/workqueue.rst:64
msgid ""
"Automatically regulate worker pool and level of concurrency so that the API "
"users don't need to worry about such details."
msgstr ""

#: ../../../core-api/workqueue.rst:69
msgid "The Design"
msgstr ""

#: ../../../core-api/workqueue.rst:71
msgid ""
"In order to ease the asynchronous execution of functions a new abstraction, "
"the work item, is introduced."
msgstr ""

#: ../../../core-api/workqueue.rst:74
msgid ""
"A work item is a simple struct that holds a pointer to the function that is "
"to be executed asynchronously.  Whenever a driver or subsystem wants a "
"function to be executed asynchronously it has to set up a work item pointing "
"to that function and queue that work item on a workqueue."
msgstr ""

#: ../../../core-api/workqueue.rst:80
msgid ""
"A work item can be executed in either a thread or the BH (softirq) context."
msgstr ""

#: ../../../core-api/workqueue.rst:82
msgid ""
"For threaded workqueues, special purpose threads, called [k]workers, execute "
"the functions off of the queue, one after the other. If no work is queued, "
"the worker threads become idle. These worker threads are managed in worker-"
"pools."
msgstr ""

#: ../../../core-api/workqueue.rst:87
msgid ""
"The cmwq design differentiates between the user-facing workqueues that "
"subsystems and drivers queue work items on and the backend mechanism which "
"manages worker-pools and processes the queued work items."
msgstr ""

#: ../../../core-api/workqueue.rst:91
msgid ""
"There are two worker-pools, one for normal work items and the other for high "
"priority ones, for each possible CPU and some extra worker-pools to serve "
"work items queued on unbound workqueues - the number of these backing pools "
"is dynamic."
msgstr ""

#: ../../../core-api/workqueue.rst:96
msgid ""
"BH workqueues use the same framework. However, as there can only be one "
"concurrent execution context, there's no need to worry about concurrency. "
"Each per-CPU BH worker pool contains only one pseudo worker which represents "
"the BH execution context. A BH workqueue can be considered a convenience "
"interface to softirq."
msgstr ""

#: ../../../core-api/workqueue.rst:102
msgid ""
"Subsystems and drivers can create and queue work items through special "
"workqueue API functions as they see fit. They can influence some aspects of "
"the way the work items are executed by setting flags on the workqueue they "
"are putting the work item on. These flags include things like CPU locality, "
"concurrency limits, priority and more.  To get a detailed overview refer to "
"the API description of ``alloc_workqueue()`` below."
msgstr ""

#: ../../../core-api/workqueue.rst:110
msgid ""
"When a work item is queued to a workqueue, the target worker-pool is "
"determined according to the queue parameters and workqueue attributes and "
"appended on the shared worklist of the worker-pool.  For example, unless "
"specifically overridden, a work item of a bound workqueue will be queued on "
"the worklist of either normal or highpri worker-pool that is associated to "
"the CPU the issuer is running on."
msgstr ""

#: ../../../core-api/workqueue.rst:117
msgid ""
"For any thread pool implementation, managing the concurrency level (how many "
"execution contexts are active) is an important issue.  cmwq tries to keep "
"the concurrency at a minimal but sufficient level. Minimal to save resources "
"and sufficient in that the system is used at its full capacity."
msgstr ""

#: ../../../core-api/workqueue.rst:123
msgid ""
"Each worker-pool bound to an actual CPU implements concurrency management by "
"hooking into the scheduler.  The worker-pool is notified whenever an active "
"worker wakes up or sleeps and keeps track of the number of the currently "
"runnable workers.  Generally, work items are not expected to hog a CPU and "
"consume many cycles.  That means maintaining just enough concurrency to "
"prevent work processing from stalling should be optimal.  As long as there "
"are one or more runnable workers on the CPU, the worker-pool doesn't start "
"execution of a new work, but, when the last running worker goes to sleep, it "
"immediately schedules a new worker so that the CPU doesn't sit idle while "
"there are pending work items.  This allows using a minimal number of workers "
"without losing execution bandwidth."
msgstr ""

#: ../../../core-api/workqueue.rst:136
msgid ""
"Keeping idle workers around doesn't cost other than the memory space for "
"kthreads, so cmwq holds onto idle ones for a while before killing them."
msgstr ""

#: ../../../core-api/workqueue.rst:140
msgid ""
"For unbound workqueues, the number of backing pools is dynamic. Unbound "
"workqueue can be assigned custom attributes using "
"``apply_workqueue_attrs()`` and workqueue will automatically create backing "
"worker pools matching the attributes.  The responsibility of regulating "
"concurrency level is on the users.  There is also a flag to mark a bound wq "
"to ignore the concurrency management.  Please refer to the API section for "
"details."
msgstr ""

#: ../../../core-api/workqueue.rst:148
msgid ""
"Forward progress guarantee relies on that workers can be created when more "
"execution contexts are necessary, which in turn is guaranteed through the "
"use of rescue workers.  All work items which might be used on code paths "
"that handle memory reclaim are required to be queued on wq's that have a "
"rescue-worker reserved for execution under memory pressure.  Else it is "
"possible that the worker-pool deadlocks waiting for execution contexts to "
"free up."
msgstr ""

#: ../../../core-api/workqueue.rst:158
msgid "Application Programming Interface (API)"
msgstr ""

#: ../../../core-api/workqueue.rst:160
msgid ""
"``alloc_workqueue()`` allocates a wq.  The original ``create_*workqueue()`` "
"functions are deprecated and scheduled for removal.  ``alloc_workqueue()`` "
"takes three arguments - ``@name``, ``@flags`` and ``@max_active``.  "
"``@name`` is the name of the wq and also used as the name of the rescuer "
"thread if there is one."
msgstr ""

#: ../../../core-api/workqueue.rst:166
msgid ""
"A wq no longer manages execution resources but serves as a domain for "
"forward progress guarantee, flush and work item attributes. ``@flags`` and "
"``@max_active`` control how work items are assigned execution resources, "
"scheduled and executed."
msgstr ""

#: ../../../core-api/workqueue.rst:173 ../../../core-api/workqueue:787:
#: ../include/linux/workqueue.h:541 ../include/linux/workqueue.h:560
msgid "``flags``"
msgstr ""

#: ../../../core-api/workqueue.rst:175
msgid "``WQ_BH``"
msgstr ""

#: ../../../core-api/workqueue.rst:176
msgid ""
"BH workqueues can be considered a convenience interface to softirq. BH "
"workqueues are always per-CPU and all BH work items are executed in the "
"queueing CPU's softirq context in the queueing order."
msgstr ""

#: ../../../core-api/workqueue.rst:180
msgid ""
"All BH workqueues must have 0 ``max_active`` and ``WQ_HIGHPRI`` is the only "
"allowed additional flag."
msgstr ""

#: ../../../core-api/workqueue.rst:183
msgid ""
"BH work items cannot sleep. All other features such as delayed queueing, "
"flushing and canceling are supported."
msgstr ""

#: ../../../core-api/workqueue.rst:186
msgid "``WQ_PERCPU``"
msgstr ""

#: ../../../core-api/workqueue.rst:187
msgid ""
"Work items queued to a per-cpu wq are bound to a specific CPU. This flag is "
"the right choice when cpu locality is important."
msgstr ""

#: ../../../core-api/workqueue.rst:190
msgid "This flag is the complement of ``WQ_UNBOUND``."
msgstr ""

#: ../../../core-api/workqueue.rst:192
msgid "``WQ_UNBOUND``"
msgstr ""

#: ../../../core-api/workqueue.rst:193
msgid ""
"Work items queued to an unbound wq are served by the special worker-pools "
"which host workers which are not bound to any specific CPU.  This makes the "
"wq behave as a simple execution context provider without concurrency "
"management.  The unbound worker-pools try to start execution of work items "
"as soon as possible.  Unbound wq sacrifices locality but is useful for the "
"following cases."
msgstr ""

#: ../../../core-api/workqueue.rst:201
msgid ""
"Wide fluctuation in the concurrency level requirement is expected and using "
"bound wq may end up creating large number of mostly unused workers across "
"different CPUs as the issuer hops through different CPUs."
msgstr ""

#: ../../../core-api/workqueue.rst:206
msgid ""
"Long running CPU intensive workloads which can be better managed by the "
"system scheduler."
msgstr ""

#: ../../../core-api/workqueue.rst:209
msgid "``WQ_FREEZABLE``"
msgstr ""

#: ../../../core-api/workqueue.rst:210
msgid ""
"A freezable wq participates in the freeze phase of the system suspend "
"operations.  Work items on the wq are drained and no new work item starts "
"execution until thawed."
msgstr ""

#: ../../../core-api/workqueue.rst:214
msgid "``WQ_MEM_RECLAIM``"
msgstr ""

#: ../../../core-api/workqueue.rst:215
msgid ""
"All wq which might be used in the memory reclaim paths **MUST** have this "
"flag set.  The wq is guaranteed to have at least one execution context "
"regardless of memory pressure."
msgstr ""

#: ../../../core-api/workqueue.rst:219
msgid "``WQ_HIGHPRI``"
msgstr ""

#: ../../../core-api/workqueue.rst:220
msgid ""
"Work items of a highpri wq are queued to the highpri worker-pool of the "
"target cpu.  Highpri worker-pools are served by worker threads with elevated "
"nice level."
msgstr ""

#: ../../../core-api/workqueue.rst:224
msgid ""
"Note that normal and highpri worker-pools don't interact with each other.  "
"Each maintains its separate pool of workers and implements concurrency "
"management among its workers."
msgstr ""

#: ../../../core-api/workqueue.rst:228
msgid "``WQ_CPU_INTENSIVE``"
msgstr ""

#: ../../../core-api/workqueue.rst:229
msgid ""
"Work items of a CPU intensive wq do not contribute to the concurrency "
"level.  In other words, runnable CPU intensive work items will not prevent "
"other work items in the same worker-pool from starting execution.  This is "
"useful for bound work items which are expected to hog CPU cycles so that "
"their execution is regulated by the system scheduler."
msgstr ""

#: ../../../core-api/workqueue.rst:236
msgid ""
"Although CPU intensive work items don't contribute to the concurrency level, "
"start of their executions is still regulated by the concurrency management "
"and runnable non-CPU-intensive work items can delay execution of CPU "
"intensive work items."
msgstr ""

#: ../../../core-api/workqueue.rst:242
msgid "This flag is meaningless for unbound wq."
msgstr ""

#: ../../../core-api/workqueue.rst:246
msgid "``max_active``"
msgstr ""

#: ../../../core-api/workqueue.rst:248
msgid ""
"``@max_active`` determines the maximum number of execution contexts per CPU "
"which can be assigned to the work items of a wq. For example, with "
"``@max_active`` of 16, at most 16 work items of the wq can be executing at "
"the same time per CPU. This is always a per-CPU attribute, even for unbound "
"workqueues."
msgstr ""

#: ../../../core-api/workqueue.rst:254
msgid ""
"The maximum limit for ``@max_active`` is 2048 and the default value used "
"when 0 is specified is 1024. These values are chosen sufficiently high such "
"that they are not the limiting factor while providing protection in runaway "
"cases."
msgstr ""

#: ../../../core-api/workqueue.rst:259
msgid ""
"The number of active work items of a wq is usually regulated by the users of "
"the wq, more specifically, by how many work items the users may queue at the "
"same time.  Unless there is a specific need for throttling the number of "
"active work items, specifying '0' is recommended."
msgstr ""

#: ../../../core-api/workqueue.rst:265
msgid ""
"Some users depend on strict execution ordering where only one work item is "
"in flight at any given time and the work items are processed in queueing "
"order. While the combination of ``@max_active`` of 1 and ``WQ_UNBOUND`` used "
"to achieve this behavior, this is no longer the case. Use "
"alloc_ordered_workqueue() instead."
msgstr ""

#: ../../../core-api/workqueue.rst:273
msgid "Example Execution Scenarios"
msgstr ""

#: ../../../core-api/workqueue.rst:275
msgid ""
"The following example execution scenarios try to illustrate how cmwq behave "
"under different configurations."
msgstr ""

#: ../../../core-api/workqueue.rst:278
msgid ""
"Work items w0, w1, w2 are queued to a bound wq q0 on the same CPU. w0 burns "
"CPU for 5ms then sleeps for 10ms then burns CPU for 5ms again before "
"finishing.  w1 and w2 burn CPU for 5ms then sleep for 10ms."
msgstr ""

#: ../../../core-api/workqueue.rst:283
msgid ""
"Ignoring all other tasks, works and processing overhead, and assuming simple "
"FIFO scheduling, the following is one highly simplified version of possible "
"sequences of events with the original wq. ::"
msgstr ""

#: ../../../core-api/workqueue.rst:299
msgid "And with cmwq with ``@max_active`` >= 3, ::"
msgstr ""

#: ../../../core-api/workqueue.rst:313
msgid "If ``@max_active`` == 2, ::"
msgstr ""

#: ../../../core-api/workqueue.rst:327
msgid ""
"Now, let's assume w1 and w2 are queued to a different wq q1 which has "
"``WQ_CPU_INTENSIVE`` set, ::"
msgstr ""

#: ../../../core-api/workqueue.rst:343
msgid "Guidelines"
msgstr ""

#: ../../../core-api/workqueue.rst:345
msgid ""
"Do not forget to use ``WQ_MEM_RECLAIM`` if a wq may process work items which "
"are used during memory reclaim.  Each wq with ``WQ_MEM_RECLAIM`` set has an "
"execution context reserved for it.  If there is dependency among multiple "
"work items used during memory reclaim, they should be queued to separate wq "
"each with ``WQ_MEM_RECLAIM``."
msgstr ""

#: ../../../core-api/workqueue.rst:352
msgid "Unless strict ordering is required, there is no need to use ST wq."
msgstr ""

#: ../../../core-api/workqueue.rst:354
msgid ""
"Unless there is a specific need, using 0 for @max_active is recommended.  In "
"most use cases, concurrency level usually stays well under the default limit."
msgstr ""

#: ../../../core-api/workqueue.rst:358
msgid ""
"A wq serves as a domain for forward progress guarantee (``WQ_MEM_RECLAIM``, "
"flush and work item attributes.  Work items which are not involved in memory "
"reclaim and don't need to be flushed as a part of a group of work items, and "
"don't require any special attribute, can use one of the system wq.  There is "
"no difference in execution characteristics between using a dedicated wq and "
"a system wq."
msgstr ""

#: ../../../core-api/workqueue.rst:366
msgid ""
"Note: If something may generate more than @max_active outstanding work items "
"(do stress test your producers), it may saturate a system wq and potentially "
"lead to deadlock. It should utilize its own dedicated workqueue rather than "
"the system wq."
msgstr ""

#: ../../../core-api/workqueue.rst:371
msgid ""
"Unless work items are expected to consume a huge amount of CPU cycles, using "
"a bound wq is usually beneficial due to the increased level of locality in "
"wq operations and work item execution."
msgstr ""

#: ../../../core-api/workqueue.rst:377
msgid "Affinity Scopes"
msgstr ""

#: ../../../core-api/workqueue.rst:379
msgid ""
"An unbound workqueue groups CPUs according to its affinity scope to improve "
"cache locality. For example, if a workqueue is using the default affinity "
"scope of \"cache\", it will group CPUs according to last level cache "
"boundaries. A work item queued on the workqueue will be assigned to a worker "
"on one of the CPUs which share the last level cache with the issuing CPU. "
"Once started, the worker may or may not be allowed to move outside the scope "
"depending on the ``affinity_strict`` setting of the scope."
msgstr ""

#: ../../../core-api/workqueue.rst:387
msgid "Workqueue currently supports the following affinity scopes."
msgstr ""

#: ../../../core-api/workqueue.rst:389
msgid "``default``"
msgstr ""

#: ../../../core-api/workqueue.rst:390
msgid ""
"Use the scope in module parameter ``workqueue.default_affinity_scope`` which "
"is always set to one of the scopes below."
msgstr ""

#: ../../../core-api/workqueue.rst:393
msgid "``cpu``"
msgstr ""

#: ../../../core-api/workqueue.rst:394
msgid ""
"CPUs are not grouped. A work item issued on one CPU is processed by a worker "
"on the same CPU. This makes unbound workqueues behave as per-cpu workqueues "
"without concurrency management."
msgstr ""

#: ../../../core-api/workqueue.rst:398
msgid "``smt``"
msgstr ""

#: ../../../core-api/workqueue.rst:399
msgid ""
"CPUs are grouped according to SMT boundaries. This usually means that the "
"logical threads of each physical CPU core are grouped together."
msgstr ""

#: ../../../core-api/workqueue.rst:402
msgid "``cache``"
msgstr ""

#: ../../../core-api/workqueue.rst:403
msgid ""
"CPUs are grouped according to cache boundaries. Which specific cache "
"boundary is used is determined by the arch code. L3 is used in a lot of "
"cases. This is the default affinity scope."
msgstr ""

#: ../../../core-api/workqueue.rst:407
msgid "``numa``"
msgstr ""

#: ../../../core-api/workqueue.rst:408
msgid "CPUs are grouped according to NUMA boundaries."
msgstr ""

#: ../../../core-api/workqueue.rst:410
msgid "``system``"
msgstr ""

#: ../../../core-api/workqueue.rst:411
msgid ""
"All CPUs are put in the same group. Workqueue makes no effort to process a "
"work item on a CPU close to the issuing CPU."
msgstr ""

#: ../../../core-api/workqueue.rst:414
msgid ""
"The default affinity scope can be changed with the module parameter "
"``workqueue.default_affinity_scope`` and a specific workqueue's affinity "
"scope can be changed using ``apply_workqueue_attrs()``."
msgstr ""

#: ../../../core-api/workqueue.rst:418
msgid ""
"If ``WQ_SYSFS`` is set, the workqueue will have the following affinity scope "
"related interface files under its ``/sys/devices/virtual/workqueue/WQ_NAME/"
"`` directory."
msgstr ""

#: ../../../core-api/workqueue.rst:422
msgid "``affinity_scope``"
msgstr ""

#: ../../../core-api/workqueue.rst:423
msgid "Read to see the current affinity scope. Write to change."
msgstr ""

#: ../../../core-api/workqueue.rst:425
msgid ""
"When default is the current scope, reading this file will also show the "
"current effective scope in parentheses, for example, ``default (cache)``."
msgstr ""

#: ../../../core-api/workqueue.rst:428
msgid "``affinity_strict``"
msgstr ""

#: ../../../core-api/workqueue.rst:429
msgid ""
"0 by default indicating that affinity scopes are not strict. When a work "
"item starts execution, workqueue makes a best-effort attempt to ensure that "
"the worker is inside its affinity scope, which is called repatriation. Once "
"started, the scheduler is free to move the worker anywhere in the system as "
"it sees fit. This enables benefiting from scope locality while still being "
"able to utilize other CPUs if necessary and available."
msgstr ""

#: ../../../core-api/workqueue.rst:437
msgid ""
"If set to 1, all workers of the scope are guaranteed always to be in the "
"scope. This may be useful when crossing affinity scopes has other "
"implications, for example, in terms of power consumption or workload "
"isolation. Strict NUMA scope can also be used to match the workqueue "
"behavior of older kernels."
msgstr ""

#: ../../../core-api/workqueue.rst:445
msgid "Affinity Scopes and Performance"
msgstr ""

#: ../../../core-api/workqueue.rst:447
msgid ""
"It'd be ideal if an unbound workqueue's behavior is optimal for vast "
"majority of use cases without further tuning. Unfortunately, in the current "
"kernel, there exists a pronounced trade-off between locality and utilization "
"necessitating explicit configurations when workqueues are heavily used."
msgstr ""

#: ../../../core-api/workqueue.rst:452
msgid ""
"Higher locality leads to higher efficiency where more work is performed for "
"the same number of consumed CPU cycles. However, higher locality may also "
"cause lower overall system utilization if the work items are not spread "
"enough across the affinity scopes by the issuers. The following performance "
"testing with dm-crypt clearly illustrates this trade-off."
msgstr ""

#: ../../../core-api/workqueue.rst:458
msgid ""
"The tests are run on a CPU with 12-cores/24-threads split across four L3 "
"caches (AMD Ryzen 9 3900x). CPU clock boost is turned off for consistency. "
"``/dev/dm-0`` is a dm-crypt device created on NVME SSD (Samsung 990 PRO) and "
"opened with ``cryptsetup`` with default settings."
msgstr ""

#: ../../../core-api/workqueue.rst:465
msgid "Scenario 1: Enough issuers and work spread across the machine"
msgstr ""

#: ../../../core-api/workqueue.rst:467 ../../../core-api/workqueue.rst:509
#: ../../../core-api/workqueue.rst:552
msgid "The command used: ::"
msgstr ""

#: ../../../core-api/workqueue.rst:473
msgid ""
"There are 24 issuers, each issuing 64 IOs concurrently. ``--verify=sha512`` "
"makes ``fio`` generate and read back the content each time which makes "
"execution locality matter between the issuer and ``kcryptd``. The following "
"are the read bandwidths and CPU utilizations depending on different affinity "
"scope settings on ``kcryptd`` measured over five runs. Bandwidths are in "
"MiBps, and CPU util in percents."
msgstr ""

#: ../../../core-api/workqueue.rst:484 ../../../core-api/workqueue.rst:523
#: ../../../core-api/workqueue.rst:566
msgid "Affinity"
msgstr ""

#: ../../../core-api/workqueue.rst:485 ../../../core-api/workqueue.rst:524
#: ../../../core-api/workqueue.rst:567
msgid "Bandwidth (MiBps)"
msgstr ""

#: ../../../core-api/workqueue.rst:486 ../../../core-api/workqueue.rst:525
#: ../../../core-api/workqueue.rst:568
msgid "CPU util (%)"
msgstr ""

#: ../../../core-api/workqueue.rst:488 ../../../core-api/workqueue.rst:527
#: ../../../core-api/workqueue.rst:570
msgid "system"
msgstr ""

#: ../../../core-api/workqueue.rst:489
msgid "1159.40 ±1.34"
msgstr ""

#: ../../../core-api/workqueue.rst:490
msgid "99.31 ±0.02"
msgstr ""

#: ../../../core-api/workqueue.rst:492 ../../../core-api/workqueue.rst:531
#: ../../../core-api/workqueue.rst:574
msgid "cache"
msgstr ""

#: ../../../core-api/workqueue.rst:493
msgid "1166.40 ±0.89"
msgstr ""

#: ../../../core-api/workqueue.rst:494
msgid "99.34 ±0.01"
msgstr ""

#: ../../../core-api/workqueue.rst:496 ../../../core-api/workqueue.rst:535
#: ../../../core-api/workqueue.rst:578
msgid "cache (strict)"
msgstr ""

#: ../../../core-api/workqueue.rst:497
msgid "1166.00 ±0.71"
msgstr ""

#: ../../../core-api/workqueue.rst:498
msgid "99.35 ±0.01"
msgstr ""

#: ../../../core-api/workqueue.rst:500
msgid ""
"With enough issuers spread across the system, there is no downside to "
"\"cache\", strict or otherwise. All three configurations saturate the whole "
"machine but the cache-affine ones outperform by 0.6% thanks to improved "
"locality."
msgstr ""

#: ../../../core-api/workqueue.rst:507
msgid "Scenario 2: Fewer issuers, enough work for saturation"
msgstr ""

#: ../../../core-api/workqueue.rst:515
msgid ""
"The only difference from the previous scenario is ``--numjobs=8``. There are "
"a third of the issuers but is still enough total work to saturate the system."
msgstr ""

#: ../../../core-api/workqueue.rst:528
msgid "1155.40 ±0.89"
msgstr ""

#: ../../../core-api/workqueue.rst:529
msgid "97.41 ±0.05"
msgstr ""

#: ../../../core-api/workqueue.rst:532
msgid "1154.40 ±1.14"
msgstr ""

#: ../../../core-api/workqueue.rst:533
msgid "96.15 ±0.09"
msgstr ""

#: ../../../core-api/workqueue.rst:536
msgid "1112.00 ±4.64"
msgstr ""

#: ../../../core-api/workqueue.rst:537
msgid "93.26 ±0.35"
msgstr ""

#: ../../../core-api/workqueue.rst:539
msgid ""
"This is more than enough work to saturate the system. Both \"system\" and "
"\"cache\" are nearly saturating the machine but not fully. \"cache\" is "
"using less CPU but the better efficiency puts it at the same bandwidth as "
"\"system\"."
msgstr ""

#: ../../../core-api/workqueue.rst:544
msgid ""
"Eight issuers moving around over four L3 cache scope still allow \"cache "
"(strict)\" to mostly saturate the machine but the loss of work conservation "
"is now starting to hurt with 3.7% bandwidth loss."
msgstr ""

#: ../../../core-api/workqueue.rst:550
msgid "Scenario 3: Even fewer issuers, not enough work to saturate"
msgstr ""

#: ../../../core-api/workqueue.rst:558
msgid ""
"Again, the only difference is ``--numjobs=4``. With the number of issuers "
"reduced to four, there now isn't enough work to saturate the whole system "
"and the bandwidth becomes dependent on completion latencies."
msgstr ""

#: ../../../core-api/workqueue.rst:571
msgid "993.60 ±1.82"
msgstr ""

#: ../../../core-api/workqueue.rst:572
msgid "75.49 ±0.06"
msgstr ""

#: ../../../core-api/workqueue.rst:575
msgid "973.40 ±1.52"
msgstr ""

#: ../../../core-api/workqueue.rst:576
msgid "74.90 ±0.07"
msgstr ""

#: ../../../core-api/workqueue.rst:579
msgid "828.20 ±4.49"
msgstr ""

#: ../../../core-api/workqueue.rst:580
msgid "66.84 ±0.29"
msgstr ""

#: ../../../core-api/workqueue.rst:582
msgid ""
"Now, the tradeoff between locality and utilization is clearer. \"cache\" "
"shows 2% bandwidth loss compared to \"system\" and \"cache (struct)\" "
"whopping 20%."
msgstr ""

#: ../../../core-api/workqueue.rst:587
msgid "Conclusion and Recommendations"
msgstr ""

#: ../../../core-api/workqueue.rst:589
msgid ""
"In the above experiments, the efficiency advantage of the \"cache\" affinity "
"scope over \"system\" is, while consistent and noticeable, small. However, "
"the impact is dependent on the distances between the scopes and may be more "
"pronounced in processors with more complex topologies."
msgstr ""

#: ../../../core-api/workqueue.rst:594
msgid ""
"While the loss of work-conservation in certain scenarios hurts, it is a lot "
"better than \"cache (strict)\" and maximizing workqueue utilization is "
"unlikely to be the common case anyway. As such, \"cache\" is the default "
"affinity scope for unbound pools."
msgstr ""

#: ../../../core-api/workqueue.rst:599
msgid ""
"As there is no one option which is great for most cases, workqueue usages "
"that may consume a significant amount of CPU are recommended to configure "
"the workqueues using ``apply_workqueue_attrs()`` and/or enable ``WQ_SYSFS``."
msgstr ""

#: ../../../core-api/workqueue.rst:604
msgid ""
"An unbound workqueue with strict \"cpu\" affinity scope behaves the same as "
"``WQ_CPU_INTENSIVE`` per-cpu workqueue. There is no real advanage to the "
"latter and an unbound workqueue provides a lot more flexibility."
msgstr ""

#: ../../../core-api/workqueue.rst:608
msgid ""
"Affinity scopes are introduced in Linux v6.5. To emulate the previous "
"behavior, use strict \"numa\" affinity scope."
msgstr ""

#: ../../../core-api/workqueue.rst:611
msgid ""
"The loss of work-conservation in non-strict affinity scopes is likely "
"originating from the scheduler. There is no theoretical reason why the "
"kernel wouldn't be able to do the right thing and maintain work-conservation "
"in most cases. As such, it is possible that future scheduler improvements "
"may make most of these tunables unnecessary."
msgstr ""

#: ../../../core-api/workqueue.rst:619
msgid "Examining Configuration"
msgstr ""

#: ../../../core-api/workqueue.rst:621
msgid ""
"Use tools/workqueue/wq_dump.py to examine unbound CPU affinity "
"configuration, worker pools and how workqueues map to the pools: ::"
msgstr ""

#: ../../../core-api/workqueue.rst:692 ../../../core-api/workqueue.rst:723
msgid "See the command's help message for more info."
msgstr ""

#: ../../../core-api/workqueue.rst:696
msgid "Monitoring"
msgstr ""

#: ../../../core-api/workqueue.rst:698
msgid "Use tools/workqueue/wq_monitor.py to monitor workqueue operations: ::"
msgstr ""

#: ../../../core-api/workqueue.rst:727
msgid "Debugging"
msgstr ""

#: ../../../core-api/workqueue.rst:729
msgid ""
"Because the work functions are executed by generic worker threads there are "
"a few tricks needed to shed some light on misbehaving workqueue users."
msgstr ""

#: ../../../core-api/workqueue.rst:733
msgid "Worker threads show up in the process list as: ::"
msgstr ""

#: ../../../core-api/workqueue.rst:740
msgid ""
"If kworkers are going crazy (using too much cpu), there are two types of "
"possible problems:"
msgstr ""

#: ../../../core-api/workqueue.rst:743
msgid "Something being scheduled in rapid succession"
msgstr ""

#: ../../../core-api/workqueue.rst:744
msgid "A single work item that consumes lots of cpu cycles"
msgstr ""

#: ../../../core-api/workqueue.rst:746
msgid "The first one can be tracked using tracing: ::"
msgstr ""

#: ../../../core-api/workqueue.rst:753
msgid ""
"If something is busy looping on work queueing, it would be dominating the "
"output and the offender can be determined with the work item function."
msgstr ""

#: ../../../core-api/workqueue.rst:757
msgid ""
"For the second type of problems it should be possible to just check the "
"stack trace of the offending worker thread. ::"
msgstr ""

#: ../../../core-api/workqueue.rst:762
msgid ""
"The work item's function should be trivially visible in the stack trace."
msgstr ""

#: ../../../core-api/workqueue.rst:767
msgid "Non-reentrance Conditions"
msgstr ""

#: ../../../core-api/workqueue.rst:769
msgid ""
"Workqueue guarantees that a work item cannot be re-entrant if the following "
"conditions hold after a work item gets queued:"
msgstr ""

#: ../../../core-api/workqueue.rst:772
msgid "The work function hasn't been changed."
msgstr ""

#: ../../../core-api/workqueue.rst:773
msgid "No one queues the work item to another workqueue."
msgstr ""

#: ../../../core-api/workqueue.rst:774
msgid "The work item hasn't been reinitiated."
msgstr ""

#: ../../../core-api/workqueue.rst:776
msgid ""
"In other words, if the above conditions hold, the work item is guaranteed to "
"be executed by at most one worker system-wide at any given time."
msgstr ""

#: ../../../core-api/workqueue.rst:779
msgid ""
"Note that requeuing the work item (to the same queue) in the self function "
"doesn't break these conditions, so it's safe to do. Otherwise, caution is "
"required when breaking the conditions inside a work function."
msgstr ""

#: ../../../core-api/workqueue.rst:785
msgid "Kernel Inline Documentations Reference"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:143
msgid "A struct for workqueue attributes."
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:147
msgid "**Definition**::"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:158
msgid "**Members**"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:149
msgid "``nice``"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:150
msgid "nice level"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:154
msgid "``cpumask``"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:155
msgid "allowed CPUs"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:157
msgid ""
"Work items in this workqueue are affine to these CPUs and not allowed to "
"execute on other CPUs. A pool serving a workqueue must have the same "
"**cpumask**."
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:163
msgid "``__pod_cpumask``"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:164
msgid "internal attribute used to create per-pod pools"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:166
msgid "Internal use only."
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:168
msgid ""
"Per-pod unbound worker pools are used to improve locality. Always a subset "
"of ->cpumask. A workqueue can be associated with multiple worker pools with "
"disjoint **__pod_cpumask**'s. Whether the enforcement of a pool's "
"**__pod_cpumask** is strict depends on **affn_strict**."
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:175
msgid "``affn_strict``"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:176
msgid "affinity scope is strict"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:178
msgid ""
"If clear, workqueue will make a best-effort attempt at starting the worker "
"inside **__pod_cpumask** but the scheduler is free to migrate it outside."
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:182
msgid "If set, workers are only allowed to run inside **__pod_cpumask**."
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:195
msgid "``affn_scope``"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:196
msgid "unbound CPU affinity scope"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:198
msgid ""
"CPU pods are used to improve execution locality of unbound work items. There "
"are multiple pod types, one for each wq_affn_scope, and every CPU in the "
"system belongs to one pod in every pod type. CPUs that belong to the same "
"pod share the worker pool. For example, selecting ``WQ_AFFN_NUMA`` makes the "
"workqueue use a separate worker pool for each NUMA node."
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:207
msgid "``ordered``"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:208
msgid "work items must be executed one by one in queueing order"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:211
#: ../include/linux/workqueue.h:482 ../include/linux/workqueue.h:524
#: ../include/linux/workqueue.h:544 ../include/linux/workqueue.h:562
#: ../include/linux/workqueue.h:648 ../include/linux/workqueue.h:678
#: ../include/linux/workqueue.h:693 ../include/linux/workqueue.h:707
#: ../include/linux/workqueue.h:718 ../include/linux/workqueue.h:738
#: ../include/linux/workqueue.h:804 ../include/linux/workqueue.h:818
#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3
#: ../kernel/workqueue.c:562 ../kernel/workqueue.c:579
#: ../kernel/workqueue.c:594 ../kernel/workqueue.c:706
#: ../kernel/workqueue.c:745 ../kernel/workqueue.c:871
#: ../kernel/workqueue.c:973 ../kernel/workqueue.c:995
#: ../kernel/workqueue.c:1029 ../kernel/workqueue.c:1063
#: ../kernel/workqueue.c:1084 ../kernel/workqueue.c:1138
#: ../kernel/workqueue.c:1176 ../kernel/workqueue.c:1249
#: ../kernel/workqueue.c:1402 ../kernel/workqueue.c:1435
#: ../kernel/workqueue.c:1481 ../kernel/workqueue.c:1531
#: ../kernel/workqueue.c:1563 ../kernel/workqueue.c:1588
#: ../kernel/workqueue.c:1639 ../kernel/workqueue.c:1653
#: ../kernel/workqueue.c:1672 ../kernel/workqueue.c:1723
#: ../kernel/workqueue.c:1798 ../kernel/workqueue.c:1822
#: ../kernel/workqueue.c:1863 ../kernel/workqueue.c:1943
#: ../kernel/workqueue.c:1994 ../kernel/workqueue.c:2041
#: ../kernel/workqueue.c:2158 ../kernel/workqueue.c:2187
#: ../kernel/workqueue.c:2387 ../kernel/workqueue.c:2418
#: ../kernel/workqueue.c:2449 ../kernel/workqueue.c:2552
#: ../kernel/workqueue.c:2590 ../kernel/workqueue.c:2684
#: ../kernel/workqueue.c:2738 ../kernel/workqueue.c:2782
#: ../kernel/workqueue.c:2883 ../kernel/workqueue.c:2917
#: ../kernel/workqueue.c:2955 ../kernel/workqueue.c:3050
#: ../kernel/workqueue.c:3124 ../kernel/workqueue.c:3165
#: ../kernel/workqueue.c:3339 ../kernel/workqueue.c:3376
#: ../kernel/workqueue.c:3518 ../kernel/workqueue.c:3777
#: ../kernel/workqueue.c:3826 ../kernel/workqueue.c:3902
#: ../kernel/workqueue.c:4017 ../kernel/workqueue.c:4174
#: ../kernel/workqueue.c:4338 ../kernel/workqueue.c:4356
#: ../kernel/workqueue.c:4468 ../kernel/workqueue.c:4492
#: ../kernel/workqueue.c:4514 ../kernel/workqueue.c:4529
#: ../kernel/workqueue.c:4547 ../kernel/workqueue.c:4566
#: ../kernel/workqueue.c:4593 ../kernel/workqueue.c:4606
#: ../kernel/workqueue.c:4619 ../kernel/workqueue.c:4631
#: ../kernel/workqueue.c:4670 ../kernel/workqueue.c:4694
#: ../kernel/workqueue.c:4840 ../kernel/workqueue.c:5006
#: ../kernel/workqueue.c:5085 ../kernel/workqueue.c:5291
#: ../kernel/workqueue.c:5474 ../kernel/workqueue.c:5501
#: ../kernel/workqueue.c:5695 ../kernel/workqueue.c:5936
#: ../kernel/workqueue.c:6029 ../kernel/workqueue.c:6063
#: ../kernel/workqueue.c:6121 ../kernel/workqueue.c:6158
#: ../kernel/workqueue.c:6193 ../kernel/workqueue.c:6216
#: ../kernel/workqueue.c:6650 ../kernel/workqueue.c:6707
#: ../kernel/workqueue.c:6843 ../kernel/workqueue.c:7028
#: ../kernel/workqueue.c:7361 ../kernel/workqueue.c:7466
#: ../kernel/workqueue.c:7533
msgid "**Description**"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:144
msgid "This can be used to change attributes of an unbound workqueue."
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:354
msgid "``work_pending (work)``"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:352
msgid "Find out whether a work item is currently pending"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:356
#: ../include/linux/workqueue.h:363 ../include/linux/workqueue.h:480
#: ../include/linux/workqueue.h:521 ../include/linux/workqueue.h:540
#: ../include/linux/workqueue.h:561 ../include/linux/workqueue.h:648
#: ../include/linux/workqueue.h:677 ../include/linux/workqueue.h:692
#: ../include/linux/workqueue.h:707 ../include/linux/workqueue.h:719
#: ../include/linux/workqueue.h:738 ../include/linux/workqueue.h:803
#: ../include/linux/workqueue.h:818 ../../../core-api/workqueue:789:
#: ../kernel/workqueue.c:562 ../kernel/workqueue.c:579
#: ../kernel/workqueue.c:594 ../kernel/workqueue.c:707
#: ../kernel/workqueue.c:746 ../kernel/workqueue.c:872
#: ../kernel/workqueue.c:973 ../kernel/workqueue.c:995
#: ../kernel/workqueue.c:1030 ../kernel/workqueue.c:1064
#: ../kernel/workqueue.c:1084 ../kernel/workqueue.c:1137
#: ../kernel/workqueue.c:1175 ../kernel/workqueue.c:1250
#: ../kernel/workqueue.c:1403 ../kernel/workqueue.c:1436
#: ../kernel/workqueue.c:1482 ../kernel/workqueue.c:1532
#: ../kernel/workqueue.c:1563 ../kernel/workqueue.c:1588
#: ../kernel/workqueue.c:1640 ../kernel/workqueue.c:1654
#: ../kernel/workqueue.c:1673 ../kernel/workqueue.c:1723
#: ../kernel/workqueue.c:1798 ../kernel/workqueue.c:1823
#: ../kernel/workqueue.c:1863 ../kernel/workqueue.c:1944
#: ../kernel/workqueue.c:1994 ../kernel/workqueue.c:2040
#: ../kernel/workqueue.c:2157 ../kernel/workqueue.c:2185
#: ../kernel/workqueue.c:2386 ../kernel/workqueue.c:2419
#: ../kernel/workqueue.c:2448 ../kernel/workqueue.c:2550
#: ../kernel/workqueue.c:2588 ../kernel/workqueue.c:2632
#: ../kernel/workqueue.c:2684 ../kernel/workqueue.c:2739
#: ../kernel/workqueue.c:2783 ../kernel/workqueue.c:2883
#: ../kernel/workqueue.c:2918 ../kernel/workqueue.c:2956
#: ../kernel/workqueue.c:3051 ../kernel/workqueue.c:3125
#: ../kernel/workqueue.c:3165 ../kernel/workqueue.c:3340
#: ../kernel/workqueue.c:3377 ../kernel/workqueue.c:3519
#: ../kernel/workqueue.c:3776 ../kernel/workqueue.c:3824
#: ../kernel/workqueue.c:3901 ../kernel/workqueue.c:4018
#: ../kernel/workqueue.c:4175 ../kernel/workqueue.c:4339
#: ../kernel/workqueue.c:4357 ../kernel/workqueue.c:4379
#: ../kernel/workqueue.c:4469 ../kernel/workqueue.c:4493
#: ../kernel/workqueue.c:4515 ../kernel/workqueue.c:4530
#: ../kernel/workqueue.c:4548 ../kernel/workqueue.c:4567
#: ../kernel/workqueue.c:4594 ../kernel/workqueue.c:4607
#: ../kernel/workqueue.c:4620 ../kernel/workqueue.c:4632
#: ../kernel/workqueue.c:4669 ../kernel/workqueue.c:4695
#: ../kernel/workqueue.c:4710 ../kernel/workqueue.c:4841
#: ../kernel/workqueue.c:5007 ../kernel/workqueue.c:5086
#: ../kernel/workqueue.c:5291 ../kernel/workqueue.c:5474
#: ../kernel/workqueue.c:5501 ../kernel/workqueue.c:5696
#: ../kernel/workqueue.c:5937 ../kernel/workqueue.c:6029
#: ../kernel/workqueue.c:6063 ../kernel/workqueue.c:6090
#: ../kernel/workqueue.c:6106 ../kernel/workqueue.c:6121
#: ../kernel/workqueue.c:6159 ../kernel/workqueue.c:6193
#: ../kernel/workqueue.c:6216 ../kernel/workqueue.c:6409
#: ../kernel/workqueue.c:6453 ../kernel/workqueue.c:6502
#: ../kernel/workqueue.c:6526 ../kernel/workqueue.c:6651
#: ../kernel/workqueue.c:6707 ../kernel/workqueue.c:6841
#: ../kernel/workqueue.c:6869 ../kernel/workqueue.c:6897
#: ../kernel/workqueue.c:6943 ../kernel/workqueue.c:7029
#: ../kernel/workqueue.c:7362 ../kernel/workqueue.c:7467
#: ../kernel/workqueue.c:7534 ../kernel/workqueue.c:7839
#: ../kernel/workqueue.c:7996 ../kernel/workqueue.c:8120
msgid "**Parameters**"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:358
msgid "``work``"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:353
#: ../include/linux/workqueue.h:361
msgid "The work item in question"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:361
msgid "``delayed_work_pending (w)``"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:359
msgid "Find out whether a delayable work item is currently pending"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:365
msgid "``w``"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:476
msgid "allocate a workqueue"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:482
#: ../include/linux/workqueue.h:523 ../../../core-api/workqueue:789:
#: ../kernel/workqueue.c:6195
msgid "``const char *fmt``"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:477
#: ../include/linux/workqueue.h:518 ../include/linux/workqueue.h:539
#: ../include/linux/workqueue.h:558
msgid "printf format for the name of the workqueue"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:479
#: ../include/linux/workqueue.h:520 ../../../core-api/workqueue:789:
#: ../kernel/workqueue.c:972 ../kernel/workqueue.c:994
msgid "``unsigned int flags``"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:478
#: ../include/linux/workqueue.h:519
msgid "WQ_* flags"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:480
#: ../include/linux/workqueue.h:521 ../../../core-api/workqueue:789:
#: ../kernel/workqueue.c:6028
msgid "``int max_active``"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:479
#: ../include/linux/workqueue.h:520
msgid "max in-flight work items, 0 for default"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:481
#: ../include/linux/workqueue.h:523 ../../../core-api/workqueue:789:
#: ../kernel/workqueue.c:6192
msgid "``...``"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:480
#: ../include/linux/workqueue.h:522 ../include/linux/workqueue.h:542
#: ../include/linux/workqueue.h:560
msgid "args for **fmt**"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:481
msgid ""
"For a per-cpu workqueue, **max_active** limits the number of in-flight work "
"items for each CPU. e.g. **max_active** of 1 indicates that each CPU can be "
"executing at most one work item for the workqueue."
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:485
msgid ""
"For unbound workqueues, **max_active** limits the number of in-flight work "
"items for the whole system. e.g. **max_active** of 16 indicates that there "
"can be at most 16 work items executing for the workqueue in the whole system."
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:489
msgid ""
"As sharing the same active counter for an unbound workqueue across multiple "
"NUMA nodes can be expensive, **max_active** is distributed to each NUMA node "
"according to the proportion of the number of online CPUs and enforced "
"independently."
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:494
msgid ""
"Depending on online CPU distribution, a node may end up with per-node "
"max_active which is significantly lower than **max_active**, which can lead "
"to deadlocks if the per-node concurrency limit is lower than the maximum "
"number of interdependent work items for the workqueue."
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:499
msgid ""
"To guarantee forward progress regardless of online CPU distribution, the "
"concurrency limit on every node is guaranteed to be equal to or greater than "
"min_active which is set to min(**max_active**, ``WQ_DFL_MIN_ACTIVE``). This "
"means that the sum of per-node max_active's may be larger than "
"**max_active**."
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:504
msgid ""
"For detailed information on ``WQ_``\\* flags, please refer to Documentation/"
"core-api/workqueue.rst."
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:507
#: ../include/linux/workqueue.h:527 ../include/linux/workqueue.h:547
#: ../include/linux/workqueue.h:565 ../../../core-api/workqueue:789:
#: ../kernel/workqueue.c:879 ../kernel/workqueue.c:1107
#: ../kernel/workqueue.c:1536 ../kernel/workqueue.c:2392
#: ../kernel/workqueue.c:2460 ../kernel/workqueue.c:2557
#: ../kernel/workqueue.c:2598 ../kernel/workqueue.c:2632
#: ../kernel/workqueue.c:2786 ../kernel/workqueue.c:3135
#: ../kernel/workqueue.c:3381 ../kernel/workqueue.c:3532
#: ../kernel/workqueue.c:3922 ../kernel/workqueue.c:4340
#: ../kernel/workqueue.c:4359 ../kernel/workqueue.c:4378
#: ../kernel/workqueue.c:4493 ../kernel/workqueue.c:4515
#: ../kernel/workqueue.c:4634 ../kernel/workqueue.c:4672
#: ../kernel/workqueue.c:4710 ../kernel/workqueue.c:4841
#: ../kernel/workqueue.c:5091 ../kernel/workqueue.c:5481
#: ../kernel/workqueue.c:6090 ../kernel/workqueue.c:6106
#: ../kernel/workqueue.c:6131 ../kernel/workqueue.c:6161
#: ../kernel/workqueue.c:6845 ../kernel/workqueue.c:6900
#: ../kernel/workqueue.c:7031 ../kernel/workqueue.c:7364
#: ../kernel/workqueue.c:7474
msgid "**Return**"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:508
#: ../include/linux/workqueue.h:528 ../include/linux/workqueue.h:548
#: ../include/linux/workqueue.h:566
msgid "Pointer to the allocated workqueue on success, ``NULL`` on failure."
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:517
msgid "allocate a workqueue with user-defined lockdep_map"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:522
msgid "``struct lockdep_map *lockdep_map``"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:521
#: ../include/linux/workqueue.h:541
msgid "user-defined lockdep_map"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:523
msgid ""
"Same as alloc_workqueue but with the a user-define lockdep_map. Useful for "
"workqueues created with the same purpose and to avoid leaking a lockdep_map "
"on each workqueue creation."
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:538
msgid ""
"``alloc_ordered_workqueue_lockdep_map (fmt, flags, lockdep_map, args...)``"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:536
msgid "allocate an ordered workqueue with user-defined lockdep_map"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:542
#: ../include/linux/workqueue.h:563
msgid "``fmt``"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:540
#: ../include/linux/workqueue.h:559
msgid "WQ_* flags (only WQ_FREEZABLE and WQ_MEM_RECLAIM are meaningful)"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:542
msgid "``lockdep_map``"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:543
#: ../include/linux/workqueue.h:561
msgid "``args...``"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:543
msgid ""
"Same as alloc_ordered_workqueue but with the a user-define lockdep_map. "
"Useful for workqueues created with the same purpose and to avoid leaking a "
"lockdep_map on each workqueue creation."
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:559
msgid "``alloc_ordered_workqueue (fmt, flags, args...)``"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:557
msgid "allocate an ordered workqueue"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:561
msgid ""
"Allocate an ordered workqueue.  An ordered workqueue executes at most one "
"work item at any given time in the queued order.  They are implemented as "
"unbound workqueues with **max_active** of one."
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:644
msgid "queue work on a workqueue"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:650
#: ../include/linux/workqueue.h:679 ../include/linux/workqueue.h:694
#: ../include/linux/workqueue.h:740 ../../../core-api/workqueue:789:
#: ../kernel/workqueue.c:748 ../kernel/workqueue.c:1565
#: ../kernel/workqueue.c:1590 ../kernel/workqueue.c:1825
#: ../kernel/workqueue.c:2385 ../kernel/workqueue.c:2447
#: ../kernel/workqueue.c:2549 ../kernel/workqueue.c:2587
#: ../kernel/workqueue.c:2634 ../kernel/workqueue.c:3903
#: ../kernel/workqueue.c:4020 ../kernel/workqueue.c:4177
#: ../kernel/workqueue.c:5476 ../kernel/workqueue.c:5503
#: ../kernel/workqueue.c:5698 ../kernel/workqueue.c:5939
#: ../kernel/workqueue.c:6031 ../kernel/workqueue.c:6065
#: ../kernel/workqueue.c:6120 ../kernel/workqueue.c:6411
#: ../kernel/workqueue.c:7469 ../kernel/workqueue.c:7536
msgid "``struct workqueue_struct *wq``"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:645
#: ../include/linux/workqueue.h:674 ../include/linux/workqueue.h:689
#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2384
#: ../kernel/workqueue.c:2446 ../kernel/workqueue.c:2548
#: ../kernel/workqueue.c:2586 ../kernel/workqueue.c:2629
msgid "workqueue to use"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:647
#: ../include/linux/workqueue.h:706 ../include/linux/workqueue.h:721
#: ../include/linux/workqueue.h:737 ../../../core-api/workqueue:789:
#: ../kernel/workqueue.c:874 ../kernel/workqueue.c:1083
#: ../kernel/workqueue.c:1139 ../kernel/workqueue.c:1177
#: ../kernel/workqueue.c:2042 ../kernel/workqueue.c:2159
#: ../kernel/workqueue.c:2184 ../kernel/workqueue.c:2386
#: ../kernel/workqueue.c:2448 ../kernel/workqueue.c:2958
#: ../kernel/workqueue.c:3164 ../kernel/workqueue.c:4341
#: ../kernel/workqueue.c:4471 ../kernel/workqueue.c:4532
#: ../kernel/workqueue.c:4550 ../kernel/workqueue.c:4569
#: ../kernel/workqueue.c:6161
msgid "``struct work_struct *work``"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:646
#: ../include/linux/workqueue.h:690 ../../../core-api/workqueue:789:
#: ../kernel/workqueue.c:2385 ../kernel/workqueue.c:2447
#: ../kernel/workqueue.c:2549 ../kernel/workqueue.c:2587
#: ../kernel/workqueue.c:2630
msgid "work to queue"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:647
msgid ""
"Returns ``false`` if **work** was already on a queue, ``true`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:649
msgid ""
"We queue the work to the CPU on which it was submitted, but if the CPU dies "
"it can be processed by another CPU."
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:652
msgid ""
"Memory-ordering properties:  If it returns ``true``, guarantees that all "
"stores preceding the call to queue_work() in the program order will be "
"visible from the CPU which will execute **work** by the time such work "
"executes, e.g.,"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:656
msgid "{ x is initially 0 }"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:658
msgid "CPU0                               CPU1"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:660
msgid ""
"WRITE_ONCE(x, 1);                  [ **work** is being executed ] r0 = "
"queue_work(wq, work);           r1 = READ_ONCE(x);"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:663
msgid "Forbids: r0 == true && r1 == 0"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:673
msgid "queue work on a workqueue after delay"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:676
#: ../include/linux/workqueue.h:691 ../include/linux/workqueue.h:802
#: ../include/linux/workqueue.h:820 ../../../core-api/workqueue:789:
#: ../kernel/workqueue.c:2550 ../kernel/workqueue.c:2588
#: ../kernel/workqueue.c:4359 ../kernel/workqueue.c:4495
#: ../kernel/workqueue.c:4517 ../kernel/workqueue.c:4596
#: ../kernel/workqueue.c:4609 ../kernel/workqueue.c:4622
msgid "``struct delayed_work *dwork``"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:675
msgid "delayable work to queue"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:677
#: ../include/linux/workqueue.h:692 ../include/linux/workqueue.h:803
#: ../include/linux/workqueue.h:817 ../../../core-api/workqueue:789:
#: ../kernel/workqueue.c:2551 ../kernel/workqueue.c:2589
msgid "``unsigned long delay``"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:676
#: ../include/linux/workqueue.h:691 ../../../core-api/workqueue:789:
#: ../kernel/workqueue.c:2550 ../kernel/workqueue.c:2588
msgid "number of jiffies to wait before queueing"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:677
msgid "Equivalent to queue_delayed_work_on() but tries to use the local CPU."
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:688
msgid "modify delay of or queue a delayed work"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:692
msgid "mod_delayed_work_on() on local CPU."
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:703
msgid "put work task on a specific cpu"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:709
#: ../include/linux/workqueue.h:805 ../../../core-api/workqueue:789:
#: ../kernel/workqueue.c:2388 ../kernel/workqueue.c:2552
#: ../kernel/workqueue.c:2590 ../kernel/workqueue.c:5290
#: ../kernel/workqueue.c:5500 ../kernel/workqueue.c:6123
#: ../kernel/workqueue.c:6706 ../kernel/workqueue.c:6843
msgid "``int cpu``"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:704
msgid "cpu to put the work task on"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:705
#: ../include/linux/workqueue.h:716 ../include/linux/workqueue.h:801
#: ../include/linux/workqueue.h:815
msgid "job to be done"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:706
msgid "This puts a job on a specific cpu"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:715
msgid "put work task in global workqueue"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:717
msgid ""
"Returns ``false`` if **work** was already on the kernel-global workqueue and "
"``true`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:720
msgid ""
"This puts a job in the kernel-global workqueue if it was not already queued "
"and leaves it in the same position on the kernel-global workqueue otherwise."
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:724
msgid ""
"Shares the same memory-ordering properties of queue_work(), cf. the DocBook "
"header of queue_work()."
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:734
msgid "Enable and queue a work item on a specific workqueue"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:735
msgid "The target workqueue"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:736
msgid "The work item to be enabled and queued"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:737
msgid ""
"This function combines the operations of enable_work() and queue_work(), "
"providing a convenient way to enable and queue a work item in a single call. "
"It invokes enable_work() on **work** and then queues it if the disable depth "
"reached 0. Returns ``true`` if the disable depth reached 0 and **work** is "
"queued, and ``false`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:743
msgid ""
"Note that **work** is always queued when disable depth reaches zero. If the "
"desired behavior is queueing only if certain events took place while "
"**work** is disabled, the user should implement the necessary state tracking "
"and perform explicit conditional queueing after enable_work()."
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:799
msgid "queue work in global workqueue on CPU after delay"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:800
msgid "cpu to use"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:802
msgid "number of jiffies to wait"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:803
msgid ""
"After waiting for a given time this puts a job in the kernel-global "
"workqueue on the specified CPU."
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:814
msgid "put work task in global workqueue after delay"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:816
msgid "number of jiffies to wait or 0 for immediate execution"
msgstr ""

#: ../../../core-api/workqueue:787: ../include/linux/workqueue.h:817
msgid ""
"After waiting for a given time this puts a job in the kernel-global "
"workqueue."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:560
msgid "``for_each_pool (pool, pi)``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:558
msgid "iterate through all worker_pools in the system"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:564
#: ../kernel/workqueue.c:578
msgid "``pool``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:559
#: ../kernel/workqueue.c:576 ../kernel/workqueue.c:591
msgid "iteration cursor"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:561
msgid "``pi``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:560
msgid "integer used for iteration"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:561
msgid ""
"This must be called either with wq_pool_mutex held or RCU read locked.  If "
"the pool needs to be used beyond the locking in effect, the caller is "
"responsible for guaranteeing that the pool stays online."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:565
#: ../kernel/workqueue.c:580 ../kernel/workqueue.c:597
msgid ""
"The if/else clause exists only for the lockdep assertion and can be ignored."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:577
msgid "``for_each_pool_worker (worker, pool)``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:575
msgid "iterate through all workers of a worker_pool"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:581
msgid "``worker``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:577
msgid "worker_pool to iterate workers of"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:578
msgid "This must be called with wq_pool_attach_mutex."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:592
msgid "``for_each_pwq (pwq, wq)``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:590
msgid "iterate through all pool_workqueues of the specified workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:596
msgid "``pwq``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:593
msgid "``wq``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:592
#: ../kernel/workqueue.c:5471 ../kernel/workqueue.c:5498
msgid "the target workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:593
msgid ""
"This must be called either with wq->mutex held or RCU read locked. If the "
"pwq needs to be used beyond the locking in effect, the caller is responsible "
"for guaranteeing that the pwq stays online."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:703
msgid "allocate ID and assign it to **pool**"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:709
#: ../kernel/workqueue.c:1086 ../kernel/workqueue.c:1252
#: ../kernel/workqueue.c:2683 ../kernel/workqueue.c:2785
#: ../kernel/workqueue.c:3053 ../kernel/workqueue.c:4843
#: ../kernel/workqueue.c:5009 ../kernel/workqueue.c:6455
#: ../kernel/workqueue.c:6653 ../kernel/workqueue.c:6709
msgid "``struct worker_pool *pool``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:704
msgid "the pool pointer of interest"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:705
msgid ""
"Returns 0 if ID in [0, WORK_OFFQ_POOL_NONE) is allocated and assigned "
"successfully, -errno on failure."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:742
msgid "effective cpumask of an unbound workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:743
#: ../kernel/workqueue.c:1560
msgid "workqueue of interest"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:744
msgid ""
"**wq->unbound_attrs->cpumask** contains the cpumask requested by the user "
"which is masked with wq_unbound_cpumask to determine the effective cpumask. "
"The default pwq is always mapped to the pool with the current effective "
"cpumask."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:868
msgid "return the worker_pool a given work was associated with"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:869
msgid "the work item of interest"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:870
msgid ""
"Pools are created and destroyed under wq_pool_mutex, and allows read access "
"under RCU read lock.  As such, this function should be called under "
"wq_pool_mutex or inside of a rcu_read_lock() region."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:874
msgid ""
"All fields of the returned pool are accessible as long as the above "
"mentioned locking is in effect.  If the returned pool needs to be used "
"beyond the critical section, the caller is responsible for ensuring the "
"returned pool is and stays online."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:880
msgid "The worker_pool **work** was last associated with.  ``NULL`` if none."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:969
msgid "set worker flags and adjust nr_running accordingly"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:975
#: ../kernel/workqueue.c:997 ../kernel/workqueue.c:1032
#: ../kernel/workqueue.c:1066 ../kernel/workqueue.c:1174
#: ../kernel/workqueue.c:2686 ../kernel/workqueue.c:2741
#: ../kernel/workqueue.c:2885 ../kernel/workqueue.c:3127
#: ../kernel/workqueue.c:3167 ../kernel/workqueue.c:3342
#: ../kernel/workqueue.c:3825
msgid "``struct worker *worker``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:970
#: ../kernel/workqueue.c:992 ../kernel/workqueue.c:3122
#: ../kernel/workqueue.c:3162 ../kernel/workqueue.c:3337
#: ../kernel/workqueue.c:3374 ../kernel/workqueue.c:3516
msgid "self"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:971
msgid "flags to set"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:972
msgid "Set **flags** in **worker->flags** and adjust nr_running accordingly."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:991
msgid "clear worker flags and adjust nr_running accordingly"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:993
msgid "flags to clear"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:994
msgid "Clear **flags** in **worker->flags** and adjust nr_running accordingly."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1026
msgid "enter idle state"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1027
msgid "worker which is entering idle state"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1028
msgid ""
"**worker** is entering idle state.  Update stats and idle timer if necessary."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1031
#: ../kernel/workqueue.c:1064
msgid "LOCKING: raw_spin_lock_irq(pool->lock)."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1060
msgid "leave idle state"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1061
msgid "worker which is leaving idle state"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1062
msgid "**worker** is leaving idle state.  Update stats."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1080
msgid "find worker which is executing a work"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1081
#: ../kernel/workqueue.c:6648
msgid "pool of interest"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1082
msgid "work to find worker for"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1083
msgid ""
"Find a worker which is executing **work** on **pool** by searching **pool-"
">busy_hash** which is keyed by the address of **work**.  For a worker to "
"match, its current execution should match the address of **work** and its "
"work function.  This is to avoid unwanted dependency between unrelated work "
"executions through a work item being recycled while still being executed."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1090
msgid ""
"This is a bit tricky.  A work item may be freed once its execution starts "
"and nothing prevents the freed area from being recycled for another work "
"item.  If the same work item address ends up being reused before the "
"original execution finishes, workqueue will identify the recycled work item "
"as currently executing and make it wait until the current execution "
"finishes, introducing an unwanted dependency."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1097
msgid ""
"This function checks the work item address and work function to avoid false "
"positives.  Note that this isn't complete as one may construct a work "
"function which can introduce dependency onto itself through a recycled work "
"item.  Well, if somebody wants to shoot oneself in the foot that badly, "
"there's only so much we can do, and if such deadlock actually occurs, it "
"should be easy to locate the culprit work function."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1104
#: ../kernel/workqueue.c:1142 ../kernel/workqueue.c:1544
#: ../kernel/workqueue.c:2001 ../kernel/workqueue.c:2189
#: ../kernel/workqueue.c:2783 ../kernel/workqueue.c:2885
#: ../kernel/workqueue.c:3131 ../kernel/workqueue.c:3170
#: ../kernel/workqueue.c:3342 ../kernel/workqueue.c:3839
#: ../kernel/workqueue.c:3919 ../kernel/workqueue.c:6031
#: ../kernel/workqueue.c:6870 ../kernel/workqueue.c:6897
#: ../kernel/workqueue.c:6943
msgid "**Context**"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1105
#: ../kernel/workqueue.c:1143 ../kernel/workqueue.c:2002
#: ../kernel/workqueue.c:2190 ../kernel/workqueue.c:2886
#: ../kernel/workqueue.c:3840
msgid "raw_spin_lock_irq(pool->lock)."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1108
msgid ""
"Pointer to worker which is executing **work** if found, ``NULL`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1133
msgid "move linked works to a list"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1134
msgid "start of series of works to be scheduled"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1136
#: ../kernel/workqueue.c:2185
msgid "``struct list_head *head``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1135
msgid "target list to append **work** to"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1137
#: ../kernel/workqueue.c:1175
msgid "``struct work_struct **nextp``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1136
#: ../kernel/workqueue.c:1174
msgid "out parameter for nested worklist walking"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1137
msgid ""
"Schedule linked works starting from **work** to **head**. Work series to be "
"scheduled starts at **work** and includes any consecutive work with "
"WORK_STRUCT_LINKED set in its predecessor. See assign_work() for details on "
"**nextp**."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1171
msgid "assign a work item and its linked work items to a worker"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1172
msgid "work to assign"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1173
msgid "worker to assign to"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1175
msgid ""
"Assign **work** and its linked work items to **worker**. If **work** is "
"already being executed by another worker in the same pool, it'll be punted "
"there."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1178
msgid ""
"If **nextp** is not NULL, it's updated to point to the next work of the last "
"scheduled work. This allows assign_work() to be nested inside "
"list_for_each_entry_safe()."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1182
msgid ""
"Returns ``true`` if **work** was successfully assigned to **worker**. "
"``false`` if **work** was punted to another worker already executing it."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1246
msgid "wake up an idle worker if necessary"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1247
msgid "pool to kick"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1248
msgid ""
"**pool** may have pending work items. Wake up worker if necessary. Returns "
"whether a worker was woken up."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1399
msgid "a worker is running again"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1405
#: ../kernel/workqueue.c:1438 ../kernel/workqueue.c:1484
#: ../kernel/workqueue.c:1534 ../kernel/workqueue.c:6215
msgid "``struct task_struct *task``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1400
msgid "task waking up"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1401
msgid "This function is called when a worker returns from schedule()"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1432
msgid "a worker is going to sleep"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1433
msgid "task going to sleep"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1434
msgid ""
"This function is called from schedule() when a busy worker is going to sleep."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1478
msgid "a scheduler tick occurred while a kworker is running"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1479
msgid "task currently running"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1480
msgid ""
"Called from sched_tick(). We're in the IRQ context and the current worker's "
"fields which follow the 'K' locking rule can be accessed safely."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1528
msgid "retrieve worker's last work function"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1529
msgid "Task to retrieve last work function of."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1530
msgid ""
"Determine the last function a worker executed. This is called from the "
"scheduler to get a worker's last known identity."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1534
msgid ""
"This function is called during schedule() when a kworker is going to sleep. "
"It's used by psi to identify aggregation workers during dequeuing, to allow "
"periodic aggregation to shut-off when that worker is the last task in the "
"system or cgroup to go to sleep."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1539
msgid ""
"As this function doesn't involve any workqueue-related locking, it only "
"returns stable values when called from inside the scheduler's queuing and "
"dequeuing paths, when **task**, which must be a kworker, is guaranteed to "
"not be processing any works."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1534
msgid "raw_spin_lock_irq(rq->lock)"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1547
msgid ""
"The last work function ``current`` executed as a worker, NULL if it hasn't "
"executed any work yet."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1559
msgid "Determine wq_node_nr_active to use"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1562
#: ../kernel/workqueue.c:2421 ../kernel/workqueue.c:2450
msgid "``int node``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1561
msgid "NUMA node, can be ``NUMA_NO_NODE``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1562
msgid "Determine wq_node_nr_active to use for **wq** on **node**. Returns:"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1564
msgid ""
"``NULL`` for per-cpu workqueues as they don't need to use shared nr_active."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1566
msgid "node_nr_active[nr_node_ids] if **node** is ``NUMA_NO_NODE``."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1568
msgid "Otherwise, node_nr_active[**node**]."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1584
msgid "Update per-node max_actives to use"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1585
msgid "workqueue to update"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1587
msgid "``int off_cpu``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1586
msgid "CPU that's going down, -1 if a CPU is not going down"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1587
msgid ""
"Update **wq->node_nr_active**[]->max. **wq** must be unbound. max_active is "
"distributed among nodes according to the proportions of numbers of online "
"cpus. The result is always between **wq->min_active** and max_active."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1636
msgid "get an extra reference on the specified pool_workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1642
#: ../kernel/workqueue.c:1656 ../kernel/workqueue.c:1675
#: ../kernel/workqueue.c:1725 ../kernel/workqueue.c:1800
#: ../kernel/workqueue.c:1946 ../kernel/workqueue.c:1996
#: ../kernel/workqueue.c:2187 ../kernel/workqueue.c:3826
msgid "``struct pool_workqueue *pwq``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1637
msgid "pool_workqueue to get"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1638
msgid ""
"Obtain an extra reference on **pwq**.  The caller should guarantee that "
"**pwq** has positive refcnt and be holding the matching pool->lock."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1650
msgid "put a pool_workqueue reference"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1651
msgid "pool_workqueue to put"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1652
msgid ""
"Drop a reference of **pwq**.  If its refcnt reaches zero, schedule its "
"destruction.  The caller should be holding the matching pool->lock."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1669
msgid "put_pwq() with surrounding pool lock/unlock"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1670
msgid "pool_workqueue to put (can be ``NULL``)"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1671
msgid "put_pwq() with locking.  This function also allows ``NULL`` **pwq**."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1719
msgid "Try to increment nr_active for a pwq"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1720
#: ../kernel/workqueue.c:1795 ../kernel/workqueue.c:1941
msgid "pool_workqueue of interest"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1722
#: ../kernel/workqueue.c:1797
msgid "``bool fill``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1721
#: ../kernel/workqueue.c:1796
msgid "max_active may have increased, try to increase concurrency level"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1722
msgid ""
"Try to increment nr_active for **pwq**. Returns ``true`` if an nr_active "
"count is successfully obtained. ``false`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1794
msgid "Activate the first inactive work item on a pwq"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1797
msgid ""
"Activate the first inactive work item of **pwq** if available and allowed by "
"max_active limit."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1800
msgid ""
"Returns ``true`` if an inactive work item has been activated. ``false`` if "
"no inactive work item is found or max_active limit is reached."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1819
msgid "unplug the oldest pool_workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1820
msgid "workqueue_struct where its oldest pwq is to be unplugged"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1821
msgid ""
"This function should only be called for ordered workqueues where only the "
"oldest pwq is unplugged, the others are plugged to suspend execution to "
"ensure proper work item ordering::"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1834
msgid ""
"When the oldest pwq is drained and removed, this function should be called "
"to unplug the next oldest one to start its work item execution. Note that "
"pwq's are linked into wq->pwqs with the oldest first, so the first one in "
"the list is the oldest."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1859
msgid "Activate a pending pwq on a wq_node_nr_active"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1865
msgid "``struct wq_node_nr_active *nna``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1860
msgid "wq_node_nr_active to activate a pending pwq for"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1862
msgid "``struct worker_pool *caller_pool``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1861
msgid "worker_pool the caller is locking"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1862
msgid ""
"Activate a pwq in **nna->pending_pwqs**. Called with **caller_pool** locked. "
"**caller_pool** may be unlocked and relocked to lock other worker_pools."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1940
msgid "Retire an active count"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1942
msgid ""
"Decrement **pwq**'s nr_active and try to activate the first inactive work "
"item. For unbound workqueues, this function may temporarily drop **pwq->pool-"
">lock**."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1990
msgid "decrement pwq's nr_in_flight"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1991
msgid "pwq of interest"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1993
msgid "``unsigned long work_data``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1992
msgid "work_data of work which left the queue"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1993
msgid ""
"A work either has completed or is removed from pending queue, decrement "
"nr_in_flight of its pwq and handle workqueue flushing."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1996
msgid "**NOTE**"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1997
msgid ""
"For unbound workqueues, this function may temporarily drop **pwq->pool-"
">lock** and thus should be called after all other state updates for the in-"
"flight work item is complete."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2036
#: ../kernel/workqueue.c:2153
msgid "steal work item from worklist and disable irq"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2037
#: ../kernel/workqueue.c:2154
msgid "work item to steal"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2039
#: ../kernel/workqueue.c:2156
msgid "``u32 cflags``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2038
#: ../kernel/workqueue.c:2155
msgid "``WORK_CANCEL_`` flags"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2040
#: ../kernel/workqueue.c:2157
msgid "``unsigned long *irq_flags``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2039
msgid "place to store irq state"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2040
msgid ""
"Try to grab PENDING bit of **work**.  This function can handle **work** in "
"any stable state - idle, on timer or on worklist."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2045
msgid "1"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2045
msgid "if **work** was pending and we successfully stole PENDING"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2046
#: ../kernel/workqueue.c:3382 ../kernel/workqueue.c:3533
msgid "0"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2046
msgid "if **work** was idle and we claimed PENDING"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2047
msgid "-EAGAIN"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2047
msgid "if PENDING couldn't be grabbed at the moment, safe to busy-retry"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2050
#: ../kernel/workqueue.c:4497
msgid "**Note**"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2052
msgid ""
"On >= 0 return, the caller owns **work**'s PENDING bit.  To avoid getting "
"interrupted while holding PENDING and **work** off queue, irq must be "
"disabled on entry.  This, combined with delayed_work->timer being irqsafe, "
"ensures that we return -EAGAIN for finite short period of time."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2057
msgid ""
"On successful return, >= 0, irq is disabled and the caller is responsible "
"for releasing it using local_irq_restore(***irq_flags**)."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2060
#: ../kernel/workqueue.c:4501
msgid "This function is safe to call from any context including IRQ handler."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2156
msgid "place to store IRQ state"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2157
msgid ""
"Grab PENDING bit of **work**. **work** can be in any stable state - idle, on "
"timer or on worklist."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2160
msgid ""
"Can be called from any context. IRQ is disabled on return with IRQ state "
"stored in ***irq_flags**. The caller is responsible for re-enabling it using "
"local_irq_restore()."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2164
msgid "Returns ``true`` if **work** was pending. ``false`` if idle."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2181
msgid "insert a work into a pool"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2182
msgid "pwq **work** belongs to"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2183
msgid "work to insert"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2184
msgid "insertion point"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2186
msgid "``unsigned int extra_flags``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2185
msgid "extra WORK_STRUCT_* flags to set"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2186
msgid ""
"Insert **work** which belongs to **pwq** after **head**.  **extra_flags** is "
"or'd to work_struct flags."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2382
msgid "queue work on specific cpu"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2383
#: ../kernel/workqueue.c:2547 ../kernel/workqueue.c:2585
msgid "CPU number to execute work on"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2386
msgid ""
"We queue the work to a specific CPU, the caller must ensure it can't go "
"away.  Callers that fail to ensure that the specified CPU cannot go away "
"will execute on a randomly chosen CPU. But note well that callers specifying "
"a CPU that never has been online will get a splat."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2393
#: ../kernel/workqueue.c:2461
msgid "``false`` if **work** was already on a queue, ``true`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2415
msgid "Select a CPU based on NUMA node"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2416
msgid "NUMA node ID that we want to select a CPU from"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2417
msgid ""
"This function will attempt to find a \"random\" cpu available on a given "
"node. If there are no CPUs available on the given node it will return "
"WORK_CPU_UNBOUND indicating that we should just schedule to any available "
"CPU if we need to schedule this work."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2444
msgid "queue work on a \"random\" cpu for a given NUMA node"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2445
msgid "NUMA node that we are targeting the work for"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2448
msgid ""
"We queue the work to a \"random\" CPU within a given NUMA node. The basic "
"idea here is to provide a way to somehow associate work with a given NUMA "
"node."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2452
msgid ""
"This function will only make a best effort attempt at getting this onto the "
"right NUMA node. If no node is requested or the requested node is offline "
"then we just fall back to standard queue_work behavior."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2456
msgid ""
"Currently the \"random\" CPU ends up being the first available CPU in the "
"intersection of cpu_online_mask and the cpumask of the node, unless we are "
"running on the node. In that case we just use the current CPU."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2546
msgid "queue work on specific CPU after delay"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2551
msgid ""
"We queue the delayed_work to a specific CPU, for non-zero delays the caller "
"must ensure it is online and can't go away. Callers that fail to ensure "
"this, may get **dwork->timer** queued to an offlined CPU and this will "
"prevent queueing of **dwork->work** unless the offlined CPU becomes online "
"again."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2558
msgid ""
"``false`` if **work** was already on a queue, ``true`` otherwise.  If "
"**delay** is zero and **dwork** is idle, it will be scheduled for immediate "
"execution."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2584
msgid "modify delay of or queue a delayed work on specific CPU"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2589
msgid ""
"If **dwork** is idle, equivalent to queue_delayed_work_on(); otherwise, "
"modify **dwork**'s timer so that it expires after **delay**.  If **delay** "
"is zero, **work** is guaranteed to be scheduled immediately regardless of "
"its current state."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2595
msgid ""
"This function is safe to call from any context including IRQ handler. See "
"try_to_grab_pending() for details."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2595
msgid ""
"``false`` if **dwork** was idle and queued, ``true`` if **dwork** was "
"pending and its timer was modified."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2628
msgid "queue work after a RCU grace period"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2631
#: ../kernel/workqueue.c:4381
msgid "``struct rcu_work *rwork``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2632
msgid ""
"``false`` if **rwork** was already pending, ``true`` otherwise.  Note that a "
"full RCU grace period is guaranteed only after a ``true`` return. While "
"**rwork** is guaranteed to be executed after a ``false`` return, the "
"execution may happen before a full RCU grace period has passed."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2680
msgid "attach a worker to a pool"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2681
msgid "worker to be attached"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2682
msgid "the target pool"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2683
msgid ""
"Attach **worker** to **pool**.  Once attached, the ``WORKER_UNBOUND`` flag "
"and cpu-binding of **worker** are kept coordinated with the pool across cpu-"
"[un]hotplugs."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2735
msgid "detach a worker from its pool"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2736
msgid "worker which is attached to its pool"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2737
msgid ""
"Undo the attaching which had been done in worker_attach_to_pool().  The "
"caller worker shouldn't access to the pool after detached except it has "
"other reference to the pool."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2779
msgid "create a new workqueue worker"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2780
msgid "pool the new worker will belong to"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2781
msgid "Create and start a new worker which is attached to **pool**."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2784
msgid "Might sleep.  Does GFP_KERNEL allocations."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2787
msgid "Pointer to the newly created worker."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2879
msgid "Tag a worker for destruction"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2880
msgid "worker to be destroyed"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2882
msgid "``struct list_head *list``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2881
msgid "transfer worker away from its pool->idle_list and into list"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2882
msgid ""
"Tag **worker** for destruction and adjust **pool** stats accordingly.  The "
"worker should be idle."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2914
msgid "check if some idle workers can now be deleted."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2920
msgid "``struct timer_list *t``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2915
msgid "The pool's idle_timer that just expired"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2916
msgid ""
"The timer is armed in worker_enter_idle(). Note that it isn't disarmed in "
"worker_leave_idle(), as a worker flicking between idle and active while its "
"pool is at the too_many_workers() tipping point would cause too much timer "
"housekeeping overhead. Since IDLE_WORKER_TIMEOUT is long enough, we just let "
"it expire and re-evaluate things from there."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2952
msgid "cull workers that have been idle for too long."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2953
msgid "the pool's work for handling these idle workers"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2954
msgid ""
"This goes through a pool's idle workers and gets rid of those that have been "
"idle for at least IDLE_WORKER_TIMEOUT seconds."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:2957
msgid ""
"We don't want to disturb isolated CPUs because of a pcpu kworker being "
"culled, so this also resets worker affinity. This requires a sleepable "
"context, hence the split between timer callback and work item."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3047
msgid "create a new worker if necessary"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3048
msgid "pool to create a new worker for"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3049
msgid ""
"Create a new worker for **pool** if necessary.  **pool** is guaranteed to "
"have at least one idle worker on return from this function.  If creating a "
"new worker takes longer than MAYDAY_INTERVAL, mayday is sent to all rescuers "
"with works scheduled on **pool** to resolve possible allocation deadlock."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3055
msgid ""
"On return, need_to_create_worker() is guaranteed to be ``false`` and "
"may_start_working() ``true``."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3058
msgid ""
"LOCKING: raw_spin_lock_irq(pool->lock) which may be released and regrabbed "
"multiple times.  Does GFP_KERNEL allocations.  Called only from manager."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3121
msgid "manage worker pool"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3123
msgid ""
"Assume the manager role and manage the worker pool **worker** belongs to.  "
"At any given time, there can be only zero or one manager per pool.  The "
"exclusion is handled automatically by this function."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3127
msgid ""
"The caller can safely start processing works on false return.  On true "
"return, it's guaranteed that need_to_create_worker() is false and "
"may_start_working() is true."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3132
msgid ""
"raw_spin_lock_irq(pool->lock) which may be released and regrabbed multiple "
"times.  Does GFP_KERNEL allocations."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3136
msgid ""
"``false`` if the pool doesn't need management and the caller can safely "
"start processing works, ``true`` if management function was performed and "
"the conditions that the caller verified before calling the function may no "
"longer be true."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3161
msgid "process single work"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3163
msgid "work to process"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3164
msgid ""
"Process **work**.  This function contains all the logics necessary to "
"process a single work including synchronization against and interaction with "
"other workers on the same cpu, queueing and flushing.  As long as context "
"requirement is met, any worker can call this function to process a work."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3171
msgid "raw_spin_lock_irq(pool->lock) which is released and regrabbed."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3336
msgid "process scheduled works"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3338
msgid ""
"Process all scheduled works.  Please note that the scheduled list may change "
"while processing a work, so this function repeatedly fetches a work from the "
"top and executes it."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3343
msgid ""
"raw_spin_lock_irq(pool->lock) which may be released and regrabbed multiple "
"times."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3373
msgid "the worker thread function"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3379
msgid "``void *__worker``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3375
msgid ""
"The worker thread function.  All workers belong to a worker_pool - either a "
"per-cpu one or dynamic unbound one.  These workers process all work items "
"regardless of their specific target workqueue.  The only exception is work "
"items which belong to workqueues with a rescuer which will be explained in "
"rescuer_thread()."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3515
msgid "the rescuer thread function"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3521
msgid "``void *__rescuer``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3517
msgid ""
"Workqueue rescuer thread function.  There's one rescuer for each workqueue "
"which has WQ_MEM_RECLAIM set."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3520
msgid ""
"Regular work processing on a pool may block trying to create a new worker "
"which uses GFP_KERNEL allocation which has slight chance of developing into "
"deadlock if some works currently on the same queue need to be processed to "
"satisfy the GFP_KERNEL allocation.  This is the problem rescuer solves."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3526
msgid ""
"When such condition is possible, the pool summons rescuers of all workqueues "
"which have works queued on the pool and let them process those works so that "
"forward progress can be guaranteed."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3530
msgid "This should happen rarely."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3772
msgid "check for flush dependency sanity"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3778
msgid "``struct workqueue_struct *target_wq``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3773
#: ../kernel/workqueue.c:3898
msgid "workqueue being flushed"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3775
msgid "``struct work_struct *target_work``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3774
msgid "work item being flushed (NULL for workqueue flushes)"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3776
msgid "``bool from_cancel``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3775
msgid "are we called from the work cancel path"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3776
msgid ""
"``current`` is trying to flush the whole **target_wq** or **target_work** on "
"it. If this is not the cancel path (which implies work being flushed is "
"either already running, or will not be at all), check if **target_wq** "
"doesn't have ``WQ_MEM_RECLAIM`` and verify that ``current`` is not "
"reclaiming memory or running on a workqueue which doesn't have "
"``WQ_MEM_RECLAIM`` as that can break forward- progress guarantee leading to "
"a deadlock."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3820
msgid "insert a barrier work"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3821
msgid "pwq to insert barrier into"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3823
msgid "``struct wq_barrier *barr``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3822
msgid "wq_barrier to insert"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3824
msgid "``struct work_struct *target``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3823
msgid "target work to attach **barr** to"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3824
msgid ""
"worker currently executing **target**, NULL if **target** is not executing"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3825
msgid ""
"**barr** is linked to **target** such that **barr** is completed only after "
"**target** finishes execution.  Please note that the ordering guarantee is "
"observed only with respect to **target** and on the local cpu."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3830
msgid ""
"Currently, a queued barrier can't be canceled.  This is because "
"try_to_grab_pending() can't determine whether the work to be grabbed is at "
"the head of the queue and thus can't clear LINKED flag of the previous work "
"while there must be a valid next work after a work with LINKED flag set."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3836
msgid ""
"Note that when **worker** is non-NULL, **target** may be modified underneath "
"us, so we can't reliably determine pwq from **target**."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3897
msgid "prepare pwqs for workqueue flushing"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3900
msgid "``int flush_color``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3899
msgid "new flush color, < 0 for no-op"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3901
msgid "``int work_color``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3900
msgid "new work color, < 0 for no-op"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3901
msgid "Prepare pwqs for workqueue flushing."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3903
msgid ""
"If **flush_color** is non-negative, flush_color on all pwqs should be -1.  "
"If no pwq has in-flight commands at the specified color, all pwq-"
">flush_color's stay at -1 and ``false`` is returned.  If any pwq has in "
"flight commands, its pwq->flush_color is set to **flush_color**, **wq-"
">nr_pwqs_to_flush** is updated accordingly, pwq wakeup logic is armed and "
"``true`` is returned."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3910
msgid ""
"The caller should have initialized **wq->first_flusher** prior to calling "
"this function with non-negative **flush_color**.  If **flush_color** is "
"negative, no flush color update is done and ``false`` is returned."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3915
msgid ""
"If **work_color** is non-negative, all pwqs should have the same work_color "
"which is previous to **work_color** and all will be advanced to "
"**work_color**."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3920
msgid "mutex_lock(wq->mutex)."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:3923
msgid ""
"``true`` if **flush_color** >= 0 and there's something to flush.  ``false`` "
"otherwise."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4014
msgid "ensure that any scheduled work has run to completion."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4015
msgid "workqueue to flush"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4016
msgid ""
"This function sleeps until all work items which were queued on entry have "
"finished execution, but it is not livelocked by new incoming ones."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4171
msgid "drain a workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4172
msgid "workqueue to drain"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4173
msgid ""
"Wait until the workqueue becomes empty.  While draining is in progress, only "
"chain queueing is allowed.  IOW, only currently pending or running work "
"items on **wq** can queue further work items on it.  **wq** is flushed "
"repeatedly until it becomes empty.  The number of flushing is determined by "
"the depth of chaining and should be relatively short.  Whine if it takes too "
"long."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4335
msgid "wait for a work to finish executing the last queueing instance"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4336
msgid "the work to flush"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4337
msgid ""
"Wait until **work** has finished execution.  **work** is guaranteed to be "
"idle on return if it hasn't been requeued since flush started."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4341
#: ../kernel/workqueue.c:4360
msgid ""
"``true`` if flush_work() waited for the work to finish execution, ``false`` "
"if it was already idle."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4353
msgid "wait for a dwork to finish executing the last queueing"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4354
msgid "the delayed work to flush"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4355
msgid ""
"Delayed timer is cancelled and the pending work is queued for immediate "
"execution.  Like flush_work(), this function only considers the last "
"queueing instance of **dwork**."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4375
msgid "wait for a rwork to finish executing the last queueing"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4376
msgid "the rcu work to flush"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4378
msgid ""
"``true`` if flush_rcu_work() waited for the work to finish execution, "
"``false`` if it was already idle."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4465
msgid "cancel a work and wait for it to finish"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4466
msgid "the work to cancel"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4467
msgid ""
"Cancel **work** and wait for its execution to finish. This function can be "
"used even if the work re-queues itself or migrates to another workqueue. On "
"return from this function, **work** is guaranteed to be not pending or "
"executing on any CPU as long as there aren't racing enqueues."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4472
msgid ""
"cancel_work_sync(:c:type:`delayed_work->work <delayed_work>`) must not be "
"used for delayed_work's. Use cancel_delayed_work_sync() instead."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4475
#: ../kernel/workqueue.c:4549
msgid ""
"Must be called from a sleepable context if **work** was last queued on a non-"
"BH workqueue. Can also be called from non-hardirq atomic contexts including "
"BH if **work** was last queued on a BH workqueue."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4479
#: ../kernel/workqueue.c:4553
msgid "Returns ``true`` if **work** was pending, ``false`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4489
msgid "cancel a delayed work"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4490
msgid "delayed_work to cancel"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4491
msgid "Kill off a pending delayed_work."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4494
msgid ""
"``true`` if **dwork** was pending and canceled; ``false`` if it wasn't "
"pending."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4497
msgid ""
"The work callback function may still be running on return, unless it returns "
"``true`` and the work doesn't re-arm itself.  Explicitly flush or use "
"cancel_delayed_work_sync() to wait on it."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4511
msgid "cancel a delayed work and wait for it to finish"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4512
msgid "the delayed work cancel"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4513
msgid "This is cancel_work_sync() for delayed works."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4516
msgid "``true`` if **dwork** was pending, ``false`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4526
msgid "Disable and cancel a work item"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4527
#: ../kernel/workqueue.c:4545
msgid "work item to disable"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4528
msgid ""
"Disable **work** by incrementing its disable count and cancel it if "
"currently pending. As long as the disable count is non-zero, any attempt to "
"queue **work** will fail and return ``false``. The maximum supported disable "
"depth is 2 to the power of ``WORK_OFFQ_DISABLE_BITS``, currently 65536."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4533
msgid ""
"Can be called from any context. Returns ``true`` if **work** was pending, "
"``false`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4544
msgid "Disable, cancel and drain a work item"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4546
msgid ""
"Similar to disable_work() but also wait for **work** to finish if currently "
"executing."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4563
msgid "Enable a work item"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4564
msgid "work item to enable"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4565
msgid ""
"Undo disable_work[_sync]() by decrementing **work**'s disable count. "
"**work** can only be queued if its disable count is 0."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4568
msgid ""
"Can be called from any context. Returns ``true`` if the disable count "
"reached 0. Otherwise, ``false``."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4590
msgid "Disable and cancel a delayed work item"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4591
#: ../kernel/workqueue.c:4604
msgid "delayed work item to disable"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4592
msgid "disable_work() for delayed work items."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4603
msgid "Disable, cancel and drain a delayed work item"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4605
msgid "disable_work_sync() for delayed work items."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4616
msgid "Enable a delayed work item"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4617
msgid "delayed work item to enable"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4618
msgid "enable_work() for delayed work items."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4628
msgid "execute a function synchronously on each online CPU"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4634
msgid "``work_func_t func``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4629
msgid "the function to call"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4630
msgid ""
"schedule_on_each_cpu() executes **func** on each online CPU using the system "
"workqueue and blocks until all CPUs have completed. schedule_on_each_cpu() "
"is very slow."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4635
#: ../kernel/workqueue.c:7475
msgid "0 on success, -errno on failure."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4665
msgid "reliably execute the routine with user context"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4671
msgid "``work_func_t fn``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4666
msgid "the function to execute"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4668
msgid "``struct execute_work *ew``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4667
msgid ""
"guaranteed storage for the execute work structure (must be available when "
"the work executes)"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4669
msgid ""
"Executes the function immediately if process context is available, otherwise "
"schedules the function for delayed execution."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4673
msgid "0 - function was executed 1 - function was scheduled for execution"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4691
msgid "free a workqueue_attrs"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4697
#: ../kernel/workqueue.c:5293
msgid "``struct workqueue_attrs *attrs``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4692
msgid "workqueue_attrs to free"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4693
msgid "Undo alloc_workqueue_attrs()."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4706
msgid "allocate a workqueue_attrs"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4712
#: ../kernel/workqueue.c:6092 ../kernel/workqueue.c:6108
#: ../kernel/workqueue.c:6504 ../kernel/workqueue.c:6528
#: ../kernel/workqueue.c:6871 ../kernel/workqueue.c:6899
#: ../kernel/workqueue.c:6945 ../kernel/workqueue.c:7841
#: ../kernel/workqueue.c:7998 ../kernel/workqueue.c:8122
msgid "``void``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:1
msgid "no arguments"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4707
msgid ""
"Allocate a new workqueue_attrs, initialize with default settings and return "
"it."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4711
msgid "The allocated new workqueue_attr on success. ``NULL`` on failure."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4837
msgid "initialize a newly zalloc'd worker_pool"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4838
msgid "worker_pool to initialize"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4839
msgid ""
"Initialize a newly zalloc'd **pool**.  It also allocates **pool->attrs**."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:4842
msgid ""
"0 on success, -errno on failure.  Even on failure, all fields inside "
"**pool** proper are initialized and put_unbound_pool() can be called on "
"**pool** safely to release it."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:5003
msgid "put a worker_pool"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:5004
msgid "worker_pool to put"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:5005
msgid ""
"Put **pool**.  If its refcnt reaches zero, it gets destroyed in RCU safe "
"manner.  get_unbound_pool() calls this function on its failure path and this "
"function should be able to release pools which went through, successfully or "
"not, init_worker_pool()."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:5010
#: ../kernel/workqueue.c:5089
msgid "Should be called with wq_pool_mutex held."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:5082
msgid "get a worker_pool with the specified attributes"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:5088
#: ../kernel/workqueue.c:5473
msgid "``const struct workqueue_attrs *attrs``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:5083
msgid "the attributes of the worker_pool to get"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:5084
msgid ""
"Obtain a worker_pool which has the same attributes as **attrs**, bump the "
"reference count and return it.  If there already is a matching worker_pool, "
"it will be used; otherwise, this function attempts to create a new one."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:5092
msgid ""
"On success, a worker_pool with the same attributes as **attrs**. On failure, "
"``NULL``."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:5287
msgid "calculate a wq_attrs' cpumask for a pod"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:5288
msgid "the wq_attrs of the default pwq of the target workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:5289
msgid "the target CPU"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:5290
msgid ""
"Calculate the cpumask a workqueue with **attrs** should use on **pod**. The "
"result is stored in **attrs->__pod_cpumask**."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:5293
msgid ""
"If pod affinity is not enabled, **attrs->cpumask** is always used. If "
"enabled and **pod** has online CPUs requested by **attrs**, the returned "
"cpumask is the intersection of the possible CPUs of **pod** and **attrs-"
">cpumask**."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:5297
msgid ""
"The caller is responsible for ensuring that the cpumask of **pod** stays "
"stable."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:5470
msgid "apply new workqueue_attrs to an unbound workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:5472
msgid "the workqueue_attrs to apply, allocated with alloc_workqueue_attrs()"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:5473
msgid ""
"Apply **attrs** to an unbound workqueue **wq**. Unless disabled, this "
"function maps a separate pwq to each CPU pod with possibles CPUs in **attrs-"
">cpumask** so that work items are affine to the pod it was issued on. Older "
"pwqs are released as in-flight work items finish. Note that a work item "
"which repeatedly requeues itself back-to-back will stay on its current pwq."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:5479
msgid "Performs GFP_KERNEL allocations."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:5482
#: ../kernel/workqueue.c:7032
msgid "0 on success and -errno on failure."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:5497
msgid "update a pwq slot for CPU hot[un]plug"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:5499
msgid "the CPU to update the pwq slot for"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:5500
msgid ""
"This function is to be called from ``CPU_DOWN_PREPARE``, ``CPU_ONLINE`` and "
"``CPU_DOWN_FAILED``.  **cpu** is in the same pod of the CPU being "
"hot[un]plugged."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:5504
msgid ""
"If pod affinity can't be adjusted due to memory allocation failure, it falls "
"back to **wq->dfl_pwq** which may not be optimal but is always correct."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:5507
msgid ""
"Note that when the last allowed CPU of a pod goes offline for a workqueue "
"with a cpumask spanning multiple pods, the workers which were already "
"executing the work items for the workqueue will lose their CPU affinity and "
"may execute on any CPU. This is similar to how per-cpu workqueues behave on "
"CPU_DOWN. If a workqueue user wants strict affinity, it's the user's "
"responsibility to flush the work item from CPU_DOWN_PREPARE."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:5692
msgid "update a wq's max_active to the current setting"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:5693
#: ../kernel/workqueue.c:5934 ../kernel/workqueue.c:6026
#: ../kernel/workqueue.c:6119
msgid "target workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:5694
msgid ""
"If **wq** isn't freezing, set **wq->max_active** to the saved_max_active and "
"activate inactive work items accordingly. If **wq** is freezing, clear **wq-"
">max_active** to zero."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:5933
msgid "safely terminate a workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:5935
msgid ""
"Safely destroy a workqueue. All work currently pending will be done first."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:5937
msgid ""
"This function does NOT guarantee that non-pending work that has been "
"submitted with queue_delayed_work() and similar functions will be done "
"before destroying the workqueue. The fundamental problem is that, currently, "
"the workqueue has no way of accessing non-pending delayed_work. delayed_work "
"is only linked on the timer-side. All delayed_work must, therefore, be "
"canceled before calling this function."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:5944
msgid ""
"TODO: It would be better if the problem described above wouldn't exist and "
"destroy_workqueue() would cleanly cancel all pending and non-pending "
"delayed_work."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6025
msgid "adjust max_active of a workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6027
msgid "new max_active value."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6028
msgid ""
"Set max_active of **wq** to **max_active**. See the alloc_workqueue() "
"function comment."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6032
msgid "Don't call from IRQ context."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6059
msgid "adjust min_active of an unbound workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6060
msgid "target unbound workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6062
msgid "``int min_active``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6061
msgid "new min_active value"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6062
msgid ""
"Set min_active of an unbound workqueue. Unlike other types of workqueues, an "
"unbound workqueue is not guaranteed to be able to process max_active "
"interdependent work items. Instead, an unbound workqueue is guaranteed to be "
"able to process min_active number of interdependent work items which is "
"``WQ_DFL_MIN_ACTIVE`` by default."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6068
msgid ""
"Use this function to adjust the min_active value between 0 and the current "
"max_active."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6086
msgid "retrieve ``current`` task's work struct"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6087
msgid ""
"Determine if ``current`` task is a workqueue worker and what it's working "
"on. Useful to find out the context that the ``current`` task is running in."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6091
msgid ""
"work struct if ``current`` task is a workqueue worker, ``NULL`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6102
msgid "is ``current`` workqueue rescuer?"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6103
msgid ""
"Determine whether ``current`` is a workqueue rescuer.  Can be used from work "
"functions to determine whether it's being run off the rescuer task."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6107
msgid "``true`` if ``current`` is a workqueue rescuer. ``false`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6117
msgid "test whether a workqueue is congested"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6118
msgid "CPU in question"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6120
msgid ""
"Test whether **wq**'s cpu workqueue for **cpu** is congested.  There is no "
"synchronization around this function and the test result is unreliable and "
"only useful as advisory hints or for debugging."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6124
msgid "If **cpu** is WORK_CPU_UNBOUND, the test is performed on the local CPU."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6126
msgid ""
"With the exception of ordered workqueues, all workqueues have per-cpu "
"pool_workqueues, each with its own congested state. A workqueue being "
"congested on one CPU doesn't mean that the workqueue is contested on any "
"other CPUs."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6132
msgid "``true`` if congested, ``false`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6155
msgid "test whether a work is currently pending or running"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6156
msgid "the work to be tested"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6157
msgid ""
"Test whether **work** is currently pending or running.  There is no "
"synchronization around this function and the test result is unreliable and "
"only useful as advisory hints or for debugging."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6162
msgid "OR'd bitmask of WORK_BUSY_* bits."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6189
msgid "set description for the current work item"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6190
msgid "printf-style format string"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6191
msgid "arguments for the format string"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6192
msgid ""
"This function can be called by a running work function to describe what the "
"work item is about.  If the worker task gets dumped, this information will "
"be printed out together to help debugging.  The description can be at most "
"WORKER_DESC_LEN including the trailing '\\0'."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6212
msgid "print out worker information and description"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6218
msgid "``const char *log_lvl``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6213
msgid "the log level to use when printing"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6214
msgid "target task"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6215
msgid ""
"If **task** is a worker and currently executing a work item, print out the "
"name of the workqueue being serviced and worker description set with "
"set_worker_desc() by the currently executing work item."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6219
msgid ""
"This function can be safely called on any task as long as the task_struct "
"itself is accessible.  While safe, this function isn't synchronized and may "
"print out mixups or garbages of limited length."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6405
msgid "dump state of specified workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6406
msgid "workqueue whose state will be printed"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6449
msgid "dump state of specified worker pool"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6450
msgid "worker pool whose state will be printed"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6498
msgid "dump workqueue state"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6499
msgid ""
"Called from a sysrq handler and prints out all busy workqueues and pools."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6522
msgid "dump freezable workqueue state"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6523
msgid ""
"Called from try_to_freeze_tasks() and prints out all freezable workqueues "
"still busy."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6647
msgid "rebind all workers of a pool to the associated CPU"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6649
msgid "**pool->cpu** is coming online.  Rebind all workers to the CPU."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6703
msgid "restore cpumask of unbound workers"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6704
msgid "unbound pool of interest"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6705
msgid "the CPU which is coming up"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6706
msgid ""
"An unbound pool may end up with a cpumask which doesn't have any online "
"CPUs.  When a worker of such pool get scheduled, the scheduler resets its "
"cpus_allowed.  If **cpu** is in **pool**'s cpumask which didn't have any "
"online CPU before, cpus_allowed of all its workers should be restored."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6837
msgid "run a function in thread context on a particular cpu"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6838
msgid "the cpu to run on"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6840
msgid "``long (*fn)(void *)``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6839
msgid "the function to run"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6841
msgid "``void *arg``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6840
msgid "the function arg"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6842
msgid "``struct lock_class_key *key``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6841
msgid "The lock class key for lock debugging purposes"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6842
msgid ""
"It is up to the caller to ensure that the cpu doesn't go offline. The caller "
"must not hold any locks which would prevent **fn** from completing."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6846
msgid "The value **fn** returns."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6865
msgid "begin freezing workqueues"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6866
msgid ""
"Start freezing workqueues.  After this function returns, all freezable "
"workqueues will queue new works to their inactive_works list instead of pool-"
">worklist."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6871
#: ../kernel/workqueue.c:6944
msgid "Grabs and releases wq_pool_mutex, wq->mutex and pool->lock's."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6893
msgid "are freezable workqueues still busy?"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6894
msgid ""
"Check whether freezing is complete.  This function must be called between "
"freeze_workqueues_begin() and thaw_workqueues()."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6898
msgid "Grabs and releases wq_pool_mutex."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6901
msgid ""
"``true`` if some freezable workqueues are still busy.  ``false`` if freezing "
"is complete."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6939
msgid "thaw workqueues"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:6940
msgid ""
"Thaw workqueues.  Normal queueing is restored and all collected frozen works "
"are transferred to their respective pool worklists."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:7025
msgid "Propagate housekeeping cpumask update"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:7031
msgid "``const struct cpumask *hk``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:7026
msgid "the new housekeeping cpumask"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:7027
msgid ""
"Update the unbound workqueue cpumask on top of the new housekeeping cpumask "
"such that the effective unbound affinity is the intersection of the new "
"housekeeping with the requested affinity set via nohz_full=/isolcpus= or "
"sysfs."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:7358
msgid "Set the low-level unbound cpumask"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:7364
msgid "``cpumask_var_t cpumask``"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:7359
msgid "the cpumask to set"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:7360
msgid ""
"The low-level workqueues cpumask is a global cpumask that limits the "
"affinity of all unbound workqueues.  This function check the **cpumask** and "
"apply it to all unbound workqueues and updates all pwqs of them."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:7365
msgid ""
"0       - Success -EINVAL - Invalid **cpumask** -ENOMEM - Failed to allocate "
"memory for attrs or pwqs."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:7463
msgid "make a workqueue visible in sysfs"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:7464
msgid "the workqueue to register"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:7465
msgid ""
"Expose **wq** in sysfs under /sys/bus/workqueue/devices. alloc_workqueue*() "
"automatically calls this function if WQ_SYSFS is set which is the preferred "
"method."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:7469
msgid ""
"Workqueue user should use this function directly iff it wants to apply "
"workqueue_attrs before making the workqueue visible in sysfs; otherwise, "
"apply_workqueue_attrs() may race against userland updating the attributes."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:7530
msgid "undo workqueue_sysfs_register()"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:7531
msgid "the workqueue to unregister"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:7532
msgid ""
"If **wq** is registered to sysfs by workqueue_sysfs_register(), unregister."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:7835
msgid "early init for workqueue subsystem"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:7836
msgid ""
"This is the first step of three-staged workqueue subsystem initialization "
"and invoked as soon as the bare basics - memory allocation, cpumasks and idr "
"are up. It sets up all the data structures and system workqueues and allows "
"early boot code to create workqueues and queue/cancel work items. Actual "
"work item execution starts only after kthreads can be created and scheduled "
"right before early initcalls."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:7992
msgid "bring workqueue subsystem fully online"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:7993
msgid ""
"This is the second step of three-staged workqueue subsystem initialization "
"and invoked as soon as kthreads can be created and scheduled. Workqueues "
"have been created and work items queued on them, but there are no kworkers "
"executing the work items yet. Populate the worker pools with the initial "
"workers and enable future kworker creations."
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:8116
msgid "initialize CPU pods for unbound workqueues"
msgstr ""

#: ../../../core-api/workqueue:789: ../kernel/workqueue.c:8117
msgid ""
"This is the third step of three-staged workqueue subsystem initialization "
"and invoked after SMP and topology information are fully initialized. It "
"initializes the unbound CPU pods accordingly."
msgstr ""
