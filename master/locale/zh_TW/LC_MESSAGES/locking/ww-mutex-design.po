# SOME DESCRIPTIVE TITLE.
# Copyright (C) The kernel development community
# This file is distributed under the same license as the The Linux Kernel package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: The Linux Kernel master\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-09-27 13:53+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../locking/ww-mutex-design.rst:3
msgid "Wound/Wait Deadlock-Proof Mutex Design"
msgstr ""

#: ../../../locking/ww-mutex-design.rst:5
msgid ""
"Please read mutex-design.rst first, as it applies to wait/wound mutexes too."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:8
msgid "Motivation for WW-Mutexes"
msgstr ""

#: ../../../locking/ww-mutex-design.rst:10
msgid ""
"GPU's do operations that commonly involve many buffers.  Those buffers can "
"be shared across contexts/processes, exist in different memory domains (for "
"example VRAM vs system memory), and so on.  And with PRIME / dmabuf, they "
"can even be shared across devices.  So there are a handful of situations "
"where the driver needs to wait for buffers to become ready.  If you think "
"about this in terms of waiting on a buffer mutex for it to become available, "
"this presents a problem because there is no way to guarantee that buffers "
"appear in a execbuf/batch in the same order in all contexts.  That is "
"directly under control of userspace, and a result of the sequence of GL "
"calls that an application makes.  Which results in the potential for "
"deadlock.  The problem gets more complex when you consider that the kernel "
"may need to migrate the buffer(s) into VRAM before the GPU operates on the "
"buffer(s), which may in turn require evicting some other buffers (and you "
"don't want to evict other buffers which are already queued up to the GPU), "
"but for a simplified understanding of the problem you can ignore this."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:27
msgid ""
"The algorithm that the TTM graphics subsystem came up with for dealing with "
"this problem is quite simple.  For each group of buffers (execbuf) that need "
"to be locked, the caller would be assigned a unique reservation id/ticket, "
"from a global counter.  In case of deadlock while locking all the buffers "
"associated with a execbuf, the one with the lowest reservation ticket (i.e. "
"the oldest task) wins, and the one with the higher reservation id (i.e. the "
"younger task) unlocks all of the buffers that it has already locked, and "
"then tries again."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:36
msgid ""
"In the RDBMS literature, a reservation ticket is associated with a "
"transaction. and the deadlock handling approach is called Wait-Die. The name "
"is based on the actions of a locking thread when it encounters an already "
"locked mutex. If the transaction holding the lock is younger, the locking "
"transaction waits. If the transaction holding the lock is older, the locking "
"transaction backs off and dies. Hence Wait-Die. There is also another "
"algorithm called Wound-Wait: If the transaction holding the lock is younger, "
"the locking transaction wounds the transaction holding the lock, requesting "
"it to die. If the transaction holding the lock is older, it waits for the "
"other transaction. Hence Wound-Wait. The two algorithms are both fair in "
"that a transaction will eventually succeed. However, the Wound-Wait "
"algorithm is typically stated to generate fewer backoffs compared to Wait-"
"Die, but is, on the other hand, associated with more work than Wait-Die when "
"recovering from a backoff. Wound-Wait is also a preemptive algorithm in that "
"transactions are wounded by other transactions, and that requires a reliable "
"way to pick up the wounded condition and preempt the running transaction. "
"Note that this is not the same as process preemption. A Wound-Wait "
"transaction is considered preempted when it dies (returning -EDEADLK) "
"following a wound."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:58
msgid "Concepts"
msgstr ""

#: ../../../locking/ww-mutex-design.rst:60
msgid ""
"Compared to normal mutexes two additional concepts/objects show up in the "
"lock interface for w/w mutexes:"
msgstr ""

#: ../../../locking/ww-mutex-design.rst:63
msgid ""
"Acquire context: To ensure eventual forward progress it is important that a "
"task trying to acquire locks doesn't grab a new reservation id, but keeps "
"the one it acquired when starting the lock acquisition. This ticket is "
"stored in the acquire context. Furthermore the acquire context keeps track "
"of debugging state to catch w/w mutex interface abuse. An acquire context is "
"representing a transaction."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:70
msgid ""
"W/w class: In contrast to normal mutexes the lock class needs to be explicit "
"for w/w mutexes, since it is required to initialize the acquire context. The "
"lock class also specifies what algorithm to use, Wound-Wait or Wait-Die."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:74
msgid ""
"Furthermore there are three different class of w/w lock acquire functions:"
msgstr ""

#: ../../../locking/ww-mutex-design.rst:76
msgid "Normal lock acquisition with a context, using ww_mutex_lock."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:78
msgid ""
"Slowpath lock acquisition on the contending lock, used by the task that just "
"killed its transaction after having dropped all already acquired locks. "
"These functions have the _slow postfix."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:82
msgid ""
"From a simple semantics point-of-view the _slow functions are not strictly "
"required, since simply calling the normal ww_mutex_lock functions on the "
"contending lock (after having dropped all other already acquired locks) will "
"work correctly. After all if no other ww mutex has been acquired yet there's "
"no deadlock potential and hence the ww_mutex_lock call will block and not "
"prematurely return -EDEADLK. The advantage of the _slow functions is in "
"interface safety:"
msgstr ""

#: ../../../locking/ww-mutex-design.rst:90
msgid ""
"ww_mutex_lock has a __must_check int return type, whereas ww_mutex_lock_slow "
"has a void return type. Note that since ww mutex code needs loops/retries "
"anyway the __must_check doesn't result in spurious warnings, even though the "
"very first lock operation can never fail."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:94
msgid ""
"When full debugging is enabled ww_mutex_lock_slow checks that all acquired "
"ww mutex have been released (preventing deadlocks) and makes sure that we "
"block on the contending lock (preventing spinning through the -EDEADLK "
"slowpath until the contended lock can be acquired)."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:99
msgid ""
"Functions to only acquire a single w/w mutex, which results in the exact "
"same semantics as a normal mutex. This is done by calling ww_mutex_lock with "
"a NULL context."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:103
msgid ""
"Again this is not strictly required. But often you only want to acquire a "
"single lock in which case it's pointless to set up an acquire context (and "
"so better to avoid grabbing a deadlock avoidance ticket)."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:107
msgid ""
"Of course, all the usual variants for handling wake-ups due to signals are "
"also provided."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:111
msgid "Usage"
msgstr ""

#: ../../../locking/ww-mutex-design.rst:113
msgid ""
"The algorithm (Wait-Die vs Wound-Wait) is chosen by using either "
"DEFINE_WW_CLASS() (Wound-Wait) or DEFINE_WD_CLASS() (Wait-Die) As a rough "
"rule of thumb, use Wound-Wait iff you expect the number of simultaneous "
"competing transactions to be typically small, and you want to reduce the "
"number of rollbacks."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:119
msgid ""
"Three different ways to acquire locks within the same w/w class. Common "
"definitions for methods #1 and #2::"
msgstr ""

#: ../../../locking/ww-mutex-design.rst:134
msgid ""
"Method 1, using a list in execbuf->buffers that's not allowed to be "
"reordered. This is useful if a list of required objects is already tracked "
"somewhere. Furthermore the lock helper can use propagate the -EALREADY "
"return code back to the caller as a signal that an object is twice on the "
"list. This is useful if the list is constructed from userspace input and the "
"ABI requires userspace to not have duplicate entries (e.g. for a gpu "
"commandbuffer submission ioctl)::"
msgstr ""

#: ../../../locking/ww-mutex-design.rst:183
msgid ""
"Method 2, using a list in execbuf->buffers that can be reordered. Same "
"semantics of duplicate entry detection using -EALREADY as method 1 above. "
"But the list-reordering allows for a bit more idiomatic code::"
msgstr ""

#: ../../../locking/ww-mutex-design.rst:223
msgid "Unlocking works the same way for both methods #1 and #2::"
msgstr ""

#: ../../../locking/ww-mutex-design.rst:235
msgid ""
"Method 3 is useful if the list of objects is constructed ad-hoc and not "
"upfront, e.g. when adjusting edges in a graph where each node has its own "
"ww_mutex lock, and edges can only be changed when holding the locks of all "
"involved nodes. w/w mutexes are a natural fit for such a case for two "
"reasons:"
msgstr ""

#: ../../../locking/ww-mutex-design.rst:240
msgid ""
"They can handle lock-acquisition in any order which allows us to start "
"walking a graph from a starting point and then iteratively discovering new "
"edges and locking down the nodes those edges connect to."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:243
msgid ""
"Due to the -EALREADY return code signalling that a given objects is already "
"held there's no need for additional book-keeping to break cycles in the "
"graph or keep track off which looks are already held (when using more than "
"one node as a starting point)."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:248
msgid ""
"Note that this approach differs in two important ways from the above methods:"
msgstr ""

#: ../../../locking/ww-mutex-design.rst:250
msgid ""
"Since the list of objects is dynamically constructed (and might very well be "
"different when retrying due to hitting the -EDEADLK die condition) there's "
"no need to keep any object on a persistent list when it's not locked. We can "
"therefore move the list_head into the object itself."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:254
msgid ""
"On the other hand the dynamic object list construction also means that the -"
"EALREADY return code can't be propagated."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:257
msgid ""
"Note also that methods #1 and #2 and method #3 can be combined, e.g. to "
"first lock a list of starting nodes (passed in from userspace) using one of "
"the above methods. And then lock any additional objects affected by the "
"operations using method #3 below. The backoff/retry procedure will be a bit "
"more involved, since when the dynamic locking step hits -EDEADLK we also "
"need to unlock all the objects acquired with the fixed list. But the w/w "
"mutex debug checks will catch any interface misuse for these cases."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:265
msgid ""
"Also, method 3 can't fail the lock acquisition step since it doesn't return -"
"EALREADY. Of course this would be different when using the _interruptible "
"variants, but that's outside of the scope of these examples here::"
msgstr ""

#: ../../../locking/ww-mutex-design.rst:327
msgid ""
"Method 4: Only lock one single objects. In that case deadlock detection and "
"prevention is obviously overkill, since with grabbing just one lock you "
"can't produce a deadlock within just one class. To simplify this case the w/"
"w mutex api can be used with a NULL context."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:333
msgid "Implementation Details"
msgstr ""

#: ../../../locking/ww-mutex-design.rst:336
msgid "Design:"
msgstr ""

#: ../../../locking/ww-mutex-design.rst:338
msgid ""
"ww_mutex currently encapsulates a struct mutex, this means no extra overhead "
"for normal mutex locks, which are far more common. As such there is only a "
"small increase in code size if wait/wound mutexes are not used."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:342
msgid "We maintain the following invariants for the wait list:"
msgstr ""

#: ../../../locking/ww-mutex-design.rst:344
msgid ""
"Waiters with an acquire context are sorted by stamp order; waiters without "
"an acquire context are interspersed in FIFO order."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:346
msgid ""
"For Wait-Die, among waiters with contexts, only the first one can have other "
"locks acquired already (ctx->acquired > 0). Note that this waiter may come "
"after other waiters without contexts in the list."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:350
msgid ""
"The Wound-Wait preemption is implemented with a lazy-preemption scheme: The "
"wounded status of the transaction is checked only when there is contention "
"for a new lock and hence a true chance of deadlock. In that situation, if "
"the transaction is wounded, it backs off, clears the wounded status and "
"retries. A great benefit of implementing preemption in this way is that the "
"wounded transaction can identify a contending lock to wait for before "
"restarting the transaction. Just blindly restarting the transaction would "
"likely make the transaction end up in a situation where it would have to "
"back off again."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:360
msgid ""
"In general, not much contention is expected. The locks are typically used to "
"serialize access to resources for devices, and optimization focus should "
"therefore be directed towards the uncontended cases."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:365
msgid "Lockdep:"
msgstr ""

#: ../../../locking/ww-mutex-design.rst:367
msgid ""
"Special care has been taken to warn for as many cases of api abuse as "
"possible. Some common api abuses will be caught with CONFIG_DEBUG_MUTEXES, "
"but CONFIG_PROVE_LOCKING is recommended."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:371
msgid "Some of the errors which will be warned about:"
msgstr ""

#: ../../../locking/ww-mutex-design.rst:372
msgid "Forgetting to call ww_acquire_fini or ww_acquire_init."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:373
msgid "Attempting to lock more mutexes after ww_acquire_done."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:374
msgid ""
"Attempting to lock the wrong mutex after -EDEADLK and unlocking all mutexes."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:376
msgid ""
"Attempting to lock the right mutex after -EDEADLK, before unlocking all "
"mutexes."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:379
msgid "Calling ww_mutex_lock_slow before -EDEADLK was returned."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:381
msgid "Unlocking mutexes with the wrong unlock function."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:382
msgid "Calling one of the ww_acquire_* twice on the same context."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:383
msgid "Using a different ww_class for the mutex than for the ww_acquire_ctx."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:384
msgid "Normal lockdep errors that can result in deadlocks."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:386
msgid "Some of the lockdep errors that can result in deadlocks:"
msgstr ""

#: ../../../locking/ww-mutex-design.rst:387
msgid ""
"Calling ww_acquire_init to initialize a second ww_acquire_ctx before having "
"called ww_acquire_fini on the first."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:389
msgid "'normal' deadlocks that can occur."
msgstr ""

#: ../../../locking/ww-mutex-design.rst:391
msgid "FIXME:"
msgstr ""

#: ../../../locking/ww-mutex-design.rst:392
msgid ""
"Update this section once we have the TASK_DEADLOCK task state flag magic "
"implemented."
msgstr ""
