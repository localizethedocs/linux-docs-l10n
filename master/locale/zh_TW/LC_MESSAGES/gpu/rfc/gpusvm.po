# SOME DESCRIPTIVE TITLE.
# Copyright (C) The kernel development community
# This file is distributed under the same license as the The Linux Kernel package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: The Linux Kernel master\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-09-29 08:26+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../gpu/rfc/gpusvm.rst:5
msgid "GPU SVM Section"
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:8
msgid "Agreed upon design principles"
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:10
msgid "migrate_to_ram path"
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:11
msgid ""
"Rely only on core MM concepts (migration PTEs, page references, and page "
"locking)."
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:13
msgid ""
"No driver specific locks other than locks for hardware interaction in this "
"path. These are not required and generally a bad idea to invent driver "
"defined locks to seal core MM races."
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:16
msgid ""
"An example of a driver-specific lock causing issues occurred before fixing "
"do_swap_page to lock the faulting page. A driver-exclusive lock in "
"migrate_to_ram produced a stable livelock if enough threads read the "
"faulting page."
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:20
msgid ""
"Partial migration is supported (i.e., a subset of pages attempting to "
"migrate can actually migrate, with only the faulting page guaranteed to "
"migrate)."
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:23
msgid "Driver handles mixed migrations via retry loops rather than locking."
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:24
msgid "Eviction"
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:25
msgid ""
"Eviction is defined as migrating data from the GPU back to the CPU without a "
"virtual address to free up GPU memory."
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:27
msgid ""
"Only looking at physical memory data structures and locks as opposed to "
"looking at virtual memory data structures and locks."
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:29
msgid "No looking at mm/vma structs or relying on those being locked."
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:30
msgid ""
"The rationale for the above two points is that CPU virtual addresses can "
"change at any moment, while the physical pages remain stable."
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:32
msgid ""
"GPU page table invalidation, which requires a GPU virtual address, is "
"handled via the notifier that has access to the GPU virtual address."
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:34
msgid "GPU fault side"
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:35
msgid ""
"mmap_read only used around core MM functions which require this lock and "
"should strive to take mmap_read lock only in GPU SVM layer."
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:37
msgid ""
"Big retry loop to handle all races with the mmu notifier under the gpu "
"pagetable locks/mmu notifier range lock/whatever we end up calling those."
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:40
msgid ""
"Races (especially against concurrent eviction or migrate_to_ram) should not "
"be handled on the fault side by trying to hold locks; rather, they should be "
"handled using retry loops. One possible exception is holding a BO's dma-resv "
"lock during the initial migration to VRAM, as this is a well-defined lock "
"that can be taken underneath the mmap_read lock."
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:46
msgid ""
"One possible issue with the above approach is if a driver has a strict "
"migration policy requiring GPU access to occur in GPU memory. Concurrent CPU "
"access could cause a livelock due to endless retries. While no current user "
"(Xe) of GPU SVM has such a policy, it is likely to be added in the future. "
"Ideally, this should be resolved on the core-MM side rather than through a "
"driver-side lock."
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:52
msgid "Physical memory to virtual backpointer"
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:53
msgid ""
"This does not work, as no pointers from physical memory to virtual memory "
"should exist. mremap() is an example of the core MM updating the virtual "
"address without notifying the driver of address change rather the driver "
"only receiving the invalidation notifier."
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:57
msgid ""
"The physical memory backpointer (page->zone_device_data) should remain "
"stable from allocation to page free. Safely updating this against a "
"concurrent user would be very difficult unless the page is free."
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:60
msgid "GPU pagetable locking"
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:61
msgid ""
"Notifier lock only protects range tree, pages valid state for a range "
"(rather than seqno due to wider notifiers), pagetable entries, and mmu "
"notifier seqno tracking, it is not a global lock to protect against races."
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:65
msgid "All races handled with big retry as mentioned above."
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:68
msgid "Overview of baseline design"
msgstr ""

#: ../../../gpu/rfc/gpusvm:70: drivers/gpu/drm/drm_gpusvm.c:23
msgid ""
"GPU Shared Virtual Memory (GPU SVM) layer for the Direct Rendering Manager "
"(DRM) is a component of the DRM framework designed to manage shared virtual "
"memory between the CPU and GPU. It enables efficient data exchange and "
"processing for GPU-accelerated applications by allowing memory sharing and "
"synchronization between the CPU's and GPU's virtual address spaces."
msgstr ""

#: ../../../gpu/rfc/gpusvm:70: drivers/gpu/drm/drm_gpusvm.c:29
msgid "Key GPU SVM Components:"
msgstr ""

#: ../../../gpu/rfc/gpusvm:70: drivers/gpu/drm/drm_gpusvm.c:31
msgid "Notifiers:"
msgstr ""

#: ../../../gpu/rfc/gpusvm:70: drivers/gpu/drm/drm_gpusvm.c:32
msgid ""
"Used for tracking memory intervals and notifying the GPU of changes, "
"notifiers are sized based on a GPU SVM initialization parameter, with a "
"recommendation of 512M or larger. They maintain a Red-BlacK tree and a list "
"of ranges that fall within the notifier interval.  Notifiers are tracked "
"within a GPU SVM Red-BlacK tree and list and are dynamically inserted or "
"removed as ranges within the interval are created or destroyed."
msgstr ""

#: ../../../gpu/rfc/gpusvm:70: drivers/gpu/drm/drm_gpusvm.c:39
msgid "Ranges:"
msgstr ""

#: ../../../gpu/rfc/gpusvm:70: drivers/gpu/drm/drm_gpusvm.c:40
msgid ""
"Represent memory ranges mapped in a DRM device and managed by GPU SVM. They "
"are sized based on an array of chunk sizes, which is a GPU SVM "
"initialization parameter, and the CPU address space.  Upon GPU fault, the "
"largest aligned chunk that fits within the faulting CPU address space is "
"chosen for the range size. Ranges are expected to be dynamically allocated "
"on GPU fault and removed on an MMU notifier UNMAP event. As mentioned above, "
"ranges are tracked in a notifier's Red-Black tree."
msgstr ""

#: ../../../gpu/rfc/gpusvm:70: drivers/gpu/drm/drm_gpusvm.c:49
msgid "Operations:"
msgstr ""

#: ../../../gpu/rfc/gpusvm:70: drivers/gpu/drm/drm_gpusvm.c:50
msgid ""
"Define the interface for driver-specific GPU SVM operations such as range "
"allocation, notifier allocation, and invalidations."
msgstr ""

#: ../../../gpu/rfc/gpusvm:70: drivers/gpu/drm/drm_gpusvm.c:53
#: ../../../gpu/rfc/gpusvm:88: drivers/gpu/drm/drm_pagemap.c:49
msgid "Device Memory Allocations:"
msgstr ""

#: ../../../gpu/rfc/gpusvm:70: drivers/gpu/drm/drm_gpusvm.c:54
msgid ""
"Embedded structure containing enough information for GPU SVM to migrate to / "
"from device memory."
msgstr ""

#: ../../../gpu/rfc/gpusvm:70: drivers/gpu/drm/drm_gpusvm.c:57
#: ../../../gpu/rfc/gpusvm:88: drivers/gpu/drm/drm_pagemap.c:53
msgid "Device Memory Operations:"
msgstr ""

#: ../../../gpu/rfc/gpusvm:70: drivers/gpu/drm/drm_gpusvm.c:58
#: ../../../gpu/rfc/gpusvm:88: drivers/gpu/drm/drm_pagemap.c:54
msgid ""
"Define the interface for driver-specific device memory operations release "
"memory, populate pfns, and copy to / from device memory."
msgstr ""

#: ../../../gpu/rfc/gpusvm:70: drivers/gpu/drm/drm_gpusvm.c:61
msgid ""
"This layer provides interfaces for allocating, mapping, migrating, and "
"releasing memory ranges between the CPU and GPU. It handles all core memory "
"management interactions (DMA mapping, HMM, and migration) and provides "
"driver-specific virtual functions (vfuncs). This infrastructure is "
"sufficient to build the expected driver components for an SVM implementation "
"as detailed below."
msgstr ""

#: ../../../gpu/rfc/gpusvm:70: drivers/gpu/drm/drm_gpusvm.c:68
msgid "Expected Driver Components:"
msgstr ""

#: ../../../gpu/rfc/gpusvm:70: drivers/gpu/drm/drm_gpusvm.c:70
msgid "GPU page fault handler:"
msgstr ""

#: ../../../gpu/rfc/gpusvm:70: drivers/gpu/drm/drm_gpusvm.c:71
msgid ""
"Used to create ranges and notifiers based on the fault address, optionally "
"migrate the range to device memory, and create GPU bindings."
msgstr ""

#: ../../../gpu/rfc/gpusvm:70: drivers/gpu/drm/drm_gpusvm.c:74
msgid "Garbage collector:"
msgstr ""

#: ../../../gpu/rfc/gpusvm:70: drivers/gpu/drm/drm_gpusvm.c:75
msgid ""
"Used to unmap and destroy GPU bindings for ranges.  Ranges are expected to "
"be added to the garbage collector upon a MMU_NOTIFY_UNMAP event in notifier "
"callback."
msgstr ""

#: ../../../gpu/rfc/gpusvm:70: drivers/gpu/drm/drm_gpusvm.c:79
msgid "Notifier callback:"
msgstr ""

#: ../../../gpu/rfc/gpusvm:70: drivers/gpu/drm/drm_gpusvm.c:80
msgid "Used to invalidate and DMA unmap GPU bindings for ranges."
msgstr ""

#: ../../../gpu/rfc/gpusvm:73: drivers/gpu/drm/drm_gpusvm.c:86
msgid ""
"GPU SVM handles locking for core MM interactions, i.e., it locks/unlocks the "
"mmap lock as needed."
msgstr ""

#: ../../../gpu/rfc/gpusvm:73: drivers/gpu/drm/drm_gpusvm.c:89
msgid ""
"GPU SVM introduces a global notifier lock, which safeguards the notifier's "
"range RB tree and list, as well as the range's DMA mappings and sequence "
"number. GPU SVM manages all necessary locking and unlocking operations, "
"except for the recheck range's pages being valid "
"(drm_gpusvm_range_pages_valid) when the driver is committing GPU bindings. "
"This lock corresponds to the ``driver->update`` lock mentioned in "
"Documentation/mm/hmm.rst. Future revisions may transition from a GPU SVM "
"global lock to a per-notifier lock if finer-grained locking is deemed "
"necessary."
msgstr ""

#: ../../../gpu/rfc/gpusvm:73: drivers/gpu/drm/drm_gpusvm.c:99
msgid ""
"In addition to the locking mentioned above, the driver should implement a "
"lock to safeguard core GPU SVM function calls that modify state, such as "
"drm_gpusvm_range_find_or_insert and drm_gpusvm_range_remove. This lock is "
"denoted as 'driver_svm_lock' in code examples. Finer grained driver side "
"locking should also be possible for concurrent GPU fault processing within a "
"single GPU SVM. The 'driver_svm_lock' can be via drm_gpusvm_driver_set_lock "
"to add annotations to GPU SVM."
msgstr ""

#: ../../../gpu/rfc/gpusvm:76: drivers/gpu/drm/drm_gpusvm.c:111
msgid ""
"Partial unmapping of ranges (e.g., 1M out of 2M is unmapped by CPU resulting "
"in MMU_NOTIFY_UNMAP event) presents several challenges, with the main one "
"being that a subset of the range still has CPU and GPU mappings. If the "
"backing store for the range is in device memory, a subset of the backing "
"store has references. One option would be to split the range and device "
"memory backing store, but the implementation for this would be quite "
"complicated. Given that partial unmappings are rare and driver-defined range "
"sizes are relatively small, GPU SVM does not support splitting of ranges."
msgstr ""

#: ../../../gpu/rfc/gpusvm:76: drivers/gpu/drm/drm_gpusvm.c:120
msgid ""
"With no support for range splitting, upon partial unmapping of a range, the "
"driver is expected to invalidate and destroy the entire range. If the range "
"has device memory as its backing, the driver is also expected to migrate any "
"remaining pages back to RAM."
msgstr ""

#: ../../../gpu/rfc/gpusvm:79: drivers/gpu/drm/drm_gpusvm.c:129
msgid ""
"This section provides three examples of how to build the expected driver "
"components: the GPU page fault handler, the garbage collector, and the "
"notifier callback."
msgstr ""

#: ../../../gpu/rfc/gpusvm:79: drivers/gpu/drm/drm_gpusvm.c:133
msgid ""
"The generic code provided does not include logic for complex migration "
"policies, optimized invalidations, fined grained driver locking, or other "
"potentially required driver locking (e.g., DMA-resv locks)."
msgstr ""

#: ../../../gpu/rfc/gpusvm:79: drivers/gpu/drm/drm_gpusvm.c:137
msgid "GPU page fault handler"
msgstr ""

#: ../../../gpu/rfc/gpusvm:79: drivers/gpu/drm/drm_gpusvm.c:202
msgid "Garbage Collector"
msgstr ""

#: ../../../gpu/rfc/gpusvm:79: drivers/gpu/drm/drm_gpusvm.c:227
msgid "Notifier callback"
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:83
msgid "Overview of drm_pagemap design"
msgstr ""

#: ../../../gpu/rfc/gpusvm:85: drivers/gpu/drm/drm_pagemap.c:13
msgid ""
"The DRM pagemap layer is intended to augment the dev_pagemap functionality "
"by providing a way to populate a struct mm_struct virtual range with device "
"private pages and to provide helpers to abstract device memory allocations, "
"to migrate memory back and forth between device memory and system RAM and to "
"handle access (and in the future migration) between devices implementing a "
"fast interconnect that is not necessarily visible to the rest of the system."
msgstr ""

#: ../../../gpu/rfc/gpusvm:85: drivers/gpu/drm/drm_pagemap.c:21
msgid ""
"Typically the DRM pagemap receives requests from one or more DRM GPU SVM "
"instances to populate struct mm_struct virtual ranges with memory, and the "
"migration is best effort only and may thus fail. The implementation should "
"also handle device unbinding by blocking (return an -ENODEV) error for new "
"population requests and after that migrate all device pages to system ram."
msgstr ""

#: ../../../gpu/rfc/gpusvm:88: drivers/gpu/drm/drm_pagemap.c:31
msgid ""
"Migration granularity typically follows the GPU SVM range requests, but if "
"there are clashes, due to races or due to the fact that multiple GPU SVM "
"instances have different views of the ranges used, and because of that parts "
"of a requested range is already present in the requested device memory, the "
"implementation has a variety of options. It can fail and it can choose to "
"populate only the part of the range that isn't already in device memory, and "
"it can evict the range to system before trying to migrate. Ideally an "
"implementation would just try to migrate the missing part of the range and "
"allocate just enough memory to do so."
msgstr ""

#: ../../../gpu/rfc/gpusvm:88: drivers/gpu/drm/drm_pagemap.c:41
msgid ""
"When migrating to system memory as a response to a cpu fault or a device "
"memory eviction request, currently a full device memory allocation is "
"migrated back to system. Moving forward this might need improvement for "
"situations where a single page needs bouncing between system memory and "
"device memory due to, for example, atomic operations."
msgstr ""

#: ../../../gpu/rfc/gpusvm:88: drivers/gpu/drm/drm_pagemap.c:47
msgid "Key DRM pagemap components:"
msgstr ""

#: ../../../gpu/rfc/gpusvm:88: drivers/gpu/drm/drm_pagemap.c:50
msgid ""
"Embedded structure containing enough information for the drm_pagemap to "
"migrate to / from device memory."
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:92
msgid "Possible future design features"
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:94
msgid "Concurrent GPU faults"
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:95
msgid "CPU faults are concurrent so makes sense to have concurrent GPU faults."
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:97
msgid ""
"Should be possible with fined grained locking in the driver GPU fault "
"handler."
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:99
msgid "No expected GPU SVM changes required."
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:100
msgid "Ranges with mixed system and device pages"
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:101
msgid "Can be added if required to drm_gpusvm_get_pages fairly easily."
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:102
msgid "Multi-GPU support"
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:103
msgid ""
"Work in progress and patches expected after initially landing on GPU SVM."
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:105
msgid "Ideally can be done with little to no changes to GPU SVM."
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:106
msgid "Drop ranges in favor of radix tree"
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:107
msgid "May be desirable for faster notifiers."
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:108
msgid "Compound device pages"
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:109
msgid ""
"Nvidia, AMD, and Intel all have agreed expensive core MM functions in "
"migrate device layer are a performance bottleneck, having compound device "
"pages should help increase performance by reducing the number of these "
"expensive calls."
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:113
msgid "Higher order dma mapping for migration"
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:114
msgid ""
"4k dma mapping adversely affects migration performance on Intel hardware, "
"higher order (2M) dma mapping should help here."
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:116
msgid "Build common userptr implementation on top of GPU SVM"
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:117
msgid "Driver side madvise implementation and migration policies"
msgstr ""

#: ../../../gpu/rfc/gpusvm.rst:118
msgid ""
"Pull in pending dma-mapping API changes from Leon / Nvidia when these land"
msgstr ""
