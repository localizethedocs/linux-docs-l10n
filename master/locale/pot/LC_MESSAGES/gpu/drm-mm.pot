# SOME DESCRIPTIVE TITLE.
# Copyright (C) The kernel development community
# This file is distributed under the same license as the The Linux Kernel package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: The Linux Kernel master\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-09-27 13:53+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../gpu/drm-mm.rst:3
msgid "DRM Memory Management"
msgstr ""

#: ../../../gpu/drm-mm.rst:5
msgid ""
"Modern Linux systems require large amount of graphics memory to store frame "
"buffers, textures, vertices and other graphics-related data. Given the very "
"dynamic nature of many of that data, managing graphics memory efficiently is "
"thus crucial for the graphics stack and plays a central role in the DRM "
"infrastructure."
msgstr ""

#: ../../../gpu/drm-mm.rst:11
msgid ""
"The DRM core includes two memory managers, namely Translation Table Manager "
"(TTM) and Graphics Execution Manager (GEM). TTM was the first DRM memory "
"manager to be developed and tried to be a one-size-fits-them all solution. "
"It provides a single userspace API to accommodate the need of all hardware, "
"supporting both Unified Memory Architecture (UMA) devices and devices with "
"dedicated video RAM (i.e. most discrete video cards). This resulted in a "
"large, complex piece of code that turned out to be hard to use for driver "
"development."
msgstr ""

#: ../../../gpu/drm-mm.rst:20
msgid ""
"GEM started as an Intel-sponsored project in reaction to TTM's complexity. "
"Its design philosophy is completely different: instead of providing a "
"solution to every graphics memory-related problems, GEM identified common "
"code between drivers and created a support library to share it. GEM has "
"simpler initialization and execution requirements than TTM, but has no video "
"RAM management capabilities and is thus limited to UMA devices."
msgstr ""

#: ../../../gpu/drm-mm.rst:29
msgid "The Translation Table Manager (TTM)"
msgstr ""

#: ../../../gpu/drm-mm:31: drivers/gpu/drm/ttm/ttm_module.c:43
msgid "TTM is a memory manager for accelerator devices with dedicated memory."
msgstr ""

#: ../../../gpu/drm-mm:31: drivers/gpu/drm/ttm/ttm_module.c:45
msgid ""
"The basic idea is that resources are grouped together in buffer objects of "
"certain size and TTM handles lifetime, movement and CPU mappings of those "
"objects."
msgstr ""

#: ../../../gpu/drm-mm:31: drivers/gpu/drm/ttm/ttm_module.c:49
msgid "TODO: Add more design background and information here."
msgstr ""

#: ../../../gpu/drm-mm:34: include/drm/ttm/ttm_caching.h:33
msgid "CPU caching and BUS snooping behavior."
msgstr ""

#: ../../../gpu/drm-mm:34: include/drm/ttm/ttm_caching.h:37
#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:53 ../../../gpu/drm-mm:463:
#: include/drm/drm_mm.h:64 ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:46
#: include/drm/drm_gpuvm.h:203 include/drm/drm_gpuvm.h:795
#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:58
#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:395
msgid "**Constants**"
msgstr ""

#: ../../../gpu/drm-mm:34: include/drm/ttm/ttm_caching.h:39
msgid "``ttm_uncached``"
msgstr ""

#: ../../../gpu/drm-mm:34: include/drm/ttm/ttm_caching.h:40
msgid ""
"Most defensive option for device mappings, don't even allow write combining."
msgstr ""

#: ../../../gpu/drm-mm:34: include/drm/ttm/ttm_caching.h:43
msgid "``ttm_write_combined``"
msgstr ""

#: ../../../gpu/drm-mm:34: include/drm/ttm/ttm_caching.h:44
msgid "Don't cache read accesses, but allow at least writes to be combined."
msgstr ""

#: ../../../gpu/drm-mm:34: include/drm/ttm/ttm_caching.h:47
msgid "``ttm_cached``"
msgstr ""

#: ../../../gpu/drm-mm:34: include/drm/ttm/ttm_caching.h:48
msgid ""
"Fully cached like normal system memory, requires that devices snoop the CPU "
"cache on accesses."
msgstr ""

#: ../../../gpu/drm-mm.rst:38
msgid "TTM device object reference"
msgstr ""

#: ../../../gpu/drm-mm:40: include/drm/ttm/ttm_device.h:39
msgid "Buffer object driver global data."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:74 include/drm/drm_gem.h:242
#: include/drm/drm_gem.h:277 ../../../gpu/drm-mm:364:
#: include/drm/drm_gem_dma_helper.h:16 ../../../gpu/drm-mm:376:
#: include/drm/drm_gem_shmem_helper.h:25 ../../../gpu/drm-mm:388:
#: include/drm/drm_gem_vram_helper.h:36 include/drm/drm_gem_vram_helper.h:158
#: ../../../gpu/drm-mm:40: include/drm/ttm/ttm_device.h:43
#: include/drm/ttm/ttm_device.h:217 ../../../gpu/drm-mm:439:
#: include/drm/drm_prime.h:44 ../../../gpu/drm-mm:463: include/drm/drm_mm.h:153
#: include/drm/drm_mm.h:187 include/drm/drm_mm.h:223 ../../../gpu/drm-mm:49:
#: include/drm/ttm/ttm_placement.h:78 include/drm/ttm/ttm_placement.h:95
#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:70
#: include/drm/drm_gpuvm.h:219 include/drm/drm_gpuvm.h:510
#: include/drm/drm_gpuvm.h:634 include/drm/drm_gpuvm.h:827
#: include/drm/drm_gpuvm.h:866 include/drm/drm_gpuvm.h:891
#: include/drm/drm_gpuvm.h:931 include/drm/drm_gpuvm.h:944
#: include/drm/drm_gpuvm.h:988 include/drm/drm_gpuvm.h:1091
#: ../../../gpu/drm-mm:532: include/drm/drm_syncobj.h:39
#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:19 ../../../gpu/drm-mm:55:
#: include/drm/ttm/ttm_resource.h:68 include/drm/ttm/ttm_resource.h:180
#: include/drm/ttm/ttm_resource.h:227 include/drm/ttm/ttm_resource.h:244
#: include/drm/ttm/ttm_resource.h:286 include/drm/ttm/ttm_resource.h:299
#: include/drm/ttm/ttm_resource.h:316 include/drm/ttm/ttm_resource.h:347
#: include/drm/ttm/ttm_resource.h:374 ../../../gpu/drm-mm:568:
#: include/drm/gpu_scheduler.h:79 include/drm/gpu_scheduler.h:243
#: include/drm/gpu_scheduler.h:266 include/drm/gpu_scheduler.h:324
#: include/drm/gpu_scheduler.h:411 include/drm/gpu_scheduler.h:543
#: include/drm/gpu_scheduler.h:605 ../../../gpu/drm-mm:64:
#: include/drm/ttm/ttm_tt.h:48 include/drm/ttm/ttm_tt.h:136
#: include/drm/ttm/ttm_tt.h:289 ../../../gpu/drm-mm:73:
#: include/drm/ttm/ttm_pool.h:46 include/drm/ttm/ttm_pool.h:67
msgid "**Definition**::"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:94 include/drm/drm_gem.h:250
#: include/drm/drm_gem.h:302 ../../../gpu/drm-mm:364:
#: include/drm/drm_gem_dma_helper.h:26 ../../../gpu/drm-mm:376:
#: include/drm/drm_gem_shmem_helper.h:42 ../../../gpu/drm-mm:388:
#: include/drm/drm_gem_vram_helper.h:46 include/drm/drm_gem_vram_helper.h:166
#: ../../../gpu/drm-mm:40: include/drm/ttm/ttm_device.h:51
#: include/drm/ttm/ttm_device.h:232 ../../../gpu/drm-mm:439:
#: include/drm/drm_prime.h:49 ../../../gpu/drm-mm:463: include/drm/drm_mm.h:161
#: include/drm/drm_mm.h:193 include/drm/drm_mm.h:228 ../../../gpu/drm-mm:49:
#: include/drm/ttm/ttm_placement.h:87 include/drm/ttm/ttm_placement.h:102
#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:92
#: include/drm/drm_gpuvm.h:247 include/drm/drm_gpuvm.h:523
#: include/drm/drm_gpuvm.h:651 include/drm/drm_gpuvm.h:840
#: include/drm/drm_gpuvm.h:873 include/drm/drm_gpuvm.h:899
#: include/drm/drm_gpuvm.h:937 include/drm/drm_gpuvm.h:957
#: include/drm/drm_gpuvm.h:994 include/drm/drm_gpuvm.h:1105
#: ../../../gpu/drm-mm:532: include/drm/drm_syncobj.h:50
#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:31 ../../../gpu/drm-mm:55:
#: include/drm/ttm/ttm_resource.h:75 include/drm/ttm/ttm_resource.h:195
#: include/drm/ttm/ttm_resource.h:236 include/drm/ttm/ttm_resource.h:257
#: include/drm/ttm/ttm_resource.h:293 include/drm/ttm/ttm_resource.h:306
#: include/drm/ttm/ttm_resource.h:327 include/drm/ttm/ttm_resource.h:362
#: include/drm/ttm/ttm_resource.h:382 ../../../gpu/drm-mm:568:
#: include/drm/gpu_scheduler.h:102 include/drm/gpu_scheduler.h:253
#: include/drm/gpu_scheduler.h:279 include/drm/gpu_scheduler.h:345
#: include/drm/gpu_scheduler.h:421 include/drm/gpu_scheduler.h:572
#: include/drm/gpu_scheduler.h:620 ../../../gpu/drm-mm:64:
#: include/drm/ttm/ttm_tt.h:69 include/drm/ttm/ttm_tt.h:144
#: include/drm/ttm/ttm_tt.h:296 ../../../gpu/drm-mm:73:
#: include/drm/ttm/ttm_pool.h:57 include/drm/ttm/ttm_pool.h:79
msgid "**Members**"
msgstr ""

#: ../../../gpu/drm-mm:40: include/drm/ttm/ttm_device.h:44
msgid "``dummy_read_page``"
msgstr ""

#: ../../../gpu/drm-mm:40: include/drm/ttm/ttm_device.h:45
msgid ""
"Pointer to a dummy page used for mapping requests of unpopulated pages. "
"Constant after init."
msgstr ""

#: ../../../gpu/drm-mm:40: include/drm/ttm/ttm_device.h:50
#: include/drm/ttm/ttm_device.h:217
msgid "``device_list``"
msgstr ""

#: ../../../gpu/drm-mm:40: include/drm/ttm/ttm_device.h:51
msgid "List of buffer object devices. Protected by ttm_global_mutex."
msgstr ""

#: ../../../gpu/drm-mm:40: include/drm/ttm/ttm_device.h:56
msgid "``bo_count``"
msgstr ""

#: ../../../gpu/drm-mm:40: include/drm/ttm/ttm_device.h:57
msgid "Number of buffer objects allocated by devices."
msgstr ""

#: ../../../gpu/drm-mm:40: include/drm/ttm/ttm_device.h:213
msgid "Buffer object driver device-specific data."
msgstr ""

#: ../../../gpu/drm-mm:40: include/drm/ttm/ttm_device.h:218
msgid "Our entry in the global device list. Constant after bo device init"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:417 ../../../gpu/drm-mm:40:
#: include/drm/ttm/ttm_device.h:223
msgid "``funcs``"
msgstr ""

#: ../../../gpu/drm-mm:40: include/drm/ttm/ttm_device.h:224
msgid "Function table for the device. Constant after bo device init"
msgstr ""

#: ../../../gpu/drm-mm:40: include/drm/ttm/ttm_device.h:229
msgid "``sysman``"
msgstr ""

#: ../../../gpu/drm-mm:40: include/drm/ttm/ttm_device.h:230
msgid "Resource manager for the system domain. Access via ttm_manager_type."
msgstr ""

#: ../../../gpu/drm-mm:40: include/drm/ttm/ttm_device.h:235
msgid "``man_drv``"
msgstr ""

#: ../../../gpu/drm-mm:40: include/drm/ttm/ttm_device.h:236
msgid "An array of resource_managers, one per resource type."
msgstr ""

#: ../../../gpu/drm-mm:40: include/drm/ttm/ttm_device.h:240
msgid "``vma_manager``"
msgstr ""

#: ../../../gpu/drm-mm:40: include/drm/ttm/ttm_device.h:241
msgid "Address space manager for finding BOs to mmap."
msgstr ""

#: ../../../gpu/drm-mm:40: include/drm/ttm/ttm_device.h:245
#: ../../../gpu/drm-mm:73: include/drm/ttm/ttm_pool.h:44
msgid "``pool``"
msgstr ""

#: ../../../gpu/drm-mm:40: include/drm/ttm/ttm_device.h:246
msgid "page pool for the device."
msgstr ""

#: ../../../gpu/drm-mm:40: include/drm/ttm/ttm_device.h:250
msgid "``lru_lock``"
msgstr ""

#: ../../../gpu/drm-mm:40: include/drm/ttm/ttm_device.h:251
msgid "Protection for the per manager LRU and ddestroy lists."
msgstr ""

#: ../../../gpu/drm-mm:40: include/drm/ttm/ttm_device.h:255
msgid "``unevictable``"
msgstr ""

#: ../../../gpu/drm-mm:40: include/drm/ttm/ttm_device.h:256
msgid ""
"Buffer objects which are pinned or swapped and as such not on an LRU list."
msgstr ""

#: ../../../gpu/drm-mm:40: include/drm/ttm/ttm_device.h:261
msgid "``dev_mapping``"
msgstr ""

#: ../../../gpu/drm-mm:40: include/drm/ttm/ttm_device.h:262
msgid ""
"A pointer to the struct address_space for invalidating CPU mappings on "
"buffer move. Protected by load/unload sync."
msgstr ""

#: ../../../gpu/drm-mm:40: include/drm/ttm/ttm_device.h:267
msgid "``wq``"
msgstr ""

#: ../../../gpu/drm-mm:40: include/drm/ttm/ttm_device.h:268
msgid "Work queue structure for the delayed delete workqueue."
msgstr ""

#: ../../../gpu/drm-mm:43: drivers/gpu/drm/ttm/ttm_device.c:129
msgid "move GTT BOs to shmem for hibernation."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:465 include/drm/drm_gem.h:500
#: include/drm/drm_gem.h:519 include/drm/drm_gem.h:577
#: include/drm/drm_gem.h:590 include/drm/drm_gem.h:603
#: include/drm/drm_gem.h:625 include/drm/drm_gem.h:641
#: include/drm/drm_gem.h:652 ../../../gpu/drm-mm:355:
#: drivers/gpu/drm/drm_gem.c:122 drivers/gpu/drm/drm_gem.c:158
#: drivers/gpu/drm/drm_gem.c:174 drivers/gpu/drm/drm_gem.c:207
#: drivers/gpu/drm/drm_gem.c:348 drivers/gpu/drm/drm_gem.c:382
#: drivers/gpu/drm/drm_gem.c:497 drivers/gpu/drm/drm_gem.c:521
#: drivers/gpu/drm/drm_gem.c:540 drivers/gpu/drm/drm_gem.c:567
#: drivers/gpu/drm/drm_gem.c:598 drivers/gpu/drm/drm_gem.c:691
#: drivers/gpu/drm/drm_gem.c:765 drivers/gpu/drm/drm_gem.c:820
#: drivers/gpu/drm/drm_gem.c:841 drivers/gpu/drm/drm_gem.c:1028
#: drivers/gpu/drm/drm_gem.c:1048 drivers/gpu/drm/drm_gem.c:1069
#: drivers/gpu/drm/drm_gem.c:1084 drivers/gpu/drm/drm_gem.c:1099
#: drivers/gpu/drm/drm_gem.c:1167 drivers/gpu/drm/drm_gem.c:1313
#: drivers/gpu/drm/drm_gem.c:1391 drivers/gpu/drm/drm_gem.c:1415
#: drivers/gpu/drm/drm_gem.c:1436 drivers/gpu/drm/drm_gem.c:1458
#: drivers/gpu/drm/drm_gem.c:1477 drivers/gpu/drm/drm_gem.c:1581
#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:56
#: include/drm/drm_gem_dma_helper.h:70 include/drm/drm_gem_dma_helper.h:87
#: include/drm/drm_gem_dma_helper.h:123 include/drm/drm_gem_dma_helper.h:160
#: include/drm/drm_gem_dma_helper.h:192 include/drm/drm_gem_dma_helper.h:251
#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:121
#: drivers/gpu/drm/drm_gem_dma_helper.c:223
#: drivers/gpu/drm/drm_gem_dma_helper.c:256
#: drivers/gpu/drm/drm_gem_dma_helper.c:289
#: drivers/gpu/drm/drm_gem_dma_helper.c:329
#: drivers/gpu/drm/drm_gem_dma_helper.c:399
#: drivers/gpu/drm/drm_gem_dma_helper.c:415
#: drivers/gpu/drm/drm_gem_dma_helper.c:449
#: drivers/gpu/drm/drm_gem_dma_helper.c:492
#: drivers/gpu/drm/drm_gem_dma_helper.c:515
#: drivers/gpu/drm/drm_gem_dma_helper.c:558 ../../../gpu/drm-mm:376:
#: include/drm/drm_gem_shmem_helper.h:156
#: include/drm/drm_gem_shmem_helper.h:170
#: include/drm/drm_gem_shmem_helper.h:187
#: include/drm/drm_gem_shmem_helper.h:201
#: include/drm/drm_gem_shmem_helper.h:215
#: include/drm/drm_gem_shmem_helper.h:267 ../../../gpu/drm-mm:379:
#: drivers/gpu/drm/drm_gem_shmem_helper.c:119
#: drivers/gpu/drm/drm_gem_shmem_helper.c:136
#: drivers/gpu/drm/drm_gem_shmem_helper.c:157
#: drivers/gpu/drm/drm_gem_shmem_helper.c:283
#: drivers/gpu/drm/drm_gem_shmem_helper.c:313
#: drivers/gpu/drm/drm_gem_shmem_helper.c:506
#: drivers/gpu/drm/drm_gem_shmem_helper.c:614
#: drivers/gpu/drm/drm_gem_shmem_helper.c:666
#: drivers/gpu/drm/drm_gem_shmem_helper.c:685
#: drivers/gpu/drm/drm_gem_shmem_helper.c:746
#: drivers/gpu/drm/drm_gem_shmem_helper.c:777
#: drivers/gpu/drm/drm_gem_shmem_helper.c:812 ../../../gpu/drm-mm:388:
#: include/drm/drm_gem_vram_helper.h:73 include/drm/drm_gem_vram_helper.h:85
#: include/drm/drm_gem_vram_helper.h:176 ../../../gpu/drm-mm:391:
#: drivers/gpu/drm/drm_gem_vram_helper.c:169
#: drivers/gpu/drm/drm_gem_vram_helper.c:238
#: drivers/gpu/drm/drm_gem_vram_helper.c:260
#: drivers/gpu/drm/drm_gem_vram_helper.c:337
#: drivers/gpu/drm/drm_gem_vram_helper.c:381
#: drivers/gpu/drm/drm_gem_vram_helper.c:414
#: drivers/gpu/drm/drm_gem_vram_helper.c:535
#: drivers/gpu/drm/drm_gem_vram_helper.c:582
#: drivers/gpu/drm/drm_gem_vram_helper.c:633
#: drivers/gpu/drm/drm_gem_vram_helper.c:843
#: drivers/gpu/drm/drm_gem_vram_helper.c:928
#: drivers/gpu/drm/drm_gem_vram_helper.c:984 ../../../gpu/drm-mm:400:
#: drivers/gpu/drm/drm_gem_ttm_helper.c:22
#: drivers/gpu/drm/drm_gem_ttm_helper.c:60
#: drivers/gpu/drm/drm_gem_ttm_helper.c:80
#: drivers/gpu/drm/drm_gem_ttm_helper.c:97
#: drivers/gpu/drm/drm_gem_ttm_helper.c:125 ../../../gpu/drm-mm:409:
#: include/drm/drm_vma_manager.h:88 include/drm/drm_vma_manager.h:111
#: include/drm/drm_vma_manager.h:132 include/drm/drm_vma_manager.h:143
#: include/drm/drm_vma_manager.h:160 include/drm/drm_vma_manager.h:179
#: include/drm/drm_vma_manager.h:196 include/drm/drm_vma_manager.h:213
#: include/drm/drm_vma_manager.h:234 ../../../gpu/drm-mm:412:
#: drivers/gpu/drm/drm_vma_manager.c:76 drivers/gpu/drm/drm_vma_manager.c:99
#: drivers/gpu/drm/drm_vma_manager.c:116 drivers/gpu/drm/drm_vma_manager.c:184
#: drivers/gpu/drm/drm_vma_manager.c:224 drivers/gpu/drm/drm_vma_manager.c:299
#: drivers/gpu/drm/drm_vma_manager.c:325 drivers/gpu/drm/drm_vma_manager.c:350
#: drivers/gpu/drm/drm_vma_manager.c:391 ../../../gpu/drm-mm:43:
#: drivers/gpu/drm/ttm/ttm_device.c:133 drivers/gpu/drm/ttm/ttm_device.c:198
#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:235
#: drivers/gpu/drm/drm_prime.c:265 drivers/gpu/drm/drm_prime.c:286
#: drivers/gpu/drm/drm_prime.c:417 drivers/gpu/drm/drm_prime.c:505
#: drivers/gpu/drm/drm_prime.c:598 drivers/gpu/drm/drm_prime.c:636
#: drivers/gpu/drm/drm_prime.c:662 drivers/gpu/drm/drm_prime.c:703
#: drivers/gpu/drm/drm_prime.c:724 drivers/gpu/drm/drm_prime.c:743
#: drivers/gpu/drm/drm_prime.c:759 drivers/gpu/drm/drm_prime.c:817
#: drivers/gpu/drm/drm_prime.c:846 drivers/gpu/drm/drm_prime.c:884
#: drivers/gpu/drm/drm_prime.c:915 drivers/gpu/drm/drm_prime.c:943
#: drivers/gpu/drm/drm_prime.c:962 drivers/gpu/drm/drm_prime.c:1031
#: drivers/gpu/drm/drm_prime.c:1051 drivers/gpu/drm/drm_prime.c:1079
#: drivers/gpu/drm/drm_prime.c:1105 ../../../gpu/drm-mm:463:
#: include/drm/drm_mm.h:250 include/drm/drm_mm.h:268 include/drm/drm_mm.h:286
#: include/drm/drm_mm.h:308 include/drm/drm_mm.h:330 include/drm/drm_mm.h:346
#: include/drm/drm_mm.h:360 include/drm/drm_mm.h:371 include/drm/drm_mm.h:383
#: include/drm/drm_mm.h:421 include/drm/drm_mm.h:449 include/drm/drm_mm.h:474
#: include/drm/drm_mm.h:490 include/drm/drm_mm.h:518 ../../../gpu/drm-mm:466:
#: drivers/gpu/drm/drm_mm.c:441 drivers/gpu/drm/drm_mm.c:503
#: drivers/gpu/drm/drm_mm.c:624 drivers/gpu/drm/drm_mm.c:688
#: drivers/gpu/drm/drm_mm.c:739 drivers/gpu/drm/drm_mm.c:822
#: drivers/gpu/drm/drm_mm.c:871 drivers/gpu/drm/drm_mm.c:925
#: drivers/gpu/drm/drm_mm.c:960 drivers/gpu/drm/drm_mm.c:988
#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:177
#: include/drm/drm_gpuvm.h:191 include/drm/drm_gpuvm.h:348
#: include/drm/drm_gpuvm.h:373 include/drm/drm_gpuvm.h:386
#: include/drm/drm_gpuvm.h:394 include/drm/drm_gpuvm.h:416
#: include/drm/drm_gpuvm.h:441 include/drm/drm_gpuvm.h:462
#: include/drm/drm_gpuvm.h:486 include/drm/drm_gpuvm.h:497
#: include/drm/drm_gpuvm.h:579 include/drm/drm_gpuvm.h:601
#: include/drm/drm_gpuvm.h:620 include/drm/drm_gpuvm.h:722
#: include/drm/drm_gpuvm.h:746 include/drm/drm_gpuvm.h:766
#: include/drm/drm_gpuvm.h:779 include/drm/drm_gpuvm.h:998
#: include/drm/drm_gpuvm.h:1007 include/drm/drm_gpuvm.h:1019
#: include/drm/drm_gpuvm.h:1030 include/drm/drm_gpuvm.h:1040
#: include/drm/drm_gpuvm.h:1047 include/drm/drm_gpuvm.h:1054
#: include/drm/drm_gpuvm.h:1060 include/drm/drm_gpuvm.h:1237
#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:926
#: drivers/gpu/drm/drm_gpuvm.c:958 drivers/gpu/drm/drm_gpuvm.c:984
#: drivers/gpu/drm/drm_gpuvm.c:1078 drivers/gpu/drm/drm_gpuvm.c:1102
#: drivers/gpu/drm/drm_gpuvm.c:1167 drivers/gpu/drm/drm_gpuvm.c:1203
#: drivers/gpu/drm/drm_gpuvm.c:1237 drivers/gpu/drm/drm_gpuvm.c:1300
#: drivers/gpu/drm/drm_gpuvm.c:1331 drivers/gpu/drm/drm_gpuvm.c:1411
#: drivers/gpu/drm/drm_gpuvm.c:1436 drivers/gpu/drm/drm_gpuvm.c:1464
#: drivers/gpu/drm/drm_gpuvm.c:1532 drivers/gpu/drm/drm_gpuvm.c:1573
#: drivers/gpu/drm/drm_gpuvm.c:1595 drivers/gpu/drm/drm_gpuvm.c:1631
#: drivers/gpu/drm/drm_gpuvm.c:1667 drivers/gpu/drm/drm_gpuvm.c:1690
#: drivers/gpu/drm/drm_gpuvm.c:1749 drivers/gpu/drm/drm_gpuvm.c:1794
#: drivers/gpu/drm/drm_gpuvm.c:1820 drivers/gpu/drm/drm_gpuvm.c:1853
#: drivers/gpu/drm/drm_gpuvm.c:1888 drivers/gpu/drm/drm_gpuvm.c:1906
#: drivers/gpu/drm/drm_gpuvm.c:1935 drivers/gpu/drm/drm_gpuvm.c:1957
#: drivers/gpu/drm/drm_gpuvm.c:1979 drivers/gpu/drm/drm_gpuvm.c:1995
#: drivers/gpu/drm/drm_gpuvm.c:2014 drivers/gpu/drm/drm_gpuvm.c:2046
#: drivers/gpu/drm/drm_gpuvm.c:2307 drivers/gpu/drm/drm_gpuvm.c:2357
#: drivers/gpu/drm/drm_gpuvm.c:2424 drivers/gpu/drm/drm_gpuvm.c:2502
#: drivers/gpu/drm/drm_gpuvm.c:2616 drivers/gpu/drm/drm_gpuvm.c:2683
#: drivers/gpu/drm/drm_gpuvm.c:2745 drivers/gpu/drm/drm_gpuvm.c:2799
#: drivers/gpu/drm/drm_gpuvm.c:2853 ../../../gpu/drm-mm:515:
#: drivers/gpu/drm/drm_buddy.c:230 drivers/gpu/drm/drm_buddy.c:326
#: drivers/gpu/drm/drm_buddy.c:396 drivers/gpu/drm/drm_buddy.c:413
#: drivers/gpu/drm/drm_buddy.c:456 drivers/gpu/drm/drm_buddy.c:505
#: drivers/gpu/drm/drm_buddy.c:904 drivers/gpu/drm/drm_buddy.c:1011
#: drivers/gpu/drm/drm_buddy.c:1177 drivers/gpu/drm/drm_buddy.c:1195
#: ../../../gpu/drm-mm:521: drivers/gpu/drm/drm_cache.c:83
#: drivers/gpu/drm/drm_cache.c:124 drivers/gpu/drm/drm_cache.c:153
#: drivers/gpu/drm/drm_cache.c:297 ../../../gpu/drm-mm:532:
#: include/drm/drm_syncobj.h:77 include/drm/drm_syncobj.h:90
#: include/drm/drm_syncobj.h:100 ../../../gpu/drm-mm:535:
#: drivers/gpu/drm/drm_syncobj.c:245 drivers/gpu/drm/drm_syncobj.c:329
#: drivers/gpu/drm/drm_syncobj.c:370 drivers/gpu/drm/drm_syncobj.c:426
#: drivers/gpu/drm/drm_syncobj.c:525 drivers/gpu/drm/drm_syncobj.c:547
#: drivers/gpu/drm/drm_syncobj.c:590 drivers/gpu/drm/drm_syncobj.c:669
#: drivers/gpu/drm/drm_syncobj.c:1224 ../../../gpu/drm-mm:544:
#: include/drm/drm_exec.h:59 include/drm/drm_exec.h:73
#: include/drm/drm_exec.h:84 include/drm/drm_exec.h:99
#: include/drm/drm_exec.h:118 include/drm/drm_exec.h:131
#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:75
#: drivers/gpu/drm/drm_exec.c:102 drivers/gpu/drm/drm_exec.c:120
#: drivers/gpu/drm/drm_exec.c:203 drivers/gpu/drm/drm_exec.c:258
#: drivers/gpu/drm/drm_exec.c:285 drivers/gpu/drm/drm_exec.c:315
#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:78
#: include/drm/ttm/ttm_resource.h:274 include/drm/ttm/ttm_resource.h:386
#: include/drm/ttm/ttm_resource.h:405 include/drm/ttm/ttm_resource.h:419
#: include/drm/ttm/ttm_resource.h:484 ../../../gpu/drm-mm:571:
#: drivers/gpu/drm/scheduler/sched_main.c:444
#: drivers/gpu/drm/scheduler/sched_main.c:460
#: drivers/gpu/drm/scheduler/sched_main.c:474
#: drivers/gpu/drm/scheduler/sched_main.c:504
#: drivers/gpu/drm/scheduler/sched_main.c:606
#: drivers/gpu/drm/scheduler/sched_main.c:699
#: drivers/gpu/drm/scheduler/sched_main.c:743
#: drivers/gpu/drm/scheduler/sched_main.c:796
#: drivers/gpu/drm/scheduler/sched_main.c:864
#: drivers/gpu/drm/scheduler/sched_main.c:897
#: drivers/gpu/drm/scheduler/sched_main.c:943
#: drivers/gpu/drm/scheduler/sched_main.c:971
#: drivers/gpu/drm/scheduler/sched_main.c:1006
#: drivers/gpu/drm/scheduler/sched_main.c:1030
#: drivers/gpu/drm/scheduler/sched_main.c:1053
#: drivers/gpu/drm/scheduler/sched_main.c:1183
#: drivers/gpu/drm/scheduler/sched_main.c:1316
#: drivers/gpu/drm/scheduler/sched_main.c:1414
#: drivers/gpu/drm/scheduler/sched_main.c:1470
#: drivers/gpu/drm/scheduler/sched_main.c:1513
#: drivers/gpu/drm/scheduler/sched_main.c:1526
#: drivers/gpu/drm/scheduler/sched_main.c:1541 ../../../gpu/drm-mm:574:
#: drivers/gpu/drm/scheduler/sched_entity.c:40
#: drivers/gpu/drm/scheduler/sched_entity.c:122
#: drivers/gpu/drm/scheduler/sched_entity.c:159
#: drivers/gpu/drm/scheduler/sched_entity.c:271
#: drivers/gpu/drm/scheduler/sched_entity.c:318
#: drivers/gpu/drm/scheduler/sched_entity.c:349
#: drivers/gpu/drm/scheduler/sched_entity.c:378
#: drivers/gpu/drm/scheduler/sched_entity.c:566 ../../../gpu/drm-mm:58:
#: drivers/gpu/drm/ttm/ttm_resource.c:123
#: drivers/gpu/drm/ttm/ttm_resource.c:136
#: drivers/gpu/drm/ttm/ttm_resource.c:153
#: drivers/gpu/drm/ttm/ttm_resource.c:323
#: drivers/gpu/drm/ttm/ttm_resource.c:358
#: drivers/gpu/drm/ttm/ttm_resource.c:516
#: drivers/gpu/drm/ttm/ttm_resource.c:584
#: drivers/gpu/drm/ttm/ttm_resource.c:602
#: drivers/gpu/drm/ttm/ttm_resource.c:763
#: drivers/gpu/drm/ttm/ttm_resource.c:921 ../../../gpu/drm-mm:64:
#: include/drm/ttm/ttm_tt.h:153 include/drm/ttm/ttm_tt.h:164
#: include/drm/ttm/ttm_tt.h:175 include/drm/ttm/ttm_tt.h:187
#: include/drm/ttm/ttm_tt.h:198 include/drm/ttm/ttm_tt.h:218
#: include/drm/ttm/ttm_tt.h:227 include/drm/ttm/ttm_tt.h:237
#: include/drm/ttm/ttm_tt.h:248 include/drm/ttm/ttm_tt.h:260
#: include/drm/ttm/ttm_tt.h:270 include/drm/ttm/ttm_tt.h:311
#: ../../../gpu/drm-mm:67: drivers/gpu/drm/ttm/ttm_tt.c:513
#: drivers/gpu/drm/ttm/ttm_tt.c:541 ../../../gpu/drm-mm:76:
#: drivers/gpu/drm/ttm/ttm_pool.c:802 drivers/gpu/drm/ttm/ttm_pool.c:880
#: drivers/gpu/drm/ttm/ttm_pool.c:1060 drivers/gpu/drm/ttm/ttm_pool.c:1110
#: drivers/gpu/drm/ttm/ttm_pool.c:1235
msgid "**Parameters**"
msgstr ""

#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:178
#: ../../../gpu/drm-mm:43: drivers/gpu/drm/ttm/ttm_device.c:135
#: drivers/gpu/drm/ttm/ttm_device.c:200 ../../../gpu/drm-mm:58:
#: drivers/gpu/drm/ttm/ttm_resource.c:138
#: drivers/gpu/drm/ttm/ttm_resource.c:516 ../../../gpu/drm-mm:64:
#: include/drm/ttm/ttm_tt.h:229 include/drm/ttm/ttm_tt.h:250
#: include/drm/ttm/ttm_tt.h:262
msgid "``struct ttm_device *bdev``"
msgstr ""

#: ../../../gpu/drm-mm:43: drivers/gpu/drm/ttm/ttm_device.c:131
msgid "A pointer to a struct ttm_device to prepare hibernation for."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:589 ../../../gpu/drm-mm:355:
#: drivers/gpu/drm/drm_gem.c:386 drivers/gpu/drm/drm_gem.c:771
#: drivers/gpu/drm/drm_gem.c:821 drivers/gpu/drm/drm_gem.c:844
#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:88
#: include/drm/drm_gem_dma_helper.h:125 ../../../gpu/drm-mm:367:
#: drivers/gpu/drm/drm_gem_dma_helper.c:129
#: drivers/gpu/drm/drm_gem_dma_helper.c:261
#: drivers/gpu/drm/drm_gem_dma_helper.c:298
#: drivers/gpu/drm/drm_gem_dma_helper.c:336
#: drivers/gpu/drm/drm_gem_dma_helper.c:417
#: drivers/gpu/drm/drm_gem_dma_helper.c:456
#: drivers/gpu/drm/drm_gem_dma_helper.c:497
#: drivers/gpu/drm/drm_gem_dma_helper.c:518
#: drivers/gpu/drm/drm_gem_dma_helper.c:568 ../../../gpu/drm-mm:376:
#: include/drm/drm_gem_shmem_helper.h:216
#: include/drm/drm_gem_shmem_helper.h:269 ../../../gpu/drm-mm:379:
#: drivers/gpu/drm/drm_gem_shmem_helper.c:120
#: drivers/gpu/drm/drm_gem_shmem_helper.c:139
#: drivers/gpu/drm/drm_gem_shmem_helper.c:284
#: drivers/gpu/drm/drm_gem_shmem_helper.c:514
#: drivers/gpu/drm/drm_gem_shmem_helper.c:616
#: drivers/gpu/drm/drm_gem_shmem_helper.c:690
#: drivers/gpu/drm/drm_gem_shmem_helper.c:753
#: drivers/gpu/drm/drm_gem_shmem_helper.c:782 ../../../gpu/drm-mm:388:
#: include/drm/drm_gem_vram_helper.h:73 include/drm/drm_gem_vram_helper.h:85
#: include/drm/drm_gem_vram_helper.h:176 ../../../gpu/drm-mm:391:
#: drivers/gpu/drm/drm_gem_vram_helper.c:175
#: drivers/gpu/drm/drm_gem_vram_helper.c:261
#: drivers/gpu/drm/drm_gem_vram_helper.c:344
#: drivers/gpu/drm/drm_gem_vram_helper.c:424
#: drivers/gpu/drm/drm_gem_vram_helper.c:539
#: drivers/gpu/drm/drm_gem_vram_helper.c:586
#: drivers/gpu/drm/drm_gem_vram_helper.c:934
#: drivers/gpu/drm/drm_gem_vram_helper.c:999 ../../../gpu/drm-mm:400:
#: drivers/gpu/drm/drm_gem_ttm_helper.c:62
#: drivers/gpu/drm/drm_gem_ttm_helper.c:132 ../../../gpu/drm-mm:409:
#: include/drm/drm_vma_manager.h:91 include/drm/drm_vma_manager.h:164
#: include/drm/drm_vma_manager.h:181 include/drm/drm_vma_manager.h:198
#: include/drm/drm_vma_manager.h:237 ../../../gpu/drm-mm:412:
#: drivers/gpu/drm/drm_vma_manager.c:134 drivers/gpu/drm/drm_vma_manager.c:198
#: drivers/gpu/drm/drm_vma_manager.c:310 drivers/gpu/drm/drm_vma_manager.c:335
#: drivers/gpu/drm/drm_vma_manager.c:395 ../../../gpu/drm-mm:43:
#: drivers/gpu/drm/ttm/ttm_device.c:133 drivers/gpu/drm/ttm/ttm_device.c:197
#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:665
#: drivers/gpu/drm/drm_prime.c:944 ../../../gpu/drm-mm:463:
#: include/drm/drm_mm.h:254 include/drm/drm_mm.h:272 include/drm/drm_mm.h:289
#: include/drm/drm_mm.h:310 include/drm/drm_mm.h:332 include/drm/drm_mm.h:349
#: include/drm/drm_mm.h:429 include/drm/drm_mm.h:454 include/drm/drm_mm.h:473
#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:446
#: drivers/gpu/drm/drm_mm.c:510 drivers/gpu/drm/drm_mm.c:741
#: drivers/gpu/drm/drm_mm.c:831 drivers/gpu/drm/drm_mm.c:873
#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:191
#: include/drm/drm_gpuvm.h:349 include/drm/drm_gpuvm.h:373
#: include/drm/drm_gpuvm.h:385 include/drm/drm_gpuvm.h:394
#: include/drm/drm_gpuvm.h:417 include/drm/drm_gpuvm.h:580
#: include/drm/drm_gpuvm.h:620 include/drm/drm_gpuvm.h:723
#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:929
#: drivers/gpu/drm/drm_gpuvm.c:960 drivers/gpu/drm/drm_gpuvm.c:1108
#: drivers/gpu/drm/drm_gpuvm.c:1183 drivers/gpu/drm/drm_gpuvm.c:1209
#: drivers/gpu/drm/drm_gpuvm.c:1243 drivers/gpu/drm/drm_gpuvm.c:1303
#: drivers/gpu/drm/drm_gpuvm.c:1334 drivers/gpu/drm/drm_gpuvm.c:1413
#: drivers/gpu/drm/drm_gpuvm.c:1466 drivers/gpu/drm/drm_gpuvm.c:1539
#: drivers/gpu/drm/drm_gpuvm.c:1577 drivers/gpu/drm/drm_gpuvm.c:1602
#: drivers/gpu/drm/drm_gpuvm.c:1638 drivers/gpu/drm/drm_gpuvm.c:1755
#: drivers/gpu/drm/drm_gpuvm.c:1889 drivers/gpu/drm/drm_gpuvm.c:1907
#: drivers/gpu/drm/drm_gpuvm.c:1939 drivers/gpu/drm/drm_gpuvm.c:1961
#: drivers/gpu/drm/drm_gpuvm.c:1981 drivers/gpu/drm/drm_gpuvm.c:2331
#: drivers/gpu/drm/drm_gpuvm.c:2376 drivers/gpu/drm/drm_gpuvm.c:2475
#: drivers/gpu/drm/drm_gpuvm.c:2508 drivers/gpu/drm/drm_gpuvm.c:2640
#: drivers/gpu/drm/drm_gpuvm.c:2704 drivers/gpu/drm/drm_gpuvm.c:2755
#: drivers/gpu/drm/drm_gpuvm.c:2809 ../../../gpu/drm-mm:515:
#: drivers/gpu/drm/drm_buddy.c:233 drivers/gpu/drm/drm_buddy.c:914
#: drivers/gpu/drm/drm_buddy.c:1022 ../../../gpu/drm-mm:532:
#: include/drm/drm_syncobj.h:102 ../../../gpu/drm-mm:544:
#: include/drm/drm_exec.h:59 ../../../gpu/drm-mm:547:
#: drivers/gpu/drm/drm_exec.c:204 drivers/gpu/drm/drm_exec.c:287
#: drivers/gpu/drm/drm_exec.c:319 ../../../gpu/drm-mm:55:
#: include/drm/ttm/ttm_resource.h:273 include/drm/ttm/ttm_resource.h:404
#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:898
#: drivers/gpu/drm/scheduler/sched_main.c:946
#: drivers/gpu/drm/scheduler/sched_main.c:974
#: drivers/gpu/drm/scheduler/sched_main.c:1012
#: drivers/gpu/drm/scheduler/sched_main.c:1030 ../../../gpu/drm-mm:58:
#: drivers/gpu/drm/ttm/ttm_resource.c:767 ../../../gpu/drm-mm:64:
#: include/drm/ttm/ttm_tt.h:152 include/drm/ttm/ttm_tt.h:163
#: include/drm/ttm/ttm_tt.h:198 ../../../gpu/drm-mm:67:
#: drivers/gpu/drm/ttm/ttm_tt.c:513 drivers/gpu/drm/ttm/ttm_tt.c:543
#: ../../../gpu/drm-mm:76: drivers/gpu/drm/ttm/ttm_pool.c:806
msgid "**Return**"
msgstr ""

#: ../../../gpu/drm-mm:43: drivers/gpu/drm/ttm/ttm_device.c:133
msgid "0 on success, negative number on failure."
msgstr ""

#: ../../../gpu/drm-mm:43: drivers/gpu/drm/ttm/ttm_device.c:196
msgid "A pointer to a struct ttm_device to initialize."
msgstr ""

#: ../../../gpu/drm-mm:43: drivers/gpu/drm/ttm/ttm_device.c:198
msgid "``const struct ttm_device_funcs *funcs``"
msgstr ""

#: ../../../gpu/drm-mm:43: drivers/gpu/drm/ttm/ttm_device.c:197
msgid "Function table for the device."
msgstr ""

#: ../../../gpu/drm-mm:43: drivers/gpu/drm/ttm/ttm_device.c:199
#: ../../../gpu/drm-mm:76: drivers/gpu/drm/ttm/ttm_pool.c:1060
msgid "``struct device *dev``"
msgstr ""

#: ../../../gpu/drm-mm:43: drivers/gpu/drm/ttm/ttm_device.c:198
msgid "The core kernel device pointer for DMA mappings and allocations."
msgstr ""

#: ../../../gpu/drm-mm:43: drivers/gpu/drm/ttm/ttm_device.c:200
msgid "``struct address_space *mapping``"
msgstr ""

#: ../../../gpu/drm-mm:43: drivers/gpu/drm/ttm/ttm_device.c:199
msgid "The address space to use for this bo."
msgstr ""

#: ../../../gpu/drm-mm:43: drivers/gpu/drm/ttm/ttm_device.c:201
msgid "``struct drm_vma_offset_manager *vma_manager``"
msgstr ""

#: ../../../gpu/drm-mm:43: drivers/gpu/drm/ttm/ttm_device.c:200
msgid "A pointer to a vma manager."
msgstr ""

#: ../../../gpu/drm-mm:43: drivers/gpu/drm/ttm/ttm_device.c:202
#: ../../../gpu/drm-mm:76: drivers/gpu/drm/ttm/ttm_pool.c:1062
msgid "``bool use_dma_alloc``"
msgstr ""

#: ../../../gpu/drm-mm:43: drivers/gpu/drm/ttm/ttm_device.c:201
msgid "If coherent DMA allocation API should be used."
msgstr ""

#: ../../../gpu/drm-mm:43: drivers/gpu/drm/ttm/ttm_device.c:203
#: ../../../gpu/drm-mm:76: drivers/gpu/drm/ttm/ttm_pool.c:1063
msgid "``bool use_dma32``"
msgstr ""

#: ../../../gpu/drm-mm:43: drivers/gpu/drm/ttm/ttm_device.c:202
msgid "If we should use GFP_DMA32 for device memory allocations."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:64 include/drm/drm_gem.h:269
#: include/drm/drm_gem.h:439 include/drm/drm_gem.h:445
#: include/drm/drm_gem.h:464 include/drm/drm_gem.h:499
#: include/drm/drm_gem.h:518 include/drm/drm_gem.h:580
#: include/drm/drm_gem.h:604 include/drm/drm_gem.h:624
#: include/drm/drm_gem.h:641 include/drm/drm_gem.h:654 ../../../gpu/drm-mm:355:
#: drivers/gpu/drm/drm_gem.c:127 drivers/gpu/drm/drm_gem.c:159
#: drivers/gpu/drm/drm_gem.c:175 drivers/gpu/drm/drm_gem.c:206
#: drivers/gpu/drm/drm_gem.c:348 drivers/gpu/drm/drm_gem.c:384
#: drivers/gpu/drm/drm_gem.c:498 drivers/gpu/drm/drm_gem.c:520
#: drivers/gpu/drm/drm_gem.c:540 drivers/gpu/drm/drm_gem.c:566
#: drivers/gpu/drm/drm_gem.c:598 drivers/gpu/drm/drm_gem.c:767
#: drivers/gpu/drm/drm_gem.c:820 drivers/gpu/drm/drm_gem.c:1027
#: drivers/gpu/drm/drm_gem.c:1047 drivers/gpu/drm/drm_gem.c:1068
#: drivers/gpu/drm/drm_gem.c:1083 drivers/gpu/drm/drm_gem.c:1100
#: drivers/gpu/drm/drm_gem.c:1167 drivers/gpu/drm/drm_gem.c:1321
#: drivers/gpu/drm/drm_gem.c:1417 drivers/gpu/drm/drm_gem.c:1439
#: drivers/gpu/drm/drm_gem.c:1463 drivers/gpu/drm/drm_gem.c:1488
#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:55
#: include/drm/drm_gem_dma_helper.h:71 include/drm/drm_gem_dma_helper.h:86
#: include/drm/drm_gem_dma_helper.h:123 include/drm/drm_gem_dma_helper.h:159
#: include/drm/drm_gem_dma_helper.h:175 include/drm/drm_gem_dma_helper.h:193
#: include/drm/drm_gem_dma_helper.h:210 include/drm/drm_gem_dma_helper.h:250
#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:121
#: drivers/gpu/drm/drm_gem_dma_helper.c:222
#: drivers/gpu/drm/drm_gem_dma_helper.c:257
#: drivers/gpu/drm/drm_gem_dma_helper.c:290
#: drivers/gpu/drm/drm_gem_dma_helper.c:332
#: drivers/gpu/drm/drm_gem_dma_helper.c:400
#: drivers/gpu/drm/drm_gem_dma_helper.c:415
#: drivers/gpu/drm/drm_gem_dma_helper.c:451
#: drivers/gpu/drm/drm_gem_dma_helper.c:494
#: drivers/gpu/drm/drm_gem_dma_helper.c:515
#: drivers/gpu/drm/drm_gem_dma_helper.c:560 ../../../gpu/drm-mm:376:
#: include/drm/drm_gem_shmem_helper.h:155
#: include/drm/drm_gem_shmem_helper.h:171
#: include/drm/drm_gem_shmem_helper.h:186
#: include/drm/drm_gem_shmem_helper.h:200
#: include/drm/drm_gem_shmem_helper.h:214
#: include/drm/drm_gem_shmem_helper.h:267
#: include/drm/drm_gem_shmem_helper.h:296 ../../../gpu/drm-mm:379:
#: drivers/gpu/drm/drm_gem_shmem_helper.c:119
#: drivers/gpu/drm/drm_gem_shmem_helper.c:138
#: drivers/gpu/drm/drm_gem_shmem_helper.c:156
#: drivers/gpu/drm/drm_gem_shmem_helper.c:282
#: drivers/gpu/drm/drm_gem_shmem_helper.c:312
#: drivers/gpu/drm/drm_gem_shmem_helper.c:507
#: drivers/gpu/drm/drm_gem_shmem_helper.c:614
#: drivers/gpu/drm/drm_gem_shmem_helper.c:685
#: drivers/gpu/drm/drm_gem_shmem_helper.c:746
#: drivers/gpu/drm/drm_gem_shmem_helper.c:779
#: drivers/gpu/drm/drm_gem_shmem_helper.c:812 ../../../gpu/drm-mm:388:
#: include/drm/drm_gem_vram_helper.h:41 include/drm/drm_gem_vram_helper.h:128
#: include/drm/drm_gem_vram_helper.h:140 include/drm/drm_gem_vram_helper.h:161
#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:170
#: drivers/gpu/drm/drm_gem_vram_helper.c:237
#: drivers/gpu/drm/drm_gem_vram_helper.c:259
#: drivers/gpu/drm/drm_gem_vram_helper.c:339
#: drivers/gpu/drm/drm_gem_vram_helper.c:381
#: drivers/gpu/drm/drm_gem_vram_helper.c:420
#: drivers/gpu/drm/drm_gem_vram_helper.c:537
#: drivers/gpu/drm/drm_gem_vram_helper.c:583
#: drivers/gpu/drm/drm_gem_vram_helper.c:634
#: drivers/gpu/drm/drm_gem_vram_helper.c:930
#: drivers/gpu/drm/drm_gem_vram_helper.c:985 ../../../gpu/drm-mm:400:
#: drivers/gpu/drm/drm_gem_ttm_helper.c:23
#: drivers/gpu/drm/drm_gem_ttm_helper.c:60
#: drivers/gpu/drm/drm_gem_ttm_helper.c:80
#: drivers/gpu/drm/drm_gem_ttm_helper.c:97
#: drivers/gpu/drm/drm_gem_ttm_helper.c:127 ../../../gpu/drm-mm:409:
#: include/drm/drm_vma_manager.h:89 include/drm/drm_vma_manager.h:110
#: include/drm/drm_vma_manager.h:131 include/drm/drm_vma_manager.h:142
#: include/drm/drm_vma_manager.h:159 include/drm/drm_vma_manager.h:178
#: include/drm/drm_vma_manager.h:195 include/drm/drm_vma_manager.h:213
#: include/drm/drm_vma_manager.h:234 ../../../gpu/drm-mm:412:
#: drivers/gpu/drm/drm_vma_manager.c:77 drivers/gpu/drm/drm_vma_manager.c:98
#: drivers/gpu/drm/drm_vma_manager.c:117 drivers/gpu/drm/drm_vma_manager.c:185
#: drivers/gpu/drm/drm_vma_manager.c:224 drivers/gpu/drm/drm_vma_manager.c:299
#: drivers/gpu/drm/drm_vma_manager.c:325 drivers/gpu/drm/drm_vma_manager.c:350
#: drivers/gpu/drm/drm_vma_manager.c:391 ../../../gpu/drm-mm:439:
#: include/drm/drm_prime.h:52 ../../../gpu/drm-mm:43:
#: drivers/gpu/drm/ttm/ttm_device.c:204 ../../../gpu/drm-mm:442:
#: drivers/gpu/drm/drm_prime.c:235 drivers/gpu/drm/drm_prime.c:264
#: drivers/gpu/drm/drm_prime.c:288 drivers/gpu/drm/drm_prime.c:419
#: drivers/gpu/drm/drm_prime.c:508 drivers/gpu/drm/drm_prime.c:598
#: drivers/gpu/drm/drm_prime.c:636 drivers/gpu/drm/drm_prime.c:662
#: drivers/gpu/drm/drm_prime.c:704 drivers/gpu/drm/drm_prime.c:724
#: drivers/gpu/drm/drm_prime.c:743 drivers/gpu/drm/drm_prime.c:759
#: drivers/gpu/drm/drm_prime.c:817 drivers/gpu/drm/drm_prime.c:847
#: drivers/gpu/drm/drm_prime.c:883 drivers/gpu/drm/drm_prime.c:915
#: drivers/gpu/drm/drm_prime.c:963 drivers/gpu/drm/drm_prime.c:1031
#: drivers/gpu/drm/drm_prime.c:1052 drivers/gpu/drm/drm_prime.c:1080
#: drivers/gpu/drm/drm_prime.c:1105 ../../../gpu/drm-mm:463:
#: include/drm/drm_mm.h:114 include/drm/drm_mm.h:166 include/drm/drm_mm.h:200
#: include/drm/drm_mm.h:231 include/drm/drm_mm.h:249 include/drm/drm_mm.h:267
#: include/drm/drm_mm.h:285 include/drm/drm_mm.h:307 include/drm/drm_mm.h:329
#: include/drm/drm_mm.h:345 include/drm/drm_mm.h:360 include/drm/drm_mm.h:372
#: include/drm/drm_mm.h:385 include/drm/drm_mm.h:425 include/drm/drm_mm.h:450
#: include/drm/drm_mm.h:493 include/drm/drm_mm.h:522 ../../../gpu/drm-mm:466:
#: drivers/gpu/drm/drm_mm.c:441 drivers/gpu/drm/drm_mm.c:509
#: drivers/gpu/drm/drm_mm.c:623 drivers/gpu/drm/drm_mm.c:694
#: drivers/gpu/drm/drm_mm.c:739 drivers/gpu/drm/drm_mm.c:822
#: drivers/gpu/drm/drm_mm.c:870 drivers/gpu/drm/drm_mm.c:926
#: drivers/gpu/drm/drm_mm.c:959 ../../../gpu/drm-mm:49:
#: include/drm/ttm/ttm_placement.h:83 include/drm/ttm/ttm_placement.h:98
#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:148
#: include/drm/drm_gpuvm.h:333 include/drm/drm_gpuvm.h:347
#: include/drm/drm_gpuvm.h:444 include/drm/drm_gpuvm.h:467
#: include/drm/drm_gpuvm.h:486 include/drm/drm_gpuvm.h:498
#: include/drm/drm_gpuvm.h:550 include/drm/drm_gpuvm.h:578
#: include/drm/drm_gpuvm.h:603 include/drm/drm_gpuvm.h:619
#: include/drm/drm_gpuvm.h:704 include/drm/drm_gpuvm.h:721
#: include/drm/drm_gpuvm.h:747 include/drm/drm_gpuvm.h:766
#: include/drm/drm_gpuvm.h:781 include/drm/drm_gpuvm.h:812
#: include/drm/drm_gpuvm.h:859 include/drm/drm_gpuvm.h:883
#: include/drm/drm_gpuvm.h:925 include/drm/drm_gpuvm.h:938
#: include/drm/drm_gpuvm.h:981 include/drm/drm_gpuvm.h:998
#: include/drm/drm_gpuvm.h:1008 include/drm/drm_gpuvm.h:1019
#: include/drm/drm_gpuvm.h:1030 include/drm/drm_gpuvm.h:1206
#: include/drm/drm_gpuvm.h:1239 ../../../gpu/drm-mm:506:
#: drivers/gpu/drm/drm_gpuvm.c:928 drivers/gpu/drm/drm_gpuvm.c:957
#: drivers/gpu/drm/drm_gpuvm.c:992 drivers/gpu/drm/drm_gpuvm.c:1077
#: drivers/gpu/drm/drm_gpuvm.c:1103 drivers/gpu/drm/drm_gpuvm.c:1168
#: drivers/gpu/drm/drm_gpuvm.c:1206 drivers/gpu/drm/drm_gpuvm.c:1236
#: drivers/gpu/drm/drm_gpuvm.c:1301 drivers/gpu/drm/drm_gpuvm.c:1332
#: drivers/gpu/drm/drm_gpuvm.c:1411 drivers/gpu/drm/drm_gpuvm.c:1464
#: drivers/gpu/drm/drm_gpuvm.c:1531 drivers/gpu/drm/drm_gpuvm.c:1574
#: drivers/gpu/drm/drm_gpuvm.c:1596 drivers/gpu/drm/drm_gpuvm.c:1631
#: drivers/gpu/drm/drm_gpuvm.c:1667 drivers/gpu/drm/drm_gpuvm.c:1691
#: drivers/gpu/drm/drm_gpuvm.c:1749 drivers/gpu/drm/drm_gpuvm.c:1793
#: drivers/gpu/drm/drm_gpuvm.c:1820 drivers/gpu/drm/drm_gpuvm.c:1852
#: drivers/gpu/drm/drm_gpuvm.c:1935 drivers/gpu/drm/drm_gpuvm.c:1957
#: drivers/gpu/drm/drm_gpuvm.c:1997 drivers/gpu/drm/drm_gpuvm.c:2016
#: drivers/gpu/drm/drm_gpuvm.c:2046 drivers/gpu/drm/drm_gpuvm.c:2311
#: drivers/gpu/drm/drm_gpuvm.c:2359 drivers/gpu/drm/drm_gpuvm.c:2429
#: drivers/gpu/drm/drm_gpuvm.c:2504 drivers/gpu/drm/drm_gpuvm.c:2619
#: drivers/gpu/drm/drm_gpuvm.c:2685 drivers/gpu/drm/drm_gpuvm.c:2746
#: drivers/gpu/drm/drm_gpuvm.c:2798 drivers/gpu/drm/drm_gpuvm.c:2853
#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:232
#: drivers/gpu/drm/drm_buddy.c:326 drivers/gpu/drm/drm_buddy.c:396
#: drivers/gpu/drm/drm_buddy.c:414 drivers/gpu/drm/drm_buddy.c:911
#: drivers/gpu/drm/drm_buddy.c:1017 ../../../gpu/drm-mm:521:
#: drivers/gpu/drm/drm_cache.c:83 drivers/gpu/drm/drm_cache.c:123
#: drivers/gpu/drm/drm_cache.c:153 drivers/gpu/drm/drm_cache.c:299
#: ../../../gpu/drm-mm:532: include/drm/drm_syncobj.h:69
#: include/drm/drm_syncobj.h:76 include/drm/drm_syncobj.h:99
#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:245
#: drivers/gpu/drm/drm_syncobj.c:331 drivers/gpu/drm/drm_syncobj.c:370
#: drivers/gpu/drm/drm_syncobj.c:429 drivers/gpu/drm/drm_syncobj.c:524
#: drivers/gpu/drm/drm_syncobj.c:548 drivers/gpu/drm/drm_syncobj.c:591
#: drivers/gpu/drm/drm_syncobj.c:669 drivers/gpu/drm/drm_syncobj.c:1224
#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:74 include/drm/drm_exec.h:86
#: include/drm/drm_exec.h:98 include/drm/drm_exec.h:117
#: include/drm/drm_exec.h:130 ../../../gpu/drm-mm:547:
#: drivers/gpu/drm/drm_exec.c:76 drivers/gpu/drm/drm_exec.c:101
#: drivers/gpu/drm/drm_exec.c:119 drivers/gpu/drm/drm_exec.c:203
#: drivers/gpu/drm/drm_exec.c:258 drivers/gpu/drm/drm_exec.c:286
#: drivers/gpu/drm/drm_exec.c:317 ../../../gpu/drm-mm:55:
#: include/drm/ttm/ttm_resource.h:221 include/drm/ttm/ttm_resource.h:232
#: include/drm/ttm/ttm_resource.h:268 include/drm/ttm/ttm_resource.h:289
#: include/drm/ttm/ttm_resource.h:302 include/drm/ttm/ttm_resource.h:328
#: include/drm/ttm/ttm_resource.h:387 include/drm/ttm/ttm_resource.h:405
#: include/drm/ttm/ttm_resource.h:419 include/drm/ttm/ttm_resource.h:484
#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:38
#: include/drm/gpu_scheduler.h:47 include/drm/gpu_scheduler.h:234
#: include/drm/gpu_scheduler.h:249 include/drm/gpu_scheduler.h:387
#: include/drm/gpu_scheduler.h:537 include/drm/gpu_scheduler.h:573
#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:444
#: drivers/gpu/drm/scheduler/sched_main.c:460
#: drivers/gpu/drm/scheduler/sched_main.c:474
#: drivers/gpu/drm/scheduler/sched_main.c:505
#: drivers/gpu/drm/scheduler/sched_main.c:607
#: drivers/gpu/drm/scheduler/sched_main.c:700
#: drivers/gpu/drm/scheduler/sched_main.c:743
#: drivers/gpu/drm/scheduler/sched_main.c:801
#: drivers/gpu/drm/scheduler/sched_main.c:863
#: drivers/gpu/drm/scheduler/sched_main.c:897
#: drivers/gpu/drm/scheduler/sched_main.c:945
#: drivers/gpu/drm/scheduler/sched_main.c:972
#: drivers/gpu/drm/scheduler/sched_main.c:1009
#: drivers/gpu/drm/scheduler/sched_main.c:1052
#: drivers/gpu/drm/scheduler/sched_main.c:1183
#: drivers/gpu/drm/scheduler/sched_main.c:1317
#: drivers/gpu/drm/scheduler/sched_main.c:1414
#: drivers/gpu/drm/scheduler/sched_main.c:1470
#: drivers/gpu/drm/scheduler/sched_main.c:1513
#: drivers/gpu/drm/scheduler/sched_main.c:1525
#: drivers/gpu/drm/scheduler/sched_main.c:1540 ../../../gpu/drm-mm:574:
#: drivers/gpu/drm/scheduler/sched_entity.c:47
#: drivers/gpu/drm/scheduler/sched_entity.c:124
#: drivers/gpu/drm/scheduler/sched_entity.c:158
#: drivers/gpu/drm/scheduler/sched_entity.c:272
#: drivers/gpu/drm/scheduler/sched_entity.c:318
#: drivers/gpu/drm/scheduler/sched_entity.c:348
#: drivers/gpu/drm/scheduler/sched_entity.c:379 ../../../gpu/drm-mm:58:
#: drivers/gpu/drm/ttm/ttm_resource.c:122
#: drivers/gpu/drm/ttm/ttm_resource.c:136
#: drivers/gpu/drm/ttm/ttm_resource.c:153
#: drivers/gpu/drm/ttm/ttm_resource.c:324
#: drivers/gpu/drm/ttm/ttm_resource.c:358
#: drivers/gpu/drm/ttm/ttm_resource.c:518
#: drivers/gpu/drm/ttm/ttm_resource.c:584
#: drivers/gpu/drm/ttm/ttm_resource.c:923 ../../../gpu/drm-mm:64:
#: include/drm/ttm/ttm_tt.h:174 include/drm/ttm/ttm_tt.h:188
#: include/drm/ttm/ttm_tt.h:202 include/drm/ttm/ttm_tt.h:218
#: include/drm/ttm/ttm_tt.h:228 include/drm/ttm/ttm_tt.h:237
#: include/drm/ttm/ttm_tt.h:250 include/drm/ttm/ttm_tt.h:261
#: include/drm/ttm/ttm_tt.h:270 include/drm/ttm/ttm_tt.h:313
#: ../../../gpu/drm-mm:67: drivers/gpu/drm/ttm/ttm_tt.c:540
#: ../../../gpu/drm-mm:76: drivers/gpu/drm/ttm/ttm_pool.c:804
#: drivers/gpu/drm/ttm/ttm_pool.c:881 drivers/gpu/drm/ttm/ttm_pool.c:1064
#: drivers/gpu/drm/ttm/ttm_pool.c:1110 drivers/gpu/drm/ttm/ttm_pool.c:1236
msgid "**Description**"
msgstr ""

#: ../../../gpu/drm-mm:43: drivers/gpu/drm/ttm/ttm_device.c:195
msgid "Initializes a struct ttm_device:"
msgstr ""

#: ../../../gpu/drm-mm:43: drivers/gpu/drm/ttm/ttm_device.c:205
msgid "!0: Failure."
msgstr ""

#: ../../../gpu/drm-mm.rst:47
msgid "TTM resource placement reference"
msgstr ""

#: ../../../gpu/drm-mm:49: include/drm/ttm/ttm_placement.h:76
msgid "``fpfn``"
msgstr ""

#: ../../../gpu/drm-mm:49: include/drm/ttm/ttm_placement.h:77
msgid "first valid page frame number to put the object"
msgstr ""

#: ../../../gpu/drm-mm:49: include/drm/ttm/ttm_placement.h:77
msgid "``lpfn``"
msgstr ""

#: ../../../gpu/drm-mm:49: include/drm/ttm/ttm_placement.h:78
msgid "last valid page frame number to put the object"
msgstr ""

#: ../../../gpu/drm-mm:49: include/drm/ttm/ttm_placement.h:78
#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:244
#: include/drm/ttm/ttm_resource.h:322
msgid "``mem_type``"
msgstr ""

#: ../../../gpu/drm-mm:49: include/drm/ttm/ttm_placement.h:79
msgid "One of TTM_PL_* where the resource should be allocated from."
msgstr ""

#: ../../../gpu/drm-mm:49: include/drm/ttm/ttm_placement.h:79
#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:86
#: include/drm/drm_gpuvm.h:233 include/drm/drm_gpuvm.h:519
#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:19
msgid "``flags``"
msgstr ""

#: ../../../gpu/drm-mm:49: include/drm/ttm/ttm_placement.h:80
msgid "memory domain and caching flags for the object"
msgstr ""

#: ../../../gpu/drm-mm:49: include/drm/ttm/ttm_placement.h:75
msgid "Structure indicating a possible place to put an object."
msgstr ""

#: ../../../gpu/drm-mm:49: include/drm/ttm/ttm_placement.h:93
msgid "``num_placement``"
msgstr ""

#: ../../../gpu/drm-mm:49: include/drm/ttm/ttm_placement.h:94
msgid "number of preferred placements"
msgstr ""

#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:35
#: ../../../gpu/drm-mm:49: include/drm/ttm/ttm_placement.h:94
#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:245
msgid "``placement``"
msgstr ""

#: ../../../gpu/drm-mm:49: include/drm/ttm/ttm_placement.h:95
msgid "preferred placements"
msgstr ""

#: ../../../gpu/drm-mm:49: include/drm/ttm/ttm_placement.h:92
msgid "Structure indicating the placement you request for an object."
msgstr ""

#: ../../../gpu/drm-mm.rst:53
msgid "TTM resource object reference"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:54
msgid "enumerate ttm_lru_item subclasses"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:60
msgid "``TTM_LRU_RESOURCE``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:61
msgid "The resource subclass"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:63
msgid "``TTM_LRU_HITCH``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:64
msgid "The iterator hitch subclass"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:64
msgid "The TTM lru list node base class"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:65
msgid "``link``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:66
msgid "The list link"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:66
msgid "``type``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:67
#: include/drm/ttm/ttm_resource.h:76
msgid "The subclass type"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:74
msgid "initialize a struct ttm_lru_item"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:80
#: include/drm/ttm/ttm_resource.h:276
msgid "``struct ttm_lru_item *item``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:75
msgid "The item to initialize"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:77
msgid "``enum ttm_lru_item_type type``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:178
msgid "``use_type``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:179
msgid "The memory type is enabled."
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:179
msgid "``use_tt``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:180
msgid "If a TT object should be used for the backing store."
msgstr ""

#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:157
#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:181
msgid "``bdev``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:182
#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:515
msgid "ttm device this manager belongs to"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:333 ../../../gpu/drm-mm:463:
#: include/drm/drm_mm.h:162 ../../../gpu/drm-mm:55:
#: include/drm/ttm/ttm_resource.h:180 include/drm/ttm/ttm_resource.h:243
msgid "``size``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:181
msgid "Size of the managed region."
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:182
msgid "``func``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:183
msgid "structure pointer implementing the range manager. See above"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:183
msgid "``move_lock``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:184
msgid "lock for move fence"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:184
msgid "``move``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:185
msgid "The fence of the last pipelined move operation."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:435 ../../../gpu/drm-mm:55:
#: include/drm/ttm/ttm_resource.h:185 include/drm/ttm/ttm_resource.h:264
msgid "``lru``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:186
msgid "The lru list for this memory type."
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:211
msgid "``usage``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:212
msgid "How much of the resources are used, protected by the bdev->lru_lock."
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:217
msgid "``cg``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:218
msgid ":c:type:`dmem_cgroup_region` used for memory accounting, if not NULL."
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:177
msgid ""
"This structure is used to identify and manage memory types for a device."
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:225
msgid "``addr``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:226
msgid "mapped virtual address"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:226
msgid "``offset``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:227
msgid "physical addr"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:227
msgid "``is_iomem``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:228
msgid "is this io memory ?"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:228
#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:123 ../../../gpu/drm-mm:73:
#: include/drm/ttm/ttm_pool.h:46 include/drm/ttm/ttm_pool.h:69
msgid "``caching``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:229
msgid "See enum ttm_caching"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:224
msgid "Structure indicating the bus placement of an object."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:160 ../../../gpu/drm-mm:55:
#: include/drm/ttm/ttm_resource.h:242 include/drm/ttm/ttm_resource.h:348
msgid "``start``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:243
msgid "Start of the allocation."
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:244
msgid "Actual size of resource in bytes."
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:245
msgid "Resource type of the allocation."
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:246
msgid "Placement flags."
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:246
msgid "``bus``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:247
msgid "Placement on io bus accessible to the CPU"
msgstr ""

#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:33
#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:247
msgid "``bo``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:248
msgid "weak reference to the BO, protected by ttm_device::lru_lock"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:248
msgid "``css``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:249
msgid "cgroup state this resource is charged to"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:265
msgid ""
"Least recently used list, see :c:type:`ttm_resource_manager.lru "
"<ttm_resource_manager>`"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:241
msgid ""
"Structure indicating the placement and space resources used by a buffer "
"object."
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:270
msgid "Downcast a struct ttm_lru_item to a struct ttm_resource"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:271
msgid "The struct ttm_lru_item to downcast"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:273
msgid "Pointer to the embedding struct ttm_resource"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:284
msgid "``first``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:285
msgid "first res in the bulk move range"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:285
msgid "``last``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:286
msgid "last res in the bulk move range"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:283
msgid "Range of resources for a lru bulk move."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:385 ../../../gpu/drm-mm:55:
#: include/drm/ttm/ttm_resource.h:296
msgid "``pos``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:297
msgid "first/last lru entry for resources in the each domain/priority"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:297
msgid "``cursor_list``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:298
msgid ""
"The list of cursors currently traversing any of the sublists of **pos**. "
"Protected by the ttm device's lru_lock."
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:299
msgid ""
"Container for the current bulk move state. Should be used with "
"ttm_lru_bulk_move_init() and ttm_bo_set_bulk_move(). All BOs in a bulk_move "
"structure need to share the same reservation object to ensure that the bulk "
"as a whole is locked for eviction even if only one BO of the bulk is evicted."
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:313
msgid "``man``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:314
msgid "The resource manager currently being iterated over"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:314
msgid "``hitch``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:315
msgid "A hitch list node inserted before the next resource to iterate over."
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:316
msgid "``bulk_link``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:317
msgid ""
"A list link for the list of cursors traversing the bulk sublist of **bulk**. "
"Protected by the ttm device's lru_lock."
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:318
msgid "``bulk``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:319
msgid ""
"Pointer to struct ttm_lru_bulk_move whose subrange **hitch** is inserted to. "
"NULL if none. Never dereference this pointer since the struct "
"ttm_lru_bulk_move object pointed to might have been freed. The pointer is "
"only for comparison."
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:323
msgid ""
"The memory type of the LRU list being traversed. This field is valid iff "
"**bulk** != NULL."
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:324
#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:138
msgid "``priority``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:325
msgid "the current priority"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:325
msgid "Cursor to iterate over the resources in a manager."
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:343
msgid ""
"Specialization for a struct io_mapping + struct sg_table backed struct "
"ttm_resource."
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:13
#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:25
#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:345
#: include/drm/ttm/ttm_resource.h:371 ../../../gpu/drm-mm:64:
#: include/drm/ttm/ttm_tt.h:133
msgid "``base``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:346
msgid "Embedded struct ttm_kmap_iter providing the usage interface."
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:346
msgid "``iomap``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:347
msgid "struct io_mapping representing the underlying linear io_memory."
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:347
msgid "``st``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:348
#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:762
msgid ""
"sg_table into **iomap**, representing the memory of the struct ttm_resource."
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:349
#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:764
msgid ""
"Offset that needs to be subtracted from **st** to make sg_dma_address(st-"
">sgl) - **start** == 0 for **iomap** start."
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:350
msgid "``cache``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:351
msgid "Scatterlist traversal cache for fast lookups."
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:351
msgid "``cache.sg``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:352
msgid "Pointer to the currently cached scatterlist segment."
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:352
msgid "``cache.i``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:353
msgid "First index of **sg**. PAGE_SIZE granularity."
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:353
msgid "``cache.end``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:354
msgid "Last index + 1 of **sg**. PAGE_SIZE granularity."
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:354
msgid "``cache.offs``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:355
msgid "First offset into **iomap** of **sg**. PAGE_SIZE granularity."
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:370
msgid "Iterator specialization for linear io"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:372
msgid "The base iterator"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:372
msgid "``dmap``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:373
msgid "Points to the starting address of the region"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:373
msgid "``needs_unmap``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:374
msgid "Whether we need to unmap on fini"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:388
#: include/drm/ttm/ttm_resource.h:407 include/drm/ttm/ttm_resource.h:421
#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:360
#: drivers/gpu/drm/ttm/ttm_resource.c:518
#: drivers/gpu/drm/ttm/ttm_resource.c:586
#: drivers/gpu/drm/ttm/ttm_resource.c:604
#: drivers/gpu/drm/ttm/ttm_resource.c:923
msgid "``struct ttm_resource_manager *man``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:384
#: include/drm/ttm/ttm_resource.h:417 ../../../gpu/drm-mm:58:
#: drivers/gpu/drm/ttm/ttm_resource.c:582
msgid "A memory manager object."
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:386
msgid "``bool used``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:385
msgid "usage state to set."
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:383
msgid ""
"Set the manager in use flag. If disabled the manager is no longer used for "
"object placement."
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:403
msgid "Manager to get used state for"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:402
msgid "Get the in use flag for a manager."
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:406
msgid "true is used, false if not."
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:416
msgid "Cleanup the move fences from the memory manager object."
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:482
msgid "``ttm_resource_manager_for_each_res (cursor, res)``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:480
msgid "iterate over all resources"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:486
msgid "``cursor``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:481
msgid "struct ttm_resource_cursor for the current position"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:483
msgid "``res``"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:482
msgid "the current resource"
msgstr ""

#: ../../../gpu/drm-mm:55: include/drm/ttm/ttm_resource.h:483
msgid "Iterate over all the evictable resources in a resource manager."
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:119
msgid "initialize a bulk move structure"
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:125
#: drivers/gpu/drm/ttm/ttm_resource.c:135
#: drivers/gpu/drm/ttm/ttm_resource.c:155
msgid "``struct ttm_lru_bulk_move *bulk``"
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:120
msgid "the structure to init"
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:121
msgid "For now just memset the structure to zero."
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:132
msgid "finalize a bulk move structure"
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:133
msgid "The struct ttm_device"
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:134
msgid "the structure to finalize"
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:135
msgid ""
"Sanity checks that bulk moves don't have any resources left and hence no "
"cursors attached."
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:149
msgid "bulk move range of resources to the LRU tail."
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:151
msgid "bulk move structure"
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:152
msgid ""
"Bulk move BOs to the LRU tail, only valid to use when driver makes sure that "
"resource order never changes. Should be called with :c:type:`ttm_device."
"lru_lock <ttm_device>` held."
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:319
msgid "resource object constructure"
msgstr ""

#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:75
#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:325
#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:189
#: include/drm/ttm/ttm_tt.h:198 include/drm/ttm/ttm_tt.h:313
msgid "``struct ttm_buffer_object *bo``"
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:320
msgid "buffer object this resources is allocated for"
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:322
msgid "``const struct ttm_place *place``"
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:321
msgid "placement of the resource"
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:323
#: drivers/gpu/drm/ttm/ttm_resource.c:357
msgid "``struct ttm_resource *res``"
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:322
msgid "the resource object to inistilize"
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:323
msgid "Initialize a new resource object. Counterpart of ttm_resource_fini()."
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:354
msgid "resource destructor"
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:355
msgid "the resource manager this resource belongs to"
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:356
msgid "the resource to clean up"
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:357
msgid ""
"Should be used by resource manager backends to clean up the TTM resource "
"objects before freeing the underlying structure. Makes sure the resource is "
"removed from the LRU before destruction. Counterpart of ttm_resource_init()."
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:514
msgid "memory manager object to init"
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:517
msgid "``uint64_t size``"
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:516
msgid "size of managed resources in arbitrary units"
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:513
msgid "Initialise core parts of a manager object."
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:581
msgid "Return how many resources are currently used."
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:600
msgid "manager type to dump."
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:72
#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:398
#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:172
#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:665
#: ../../../gpu/drm-mm:400: drivers/gpu/drm/drm_gem_ttm_helper.c:24
#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:987
#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:1178
#: drivers/gpu/drm/drm_buddy.c:1195 ../../../gpu/drm-mm:58:
#: drivers/gpu/drm/ttm/ttm_resource.c:602
msgid "``struct drm_printer *p``"
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:601
msgid "printer to use for debug."
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:759
msgid "Initialize a struct ttm_kmap_iter_iomap"
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:765
msgid "``struct ttm_kmap_iter_iomap *iter_io``"
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:760
msgid "The struct ttm_kmap_iter_iomap to initialize."
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:762
msgid "``struct io_mapping *iomap``"
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:761
msgid "The struct io_mapping representing the underlying linear io_memory."
msgstr ""

#: ../../../gpu/drm-mm:521: drivers/gpu/drm/drm_cache.c:126
#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:763
msgid "``struct sg_table *st``"
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:765
msgid "``resource_size_t start``"
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:767
#: ../../../gpu/drm-mm:67: drivers/gpu/drm/ttm/ttm_tt.c:513
msgid "Pointer to the embedded struct ttm_kmap_iter."
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:917
msgid "Create debugfs entry for specified resource manager."
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:919
msgid "The TTM resource manager for which the debugfs stats file be creates"
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:921
msgid "``struct dentry * parent``"
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:920
msgid "debugfs directory in which the file will reside"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:983
#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:922
msgid "``const char *name``"
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:921
msgid "The filename to create."
msgstr ""

#: ../../../gpu/drm-mm:58: drivers/gpu/drm/ttm/ttm_resource.c:922
msgid ""
"This function setups up a debugfs file that can be used to look at debug "
"statistics of the specified ttm_resource_manager."
msgstr ""

#: ../../../gpu/drm-mm.rst:62
msgid "TTM TT object reference"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:44
msgid ""
"This is a structure holding the pages, caching- and aperture binding status "
"for a buffer object that isn't backed by fixed (VRAM / AGP) memory."
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:30
#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:49 ../../../gpu/drm-mm:73:
#: include/drm/ttm/ttm_pool.h:49
msgid "``pages``"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:50
msgid "Array of pages backing the data."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:52
msgid "``page_flags``"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:53
msgid "The page flags."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:55
msgid "Supported values:"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:57
msgid ""
"TTM_TT_FLAG_SWAPPED: Set by TTM when the pages have been unpopulated and "
"swapped out by TTM.  Calling ttm_tt_populate() will then swap the pages back "
"in, and unset the flag. Drivers should in general never need to touch this."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:62
msgid "TTM_TT_FLAG_ZERO_ALLOC: Set if the pages will be zeroed on allocation."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:65
msgid ""
"TTM_TT_FLAG_EXTERNAL: Set if the underlying pages were allocated externally, "
"like with dma-buf or userptr. This effectively disables TTM swapping out "
"such pages.  Also important is to prevent TTM from ever directly mapping "
"these pages."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:70
msgid ""
"Note that enum ttm_bo_type.ttm_bo_type_sg objects will always enable this "
"flag."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:73
msgid ""
"TTM_TT_FLAG_EXTERNAL_MAPPABLE: Same behaviour as TTM_TT_FLAG_EXTERNAL, but "
"with the reduced restriction that it is still valid to use TTM to map the "
"pages directly. This is useful when implementing a ttm_tt backend which "
"still allocates driver owned pages underneath(say with shmem)."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:79
msgid ""
"Note that since this also implies TTM_TT_FLAG_EXTERNAL, the usage here "
"should always be:"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:82
msgid "page_flags = TTM_TT_FLAG_EXTERNAL |"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:83
msgid "TTM_TT_FLAG_EXTERNAL_MAPPABLE;"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:85
msgid ""
"TTM_TT_FLAG_DECRYPTED: The mapped ttm pages should be marked as not "
"encrypted. The framework will try to match what the dma layer is doing, but "
"note that it is a little fragile because ttm page fault handling abuses the "
"DMA api a bit and dma_map_attrs can't be used to assure pgprot always "
"matches."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:91
msgid ""
"TTM_TT_FLAG_BACKED_UP: TTM internal only. This is set if the struct ttm_tt "
"has been (possibly partially) backed up."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:94
msgid ""
"TTM_TT_FLAG_PRIV_POPULATED: TTM internal only. DO NOT USE. This is set by "
"TTM after ttm_tt_populate() has successfully returned, and is then unset "
"when TTM calls ttm_tt_unpopulate()."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:107
msgid "``num_pages``"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:108
msgid "Number of pages in the page array."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:109
msgid "``sg``"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:110
msgid "for SG objects via dma-buf."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:111
msgid "``dma_address``"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:112
msgid "The DMA (bus) addresses of the pages."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:113
msgid "``swap_storage``"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:114
msgid "Pointer to shmem struct file for swap storage."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:116
msgid "``backup``"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:117
msgid ""
"Pointer to backup struct for backed up tts. Could be unified with "
"**swap_storage**. Meanwhile, the driver's ttm_tt_create() callback is "
"responsible for assigning this field."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:124
msgid "The current caching state of the pages, see enum ttm_caching."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:127
msgid "``restore``"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:128
msgid "Partial restoration from backup state. TTM private"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:132
msgid "Specialization of a mappig iterator for a tt."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:134
msgid "Embedded struct ttm_kmap_iter providing the usage interface"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:134
msgid "``tt``"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:135
msgid "Cached struct ttm_tt."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:135
msgid "``prot``"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:136
msgid "Cached page protection for mapping."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:149
msgid "Whether the ttm_tt is swapped out or backed up"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:155
#: include/drm/ttm/ttm_tt.h:166
msgid "``const struct ttm_tt *tt``"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:150
#: include/drm/ttm/ttm_tt.h:161 include/drm/ttm/ttm_tt.h:172
#: include/drm/ttm/ttm_tt.h:196 include/drm/ttm/ttm_tt.h:226
#: include/drm/ttm/ttm_tt.h:235
msgid "The struct ttm_tt."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:152
#: include/drm/ttm/ttm_tt.h:163
msgid "true if swapped or backed up, false otherwise."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:160
msgid "Whether the ttm_tt backed up"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:171
msgid "Clear the ttm_tt backed-up status"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:177 ../../../gpu/drm-mm:67:
#: drivers/gpu/drm/ttm/ttm_tt.c:512 drivers/gpu/drm/ttm/ttm_tt.c:543
#: ../../../gpu/drm-mm:76: drivers/gpu/drm/ttm/ttm_pool.c:802
#: drivers/gpu/drm/ttm/ttm_pool.c:880
msgid "``struct ttm_tt *tt``"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:173
msgid ""
"Drivers can use this functionto clear the backed-up status, for example "
"before destroying or re-validating a purged tt."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:185
msgid "pointer to a struct ttm_buffer_object"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:187
msgid "``bool zero_alloc``"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:186
msgid "true if allocated pages needs to be zeroed"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:184
msgid ""
"Make sure we have a TTM structure allocated for the given BO. No pages are "
"actually allocated."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:200
#: include/drm/ttm/ttm_tt.h:220 include/drm/ttm/ttm_tt.h:227
#: include/drm/ttm/ttm_tt.h:239 include/drm/ttm/ttm_tt.h:248
#: include/drm/ttm/ttm_tt.h:260 include/drm/ttm/ttm_tt.h:272
msgid "``struct ttm_tt *ttm``"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:197
msgid "The buffer object we create the ttm for."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:199
#: include/drm/ttm/ttm_tt.h:312
msgid "``uint32_t page_flags``"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:198
#: include/drm/ttm/ttm_tt.h:311
msgid "Page flags as identified by TTM_TT_FLAG_XX flags."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:200
msgid "``enum ttm_caching caching``"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:199
msgid "the desired caching state of the pages"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:201
msgid "``unsigned long extra_pages``"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:200
msgid "Extra pages needed for the driver."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:195
msgid ""
"Create a struct ttm_tt to back data with system memory pages. No pages are "
"actually allocated."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:204
msgid "NULL: Out of memory."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:216
msgid "the ttm_tt structure."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:215
msgid "Free memory of ttm_tt structure"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:225
#: include/drm/ttm/ttm_tt.h:246 include/drm/ttm/ttm_tt.h:258
msgid "the ttm_device this object belongs to"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:227
msgid "Unbind, unpopulate and destroy common struct ttm_tt."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:236
msgid "Swap in a previously swap out ttm_tt."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:244
msgid "allocate pages for a ttm"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:247
#: include/drm/ttm/ttm_tt.h:259 include/drm/ttm/ttm_tt.h:268
msgid "Pointer to the ttm_tt structure"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:249 ../../../gpu/drm-mm:76:
#: drivers/gpu/drm/ttm/ttm_pool.c:803
msgid "``struct ttm_operation_ctx *ctx``"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:248
msgid "operation context for populating the tt object."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:249
msgid "Calls the driver method to allocate pages for a ttm"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:256
msgid "free pages from a ttm"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:260
msgid "Calls the driver method to free all pages from a ttm"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:266
msgid "Mark pages for clearing on populate."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:269
msgid ""
"Marks pages for clearing so that the next time the page vector is populated, "
"the pages will be cleared."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:285
msgid "Flags to govern backup behaviour."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:286
msgid "``purge``"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:287
msgid "Free pages without backing up. Bypass pools."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:287
msgid "``writeback``"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:288
msgid ""
"Attempt to copy contents directly to swap space, even if that means blocking "
"on writes to external memory."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:309
msgid "Buffer object we allocate the ttm for."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:311
msgid "``struct agp_bridge_data *bridge``"
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:310
msgid "The agp bridge this device is sitting on."
msgstr ""

#: ../../../gpu/drm-mm:64: include/drm/ttm/ttm_tt.h:308
msgid ""
"Create a TTM backend that uses the indicated AGP bridge as an aperture for "
"TT memory. This function uses the linux agpgart interface to bind and unbind "
"memory backing a ttm_tt."
msgstr ""

#: ../../../gpu/drm-mm:67: drivers/gpu/drm/ttm/ttm_tt.c:509
msgid "Initialize a struct ttm_kmap_iter_tt"
msgstr ""

#: ../../../gpu/drm-mm:67: drivers/gpu/drm/ttm/ttm_tt.c:515
msgid "``struct ttm_kmap_iter_tt *iter_tt``"
msgstr ""

#: ../../../gpu/drm-mm:67: drivers/gpu/drm/ttm/ttm_tt.c:510
msgid "The struct ttm_kmap_iter_tt to initialize."
msgstr ""

#: ../../../gpu/drm-mm:67: drivers/gpu/drm/ttm/ttm_tt.c:511
msgid "Struct ttm_tt holding page pointers of the struct ttm_resource."
msgstr ""

#: ../../../gpu/drm-mm:67: drivers/gpu/drm/ttm/ttm_tt.c:537
msgid "Allocate and assign a backup structure for a ttm_tt"
msgstr ""

#: ../../../gpu/drm-mm:67: drivers/gpu/drm/ttm/ttm_tt.c:538
msgid "The ttm_tt for wich to allocate and assign a backup structure."
msgstr ""

#: ../../../gpu/drm-mm:67: drivers/gpu/drm/ttm/ttm_tt.c:539
msgid ""
"Assign a backup structure to be used for tt backup. This should typically be "
"done at bo creation, to avoid allocations at shrinking time."
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:199
#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:581
#: include/drm/drm_gpuvm.h:621 ../../../gpu/drm-mm:506:
#: drivers/gpu/drm/drm_gpuvm.c:1109 drivers/gpu/drm/drm_gpuvm.c:1183
#: drivers/gpu/drm/drm_gpuvm.c:1210 drivers/gpu/drm/drm_gpuvm.c:1244
#: drivers/gpu/drm/drm_gpuvm.c:1304 drivers/gpu/drm/drm_gpuvm.c:1335
#: drivers/gpu/drm/drm_gpuvm.c:1414 drivers/gpu/drm/drm_gpuvm.c:1756
#: ../../../gpu/drm-mm:67: drivers/gpu/drm/ttm/ttm_tt.c:544
msgid "0 on success, negative error code on failure."
msgstr ""

#: ../../../gpu/drm-mm.rst:71
msgid "TTM page pool reference"
msgstr ""

#: ../../../gpu/drm-mm:73: include/drm/ttm/ttm_pool.h:42
msgid "Pool for a certain memory type"
msgstr ""

#: ../../../gpu/drm-mm:73: include/drm/ttm/ttm_pool.h:45
msgid "the pool we belong to, might be NULL for the global ones"
msgstr ""

#: ../../../gpu/drm-mm:73: include/drm/ttm/ttm_pool.h:45
msgid "``order``"
msgstr ""

#: ../../../gpu/drm-mm:73: include/drm/ttm/ttm_pool.h:46
msgid "the allocation order our pages have"
msgstr ""

#: ../../../gpu/drm-mm:73: include/drm/ttm/ttm_pool.h:47
msgid "the caching type our pages have"
msgstr ""

#: ../../../gpu/drm-mm:73: include/drm/ttm/ttm_pool.h:47
msgid "``shrinker_list``"
msgstr ""

#: ../../../gpu/drm-mm:73: include/drm/ttm/ttm_pool.h:48
msgid "our place on the global shrinker list"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:248 include/drm/drm_gem.h:602
#: ../../../gpu/drm-mm:532: include/drm/drm_syncobj.h:61
#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:94
#: include/drm/gpu_scheduler.h:242 include/drm/gpu_scheduler.h:301
#: ../../../gpu/drm-mm:73: include/drm/ttm/ttm_pool.h:48
msgid "``lock``"
msgstr ""

#: ../../../gpu/drm-mm:73: include/drm/ttm/ttm_pool.h:49
msgid "protection of the page list"
msgstr ""

#: ../../../gpu/drm-mm:73: include/drm/ttm/ttm_pool.h:50
msgid "the list of pages in the pool"
msgstr ""

#: ../../../gpu/drm-mm:73: include/drm/ttm/ttm_pool.h:63
msgid "Pool for all caching and orders"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:306 ../../../gpu/drm-mm:568:
#: include/drm/gpu_scheduler.h:569 include/drm/gpu_scheduler.h:615
#: ../../../gpu/drm-mm:73: include/drm/ttm/ttm_pool.h:65
msgid "``dev``"
msgstr ""

#: ../../../gpu/drm-mm:73: include/drm/ttm/ttm_pool.h:66
msgid "the device we allocate pages for"
msgstr ""

#: ../../../gpu/drm-mm:73: include/drm/ttm/ttm_pool.h:66
msgid "``nid``"
msgstr ""

#: ../../../gpu/drm-mm:73: include/drm/ttm/ttm_pool.h:67
msgid "which numa node to use"
msgstr ""

#: ../../../gpu/drm-mm:73: include/drm/ttm/ttm_pool.h:67
msgid "``use_dma_alloc``"
msgstr ""

#: ../../../gpu/drm-mm:73: include/drm/ttm/ttm_pool.h:68
msgid "if coherent DMA allocations should be used"
msgstr ""

#: ../../../gpu/drm-mm:73: include/drm/ttm/ttm_pool.h:68
msgid "``use_dma32``"
msgstr ""

#: ../../../gpu/drm-mm:73: include/drm/ttm/ttm_pool.h:69
msgid "if GFP_DMA32 should be used"
msgstr ""

#: ../../../gpu/drm-mm:73: include/drm/ttm/ttm_pool.h:70
msgid "pools for each caching/order"
msgstr ""

#: ../../../gpu/drm-mm:76: drivers/gpu/drm/ttm/ttm_pool.c:798
msgid "Fill a ttm_tt object"
msgstr ""

#: ../../../gpu/drm-mm:76: drivers/gpu/drm/ttm/ttm_pool.c:804
#: drivers/gpu/drm/ttm/ttm_pool.c:882 drivers/gpu/drm/ttm/ttm_pool.c:1062
#: drivers/gpu/drm/ttm/ttm_pool.c:1112 drivers/gpu/drm/ttm/ttm_pool.c:1237
msgid "``struct ttm_pool *pool``"
msgstr ""

#: ../../../gpu/drm-mm:76: drivers/gpu/drm/ttm/ttm_pool.c:800
msgid "ttm_pool to use"
msgstr ""

#: ../../../gpu/drm-mm:76: drivers/gpu/drm/ttm/ttm_pool.c:801
msgid "ttm_tt object to fill"
msgstr ""

#: ../../../gpu/drm-mm:76: drivers/gpu/drm/ttm/ttm_pool.c:802
msgid "operation context"
msgstr ""

#: ../../../gpu/drm-mm:76: drivers/gpu/drm/ttm/ttm_pool.c:803
msgid ""
"Fill the ttm_tt object with pages and also make sure to DMA map them when "
"necessary."
msgstr ""

#: ../../../gpu/drm-mm:76: drivers/gpu/drm/ttm/ttm_pool.c:807
msgid "0 on successe, negative error code otherwise."
msgstr ""

#: ../../../gpu/drm-mm:76: drivers/gpu/drm/ttm/ttm_pool.c:876
msgid "Free the backing pages from a ttm_tt object"
msgstr ""

#: ../../../gpu/drm-mm:76: drivers/gpu/drm/ttm/ttm_pool.c:878
msgid "Pool to give pages back to."
msgstr ""

#: ../../../gpu/drm-mm:76: drivers/gpu/drm/ttm/ttm_pool.c:879
msgid "ttm_tt object to unpopulate"
msgstr ""

#: ../../../gpu/drm-mm:76: drivers/gpu/drm/ttm/ttm_pool.c:880
msgid "Give the packing pages back to a pool or free them"
msgstr ""

#: ../../../gpu/drm-mm:76: drivers/gpu/drm/ttm/ttm_pool.c:1056
msgid "Initialize a pool"
msgstr ""

#: ../../../gpu/drm-mm:76: drivers/gpu/drm/ttm/ttm_pool.c:1058
msgid "the pool to initialize"
msgstr ""

#: ../../../gpu/drm-mm:76: drivers/gpu/drm/ttm/ttm_pool.c:1059
msgid "device for DMA allocations and mappings"
msgstr ""

#: ../../../gpu/drm-mm:76: drivers/gpu/drm/ttm/ttm_pool.c:1061
msgid "``int nid``"
msgstr ""

#: ../../../gpu/drm-mm:76: drivers/gpu/drm/ttm/ttm_pool.c:1060
msgid "NUMA node to use for allocations"
msgstr ""

#: ../../../gpu/drm-mm:76: drivers/gpu/drm/ttm/ttm_pool.c:1061
msgid "true if coherent DMA alloc should be used"
msgstr ""

#: ../../../gpu/drm-mm:76: drivers/gpu/drm/ttm/ttm_pool.c:1062
msgid "true if GFP_DMA32 should be used"
msgstr ""

#: ../../../gpu/drm-mm:76: drivers/gpu/drm/ttm/ttm_pool.c:1063
msgid "Initialize the pool and its pool types."
msgstr ""

#: ../../../gpu/drm-mm:76: drivers/gpu/drm/ttm/ttm_pool.c:1106
msgid "Cleanup a pool"
msgstr ""

#: ../../../gpu/drm-mm:76: drivers/gpu/drm/ttm/ttm_pool.c:1108
msgid "the pool to clean up"
msgstr ""

#: ../../../gpu/drm-mm:76: drivers/gpu/drm/ttm/ttm_pool.c:1109
msgid ""
"Free all pages in the pool and unregister the types from the global shrinker."
msgstr ""

#: ../../../gpu/drm-mm:76: drivers/gpu/drm/ttm/ttm_pool.c:1231
msgid "Debugfs dump function for a pool"
msgstr ""

#: ../../../gpu/drm-mm:76: drivers/gpu/drm/ttm/ttm_pool.c:1233
msgid "the pool to dump the information for"
msgstr ""

#: ../../../gpu/drm-mm:76: drivers/gpu/drm/ttm/ttm_pool.c:1235
msgid "``struct seq_file *m``"
msgstr ""

#: ../../../gpu/drm-mm:76: drivers/gpu/drm/ttm/ttm_pool.c:1234
msgid "seq_file to dump to"
msgstr ""

#: ../../../gpu/drm-mm:76: drivers/gpu/drm/ttm/ttm_pool.c:1235
msgid "Make a debugfs dump with the per pool and global information."
msgstr ""

#: ../../../gpu/drm-mm.rst:80
msgid "The Graphics Execution Manager (GEM)"
msgstr ""

#: ../../../gpu/drm-mm.rst:82
msgid ""
"The GEM design approach has resulted in a memory manager that doesn't "
"provide full coverage of all (or even all common) use cases in its userspace "
"or kernel API. GEM exposes a set of standard memory-related operations to "
"userspace and a set of helper functions to drivers, and let drivers "
"implement hardware-specific operations with their own private API."
msgstr ""

#: ../../../gpu/drm-mm.rst:89
msgid ""
"The GEM userspace API is described in the `GEM - the Graphics Execution "
"Manager <http://lwn.net/Articles/283798/>`__ article on LWN. While slightly "
"outdated, the document provides a good overview of the GEM API principles. "
"Buffer allocation and read and write operations, described as part of the "
"common GEM API, are currently implemented using driver-specific ioctls."
msgstr ""

#: ../../../gpu/drm-mm.rst:96
msgid ""
"GEM is data-agnostic. It manages abstract buffer objects without knowing "
"what individual buffers contain. APIs that require knowledge of buffer "
"contents or purpose, such as buffer allocation or synchronization "
"primitives, are thus outside of the scope of GEM and must be implemented "
"using driver-specific ioctls."
msgstr ""

#: ../../../gpu/drm-mm.rst:102
msgid "On a fundamental level, GEM involves several operations:"
msgstr ""

#: ../../../gpu/drm-mm.rst:104
msgid "Memory allocation and freeing"
msgstr ""

#: ../../../gpu/drm-mm.rst:105
msgid "Command execution"
msgstr ""

#: ../../../gpu/drm-mm.rst:106
msgid "Aperture management at command execution time"
msgstr ""

#: ../../../gpu/drm-mm.rst:108
msgid ""
"Buffer object allocation is relatively straightforward and largely provided "
"by Linux's shmem layer, which provides memory to back each object."
msgstr ""

#: ../../../gpu/drm-mm.rst:112
msgid ""
"Device-specific operations, such as command execution, pinning, buffer read "
"& write, mapping, and domain ownership transfers are left to driver-specific "
"ioctls."
msgstr ""

#: ../../../gpu/drm-mm.rst:117
msgid "GEM Initialization"
msgstr ""

#: ../../../gpu/drm-mm.rst:119
msgid ""
"Drivers that use GEM must set the DRIVER_GEM bit in the struct :c:type:"
"`struct drm_driver <drm_driver>` driver_features field. The DRM core will "
"then automatically initialize the GEM core before calling the load "
"operation. Behind the scene, this will create a DRM Memory Manager object "
"which provides an address space pool for object allocation."
msgstr ""

#: ../../../gpu/drm-mm.rst:126
msgid ""
"In a KMS configuration, drivers need to allocate and initialize a command "
"ring buffer following core GEM initialization if required by the hardware. "
"UMA devices usually have what is called a \"stolen\" memory region, which "
"provides space for the initial framebuffer and large, contiguous memory "
"regions required by the device. This space is typically not managed by GEM, "
"and must be initialized separately into its own DRM MM object."
msgstr ""

#: ../../../gpu/drm-mm.rst:135
msgid "GEM Objects Creation"
msgstr ""

#: ../../../gpu/drm-mm.rst:137
msgid ""
"GEM splits creation of GEM objects and allocation of the memory that backs "
"them in two distinct operations."
msgstr ""

#: ../../../gpu/drm-mm.rst:140
msgid ""
"GEM objects are represented by an instance of struct :c:type:`struct "
"drm_gem_object <drm_gem_object>`. Drivers usually need to extend GEM objects "
"with private information and thus create a driver-specific GEM object "
"structure type that embeds an instance of struct :c:type:`struct "
"drm_gem_object <drm_gem_object>`."
msgstr ""

#: ../../../gpu/drm-mm.rst:146
msgid ""
"To create a GEM object, a driver allocates memory for an instance of its "
"specific GEM object type and initializes the embedded struct :c:type:`struct "
"drm_gem_object <drm_gem_object>` with a call to drm_gem_object_init(). The "
"function takes a pointer to the DRM device, a pointer to the GEM object and "
"the buffer object size in bytes."
msgstr ""

#: ../../../gpu/drm-mm.rst:153
msgid ""
"GEM uses shmem to allocate anonymous pageable memory. drm_gem_object_init() "
"will create an shmfs file of the requested size and store it into the "
"struct :c:type:`struct drm_gem_object <drm_gem_object>` filp field. The "
"memory is used as either main storage for the object when the graphics "
"hardware uses system memory directly or as a backing store otherwise."
msgstr ""

#: ../../../gpu/drm-mm.rst:160
msgid ""
"Drivers are responsible for the actual physical pages allocation by calling "
"shmem_read_mapping_page_gfp() for each page. Note that they can decide to "
"allocate pages when initializing the GEM object, or to delay allocation "
"until the memory is needed (for instance when a page fault occurs as a "
"result of a userspace memory access or when the driver needs to start a DMA "
"transfer involving the memory)."
msgstr ""

#: ../../../gpu/drm-mm.rst:167
msgid ""
"Anonymous pageable memory allocation is not always desired, for instance "
"when the hardware requires physically contiguous system memory as is often "
"the case in embedded devices. Drivers can create GEM objects with no shmfs "
"backing (called private GEM objects) by initializing them with a call to "
"drm_gem_private_object_init() instead of drm_gem_object_init(). Storage for "
"private GEM objects must be managed by drivers."
msgstr ""

#: ../../../gpu/drm-mm.rst:175
msgid "GEM Objects Lifetime"
msgstr ""

#: ../../../gpu/drm-mm.rst:177
msgid ""
"All GEM objects are reference-counted by the GEM core. References can be "
"acquired and release by calling drm_gem_object_get() and "
"drm_gem_object_put() respectively."
msgstr ""

#: ../../../gpu/drm-mm.rst:181
msgid ""
"When the last reference to a GEM object is released the GEM core calls the :"
"c:type:`struct drm_gem_object_funcs <gem_object_funcs>` free operation. That "
"operation is mandatory for GEM-enabled drivers and must free the GEM object "
"and all associated resources."
msgstr ""

#: ../../../gpu/drm-mm.rst:186
msgid ""
"void (\\*free) (struct drm_gem_object \\*obj); Drivers are responsible for "
"freeing all GEM object resources. This includes the resources created by the "
"GEM core, which need to be released with drm_gem_object_release()."
msgstr ""

#: ../../../gpu/drm-mm.rst:192
msgid "GEM Objects Naming"
msgstr ""

#: ../../../gpu/drm-mm.rst:194
msgid ""
"Communication between userspace and the kernel refers to GEM objects using "
"local handles, global names or, more recently, file descriptors. All of "
"those are 32-bit integer values; the usual Linux kernel limits apply to the "
"file descriptors."
msgstr ""

#: ../../../gpu/drm-mm.rst:199
msgid ""
"GEM handles are local to a DRM file. Applications get a handle to a GEM "
"object through a driver-specific ioctl, and can use that handle to refer to "
"the GEM object in other standard or driver-specific ioctls. Closing a DRM "
"file handle frees all its GEM handles and dereferences the associated GEM "
"objects."
msgstr ""

#: ../../../gpu/drm-mm.rst:205
msgid ""
"To create a handle for a GEM object drivers call drm_gem_handle_create(). "
"The function takes a pointer to the DRM file and the GEM object and returns "
"a locally unique handle.  When the handle is no longer needed drivers delete "
"it with a call to drm_gem_handle_delete(). Finally the GEM object associated "
"with a handle can be retrieved by a call to drm_gem_object_lookup()."
msgstr ""

#: ../../../gpu/drm-mm.rst:211
msgid ""
"Handles don't take ownership of GEM objects, they only take a reference to "
"the object that will be dropped when the handle is destroyed. To avoid "
"leaking GEM objects, drivers must make sure they drop the reference(s) they "
"own (such as the initial reference taken at object creation time) as "
"appropriate, without any special consideration for the handle. For example, "
"in the particular case of combined GEM object and handle creation in the "
"implementation of the dumb_create operation, drivers must drop the initial "
"reference to the GEM object before returning the handle."
msgstr ""

#: ../../../gpu/drm-mm.rst:221
msgid ""
"GEM names are similar in purpose to handles but are not local to DRM files. "
"They can be passed between processes to reference a GEM object globally. "
"Names can't be used directly to refer to objects in the DRM API, "
"applications must convert handles to names and names to handles using the "
"DRM_IOCTL_GEM_FLINK and DRM_IOCTL_GEM_OPEN ioctls respectively. The "
"conversion is handled by the DRM core without any driver-specific support."
msgstr ""

#: ../../../gpu/drm-mm.rst:229
msgid ""
"GEM also supports buffer sharing with dma-buf file descriptors through "
"PRIME. GEM-based drivers must use the provided helpers functions to "
"implement the exporting and importing correctly. See ?. Since sharing file "
"descriptors is inherently more secure than the easily guessable and global "
"GEM names it is the preferred buffer sharing mechanism. Sharing buffers "
"through GEM names is only supported for legacy userspace. Furthermore PRIME "
"also allows cross-device buffer sharing since it is based on dma-bufs."
msgstr ""

#: ../../../gpu/drm-mm.rst:239
msgid "GEM Objects Mapping"
msgstr ""

#: ../../../gpu/drm-mm.rst:241
msgid ""
"Because mapping operations are fairly heavyweight GEM favours read/write-"
"like access to buffers, implemented through driver-specific ioctls, over "
"mapping buffers to userspace. However, when random access to the buffer is "
"needed (to perform software rendering for instance), direct access to the "
"object can be more efficient."
msgstr ""

#: ../../../gpu/drm-mm.rst:247
msgid ""
"The mmap system call can't be used directly to map GEM objects, as they "
"don't have their own file handle. Two alternative methods currently co-exist "
"to map GEM objects to userspace. The first method uses a driver-specific "
"ioctl to perform the mapping operation, calling do_mmap() under the hood. "
"This is often considered dubious, seems to be discouraged for new GEM-"
"enabled drivers, and will thus not be described here."
msgstr ""

#: ../../../gpu/drm-mm.rst:255
msgid ""
"The second method uses the mmap system call on the DRM file handle. void "
"\\*mmap(void \\*addr, size_t length, int prot, int flags, int fd, off_t "
"offset); DRM identifies the GEM object to be mapped by a fake offset passed "
"through the mmap offset argument. Prior to being mapped, a GEM object must "
"thus be associated with a fake offset. To do so, drivers must call "
"drm_gem_create_mmap_offset() on the object."
msgstr ""

#: ../../../gpu/drm-mm.rst:262
msgid ""
"Once allocated, the fake offset value must be passed to the application in a "
"driver-specific way and can then be used as the mmap offset argument."
msgstr ""

#: ../../../gpu/drm-mm.rst:266
msgid ""
"The GEM core provides a helper method drm_gem_mmap() to handle object "
"mapping. The method can be set directly as the mmap file operation handler. "
"It will look up the GEM object based on the offset value and set the VMA "
"operations to the :c:type:`struct drm_driver <drm_driver>` gem_vm_ops field. "
"Note that drm_gem_mmap() doesn't map memory to userspace, but relies on the "
"driver-provided fault handler to map pages individually."
msgstr ""

#: ../../../gpu/drm-mm.rst:274
msgid ""
"To use drm_gem_mmap(), drivers must fill the struct :c:type:`struct "
"drm_driver <drm_driver>` gem_vm_ops field with a pointer to VM operations."
msgstr ""

#: ../../../gpu/drm-mm.rst:277
msgid ""
"The VM operations is a :c:type:`struct vm_operations_struct "
"<vm_operations_struct>` made up of several fields, the more interesting ones "
"being:"
msgstr ""

#: ../../../gpu/drm-mm.rst:289
msgid ""
"The open and close operations must update the GEM object reference count. "
"Drivers can use the drm_gem_vm_open() and drm_gem_vm_close() helper "
"functions directly as open and close handlers."
msgstr ""

#: ../../../gpu/drm-mm.rst:293
msgid ""
"The fault operation handler is responsible for mapping individual pages to "
"userspace when a page fault occurs. Depending on the memory allocation "
"scheme, drivers can allocate pages at fault time, or can decide to allocate "
"memory for the GEM object at the time the object is created."
msgstr ""

#: ../../../gpu/drm-mm.rst:299
msgid ""
"Drivers that want to map the GEM object upfront instead of handling page "
"faults can implement their own mmap file operation handler."
msgstr ""

#: ../../../gpu/drm-mm.rst:302
msgid ""
"For platforms without MMU the GEM core provides a helper method "
"drm_gem_dma_get_unmapped_area(). The mmap() routines will call this to get a "
"proposed address for the mapping."
msgstr ""

#: ../../../gpu/drm-mm.rst:306
msgid ""
"To use drm_gem_dma_get_unmapped_area(), drivers must fill the struct :c:type:"
"`struct file_operations <file_operations>` get_unmapped_area field with a "
"pointer on drm_gem_dma_get_unmapped_area()."
msgstr ""

#: ../../../gpu/drm-mm.rst:310
msgid ""
"More detailed information about get_unmapped_area can be found in "
"Documentation/admin-guide/mm/nommu-mmap.rst"
msgstr ""

#: ../../../gpu/drm-mm.rst:314
msgid "Memory Coherency"
msgstr ""

#: ../../../gpu/drm-mm.rst:316
msgid ""
"When mapped to the device or used in a command buffer, backing pages for an "
"object are flushed to memory and marked write combined so as to be coherent "
"with the GPU. Likewise, if the CPU accesses an object after the GPU has "
"finished rendering to the object, then the object must be made coherent with "
"the CPU's view of memory, usually involving GPU cache flushing of various "
"kinds. This core CPU<->GPU coherency management is provided by a device-"
"specific ioctl, which evaluates an object's current domain and performs any "
"necessary flushing or synchronization to put the object into the desired "
"coherency domain (note that the object may be busy, i.e. an active render "
"target; in that case, setting the domain blocks the client and waits for "
"rendering to complete before performing any necessary flushing operations)."
msgstr ""

#: ../../../gpu/drm-mm.rst:330
msgid "Command Execution"
msgstr ""

#: ../../../gpu/drm-mm.rst:332
msgid ""
"Perhaps the most important GEM function for GPU devices is providing a "
"command execution interface to clients. Client programs construct command "
"buffers containing references to previously allocated memory objects, and "
"then submit them to GEM. At that point, GEM takes care to bind all the "
"objects into the GTT, execute the buffer, and provide necessary "
"synchronization between clients accessing the same buffers. This often "
"involves evicting some objects from the GTT and re-binding others (a fairly "
"expensive operation), and providing relocation support which hides fixed GTT "
"offsets from clients. Clients must take care not to submit command buffers "
"that reference more objects than can fit in the GTT; otherwise, GEM will "
"reject them and no rendering will occur. Similarly, if several objects in "
"the buffer require fence registers to be allocated for correct rendering (e."
"g. 2D blits on pre-965 chips), care must be taken not to require more fence "
"registers than are available to the client. Such resource management should "
"be abstracted from the client in libdrm."
msgstr ""

#: ../../../gpu/drm-mm.rst:350
msgid "GEM Function Reference"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:49
msgid "bitmask of object state for fdinfo reporting"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:55
msgid "``DRM_GEM_OBJECT_RESIDENT``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:56
msgid "object is resident in memory (ie. not unpinned)"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:58
msgid "``DRM_GEM_OBJECT_PURGEABLE``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:59
msgid "object marked as purgeable by userspace"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:61
msgid "``DRM_GEM_OBJECT_ACTIVE``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:62
msgid "object is currently used by an active submission"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:53
msgid ""
"Bitmask of status used for fdinfo memory stats, see :c:type:"
"`drm_gem_object_funcs.status <drm_gem_object_funcs>` and drm_show_fdinfo().  "
"Note that an object can report DRM_GEM_OBJECT_PURGEABLE and be active or not "
"resident, in which case drm_show_fdinfo() will not account for it as "
"purgeable.  So drivers do not need to check if the buffer is idle and "
"resident to return this bit, i.e. userspace can mark a buffer as purgeable "
"even while it is still busy on the GPU. It will not get reported in the "
"puregeable stats until it becomes idle.  The status gem object func does not "
"need to consider this."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:70
msgid "GEM object functions"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:74
msgid "``free``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:75
msgid "Deconstructor for drm_gem_objects."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:77 ../../../gpu/drm-mm:503:
#: include/drm/drm_gpuvm.h:1099
msgid "This callback is mandatory."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:83
msgid "``open``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:84
msgid "Called upon GEM handle creation."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:86 include/drm/drm_gem.h:95
#: include/drm/drm_gem.h:110 include/drm/drm_gem.h:121
#: include/drm/drm_gem.h:131 include/drm/drm_gem.h:140
#: include/drm/drm_gem.h:165 include/drm/drm_gem.h:176
#: include/drm/drm_gem.h:185 include/drm/drm_gem.h:200 ../../../gpu/drm-mm:503:
#: include/drm/drm_gpuvm.h:1111 include/drm/drm_gpuvm.h:1123
#: include/drm/drm_gpuvm.h:1135 include/drm/drm_gpuvm.h:1147
msgid "This callback is optional."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:92
msgid "``close``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:93
msgid "Called upon GEM handle release."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:101
msgid "``print_info``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:102
msgid ""
"If driver subclasses struct :c:type:`drm_gem_object`, it can implement this "
"optional hook for printing additional driver specific info."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:105
msgid ""
"drm_printf_indent() should be used in the callback passing it the indent "
"argument."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:108
msgid "This callback is called from drm_gem_print_info()."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:117
msgid "``export``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:118
msgid ""
"Export backing buffer as a :c:type:`dma_buf`. If this is not set "
"drm_gem_prime_export() is used."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:127
msgid "``pin``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:128
msgid ""
"Pin backing buffer in memory, such that dma-buf importers can access it. "
"Used by the drm_gem_map_attach() helper."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:137
msgid "``unpin``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:138
msgid "Unpin backing buffer. Used by the drm_gem_map_detach() helper."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:146
msgid "``get_sg_table``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:147
msgid ""
"Returns a Scatter-Gather table representation of the buffer. Used when "
"exporting a buffer by the drm_gem_map_dma_buf() helper. Releasing is done by "
"calling dma_unmap_sg_attrs() and sg_free_table() in drm_gem_unmap_buf(), "
"therefore these helpers and this callback here cannot be used for sg tables "
"pointing at driver private memory ranges."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:154
msgid "See also drm_prime_pages_to_sg()."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:160
msgid "``vmap``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:161
msgid ""
"Returns a virtual address for the buffer. Used by the drm_gem_dmabuf_vmap() "
"helper. Called with a held GEM reservation lock."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:171
msgid "``vunmap``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:172
msgid ""
"Releases the address previously returned by **vmap**. Used by the "
"drm_gem_dmabuf_vunmap() helper. Called with a held GEM reservation lock."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:182
msgid "``mmap``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:183
msgid "Handle mmap() of the gem object, setup vma accordingly."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:187
msgid ""
"The callback is used by both drm_gem_mmap_obj() and drm_gem_prime_mmap().  "
"When **mmap** is present **vm_ops** is not used, the **mmap** callback must "
"set vma->vm_ops instead."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:195 ../../../gpu/drm-mm:503:
#: include/drm/drm_gpuvm.h:313
msgid "``evict``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:196
msgid ""
"Evicts gem object out from memory. Used by the drm_gem_object_evict() "
"helper. Returns 0 on success, -errno otherwise. Called with a held GEM "
"reservation lock."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:206
msgid "``status``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:207
msgid ""
"The optional status callback can return additional object state which "
"determines which stats the object is counted against.  The callback is "
"called under table_lock.  Racing against object status change is "
"\"harmless\", and the callback can expect to not race against object "
"destruction."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:213 include/drm/drm_gem.h:222
msgid "Called by drm_show_memory_stats()."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:219
msgid "``rss``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:220
msgid "Return resident size of the object in physical memory."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:228
msgid "``vm_ops``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:229
msgid "Virtual memory operations used with mmap."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:231
msgid "This is optional but necessary for mmap support."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:238
msgid "A simple LRU helper"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:249
msgid ""
"Lock protecting movement of GEM objects between LRUs.  All LRUs that the "
"object can move between should be protected by the same lock."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:257
msgid "``count``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:258
msgid "The total number of backing pages of the GEM objects in this LRU."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:265 ../../../gpu/drm-mm:503:
#: include/drm/drm_gpuvm.h:670 include/drm/drm_gpuvm.h:988
#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:84
#: include/drm/gpu_scheduler.h:323
msgid "``list``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:266
msgid "The LRU list."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:239
msgid ""
"A helper for tracking GEM objects in a given state, to aid in driver's "
"shrinker implementation.  Tracks the count of pages for lockless :c:type:"
"`shrinker.count_objects <shrinker>`, and provides :c:type:`drm_gem_lru_scan` "
"for driver's :c:type:`shrinker.scan_objects <shrinker>` implementation."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:273 include/drm/drm_gem.h:497
#: include/drm/drm_gem.h:516 ../../../gpu/drm-mm:355:
#: drivers/gpu/drm/drm_gem.c:1025
msgid "GEM buffer object"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:282 ../../../gpu/drm-mm:532:
#: include/drm/drm_syncobj.h:41
msgid "``refcount``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:283
msgid "Reference count of this object"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:285
msgid ""
"Please use drm_gem_object_get() to acquire and drm_gem_object_put_locked() "
"or drm_gem_object_put() to release a reference to a GEM buffer object."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:293
msgid "``handle_count``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:294
msgid "This is the GEM file_priv handle count of this object."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:296
msgid ""
"Each handle also holds a reference. Note that when the handle_count drops to "
"0 any global names (e.g. the id in the flink namespace) will be cleared."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:300 include/drm/drm_gem.h:357
msgid "Protected by :c:type:`drm_device.object_name_lock <drm_device>`."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:307
msgid "DRM dev this object belongs to."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:311
msgid "``filp``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:312
msgid ""
"SHMEM file node used as backing storage for swappable buffer objects. GEM "
"also supports driver private objects with driver-specific backing storage "
"(contiguous DMA memory, special reserved blocks). In this case **filp** is "
"NULL."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:321
msgid "``vma_node``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:322
msgid ""
"Mapping info for this object to support mmap. Drivers are supposed to "
"allocate the mmap offset using drm_gem_create_mmap_offset(). The offset "
"itself can be retrieved using drm_vma_node_offset_addr()."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:326
msgid ""
"Memory mapping itself is handled by drm_gem_mmap(), which also checks that "
"userspace is allowed to access the object."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:334
msgid "Size of the object, in bytes.  Immutable over the object's lifetime."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:341 include/drm/drm_gem.h:467
#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:253
#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:228
#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:545
#: include/drm/gpu_scheduler.h:614
msgid "``name``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:342
msgid ""
"Global name for this object, starts at 1. 0 means unnamed. Access is covered "
"by :c:type:`drm_device.object_name_lock <drm_device>`. This is used by the "
"GEM_FLINK and GEM_OPEN ioctls."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:350
msgid "``dma_buf``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:351
msgid "dma-buf associated with this GEM object."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:353
msgid ""
"Pointer to the dma-buf associated with this gem object (either through "
"importing or exporting). We break the resulting reference loop when the last "
"gem handle for this object is released."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:363
msgid "``import_attach``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:364
msgid "dma-buf attachment backing this object."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:366
msgid ""
"Any foreign dma_buf imported as a gem object has this set to the attachment "
"point for the device. This is invariant over the lifetime of a gem object."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:370
msgid ""
"The :c:type:`drm_gem_object_funcs.free <drm_gem_object_funcs>` callback is "
"responsible for cleaning up the dma_buf attachment and references acquired "
"at import time."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:374
msgid ""
"Note that the drm gem/prime core does not depend upon drivers setting this "
"field any more. So for drivers where this doesn't make sense (e.g. virtual "
"devices or a displaylink behind an usb bus) they can simply leave it as NULL."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:383
msgid "``resv``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:384
msgid "Pointer to reservation object associated with the this GEM object."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:386
msgid "Normally (**resv** == &**_resv**) except for imported GEM objects."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:392
msgid "``_resv``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:393
msgid "A reservation object for this GEM object."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:395
msgid "This is unused for imported GEM objects."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:401
msgid "``gpuva``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:402
msgid "Provides the list of GPU VAs attached to this GEM object."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:404
msgid ""
"Drivers should lock list accesses with the GEMs :c:type:`dma_resv` lock (:c:"
"type:`drm_gem_object.resv <drm_gem_object>`) or a custom lock if one is "
"provided."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:418
msgid ""
"Optional GEM object functions. If this is set, it will be used instead of "
"the corresponding :c:type:`drm_driver` GEM callbacks."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:421
msgid "New drivers should use this."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:428
msgid "``lru_node``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:429
msgid "List node in a :c:type:`drm_gem_lru`."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:436
msgid "The current LRU list that the GEM object is on."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:274
msgid ""
"This structure defines the generic parts for GEM buffer objects, which are "
"mostly around handling mmap and userspace handles."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:277
msgid "Buffer objects are often abbreviated to BO."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:445
msgid "``DRM_GEM_FOPS``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:443
msgid "Default drm GEM file operations"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:444
msgid ""
"This macro provides a shorthand for setting the GEM file ops in the :c:type:"
"`file_operations` structure.  If all you need are the default ops, use "
"DEFINE_DRM_GEM_FOPS instead."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:463
msgid "``DEFINE_DRM_GEM_FOPS (name)``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:461
msgid "macro to generate file operations for GEM drivers"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:462 ../../../gpu/drm-mm:364:
#: include/drm/drm_gem_dma_helper.h:248
msgid "name for the generated structure"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:463
msgid ""
"This macro autogenerates a suitable :c:type:`struct file_operations "
"<file_operations>` for GEM based drivers, which can be assigned to :c:type:"
"`drm_driver.fops <drm_driver>`. Note that this structure cannot be shared "
"between drivers, because it contains a reference to the current module using "
"THIS_MODULE."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:468 ../../../gpu/drm-mm:364:
#: include/drm/drm_gem_dma_helper.h:254
msgid ""
"Note that the declaration is already marked as static - if you need a non-"
"static version of this you're probably doing it wrong and will break the "
"THIS_MODULE reference by accident."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:496
msgid "acquire a GEM buffer object reference"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:502 include/drm/drm_gem.h:521
#: include/drm/drm_gem.h:579 include/drm/drm_gem.h:627 ../../../gpu/drm-mm:355:
#: drivers/gpu/drm/drm_gem.c:123 drivers/gpu/drm/drm_gem.c:157
#: drivers/gpu/drm/drm_gem.c:173 drivers/gpu/drm/drm_gem.c:209
#: drivers/gpu/drm/drm_gem.c:496 drivers/gpu/drm/drm_gem.c:523
#: drivers/gpu/drm/drm_gem.c:542 drivers/gpu/drm/drm_gem.c:569
#: drivers/gpu/drm/drm_gem.c:600 drivers/gpu/drm/drm_gem.c:693
#: drivers/gpu/drm/drm_gem.c:1030 drivers/gpu/drm/drm_gem.c:1101
#: drivers/gpu/drm/drm_gem.c:1417 drivers/gpu/drm/drm_gem.c:1438
#: drivers/gpu/drm/drm_gem.c:1462 drivers/gpu/drm/drm_gem.c:1583
#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:58
#: include/drm/drm_gem_dma_helper.h:89 include/drm/drm_gem_dma_helper.h:125
#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:158
#: include/drm/drm_gem_shmem_helper.h:189
#: include/drm/drm_gem_shmem_helper.h:203
#: include/drm/drm_gem_shmem_helper.h:217
#: include/drm/drm_gem_shmem_helper.h:269 ../../../gpu/drm-mm:442:
#: drivers/gpu/drm/drm_prime.c:761 drivers/gpu/drm/drm_prime.c:917
#: drivers/gpu/drm/drm_prime.c:1107 ../../../gpu/drm-mm:503:
#: include/drm/drm_gpuvm.h:416 include/drm/drm_gpuvm.h:748
#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1463
#: drivers/gpu/drm/drm_gpuvm.c:1573 drivers/gpu/drm/drm_gpuvm.c:1595
#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:202
#: drivers/gpu/drm/drm_exec.c:257 drivers/gpu/drm/drm_exec.c:284
#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1006
msgid "``struct drm_gem_object *obj``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:498
msgid ""
"This function acquires an additional reference to **obj**. It is illegal to "
"call this without already holding a reference. No locks required."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:515
msgid "drop a GEM buffer object reference"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:517
msgid "This releases a reference to **obj**."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:573
msgid "helper for shared memory stats"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:578 ../../../gpu/drm-mm:355:
#: drivers/gpu/drm/drm_gem.c:518 drivers/gpu/drm/drm_gem.c:537
#: drivers/gpu/drm/drm_gem.c:564 drivers/gpu/drm/drm_gem.c:596
#: drivers/gpu/drm/drm_gem.c:688 drivers/gpu/drm/drm_gem.c:1578
msgid "obj in question"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:574
msgid ""
"This helper should only be used for fdinfo shared memory stats to determine "
"if a GEM object is shared."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:586
msgid "Tests if GEM object's buffer has been imported"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:592 ../../../gpu/drm-mm:364:
#: include/drm/drm_gem_dma_helper.h:70 ../../../gpu/drm-mm:376:
#: include/drm/drm_gem_shmem_helper.h:170
msgid "``const struct drm_gem_object *obj``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:587 ../../../gpu/drm-mm:388:
#: include/drm/drm_gem_vram_helper.h:83
msgid "the GEM object"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:589
msgid "True if the GEM object's buffer has been imported, false otherwise"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:601
msgid "``drm_gem_gpuva_set_lock (obj, lock)``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:599
msgid "Set the lock protecting accesses to the gpuva list."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:605 ../../../gpu/drm-mm:503:
#: include/drm/drm_gpuvm.h:653 ../../../gpu/drm-mm:544:
#: include/drm/drm_exec.h:73 include/drm/drm_exec.h:85
msgid "``obj``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:600 include/drm/drm_gem.h:622
#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:744
msgid "the :c:type:`drm_gem_object`"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:601
msgid ""
"the lock used to protect the gpuva list. The locking primitive must contain "
"a dep_map field."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:603
msgid ""
"Call this if you're not proctecting access to the gpuva list with the dma-"
"resv lock, but with a custom lock."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:621
msgid "initialize the gpuva list of a GEM object"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:623
msgid ""
"This initializes the :c:type:`drm_gem_object`'s :c:type:`drm_gpuvm_bo` list."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:625
msgid ""
"Calling this function is only necessary for drivers intending to support "
"the :c:type:`drm_driver_feature` DRIVER_GEM_GPUVA."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:628
msgid "See also drm_gem_gpuva_set_lock()."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:639
msgid "``drm_gem_for_each_gpuvm_bo (entry__, obj__)``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:637
msgid "iterator to walk over a list of :c:type:`drm_gpuvm_bo`"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:643 include/drm/drm_gem.h:654
msgid "``entry__``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:638
msgid ":c:type:`drm_gpuvm_bo` structure to assign to in each iteration step"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:640 include/drm/drm_gem.h:653
msgid "``obj__``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:639 include/drm/drm_gem.h:652
msgid ""
"the :c:type:`drm_gem_object` the :c:type:`drm_gpuvm_bo` to walk are "
"associated with"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:640
msgid ""
"This iterator walks over all :c:type:`drm_gpuvm_bo` structures associated "
"with the :c:type:`drm_gem_object`."
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:650
msgid "``drm_gem_for_each_gpuvm_bo_safe (entry__, next__, obj__)``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:648
msgid "iterator to safely walk over a list of :c:type:`drm_gpuvm_bo`"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:650
msgid ":c:type:`drm_gpuvm_bostructure` to assign to in each iteration step"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:652 ../../../gpu/drm-mm:503:
#: include/drm/drm_gpuvm.h:462 include/drm/drm_gpuvm.h:496
#: include/drm/drm_gpuvm.h:779
msgid "``next__``"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:651
msgid ":c:type:`next` :c:type:`drm_gpuvm_bo` to store the next step"
msgstr ""

#: ../../../gpu/drm-mm:352: include/drm/drm_gem.h:653
msgid ""
"This iterator walks over all :c:type:`drm_gpuvm_bo` structures associated "
"with the :c:type:`drm_gem_object`. It is implemented with "
"list_for_each_entry_safe(), hence it is save against removal of elements."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:118
msgid ""
"initialize an allocated shmem-backed GEM object in a given shmfs mountpoint"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:124
#: drivers/gpu/drm/drm_gem.c:160 drivers/gpu/drm/drm_gem.c:176
#: drivers/gpu/drm/drm_gem.c:381 ../../../gpu/drm-mm:367:
#: drivers/gpu/drm/drm_gem_dma_helper.c:451
#: drivers/gpu/drm/drm_gem_dma_helper.c:560 ../../../gpu/drm-mm:379:
#: drivers/gpu/drm/drm_gem_shmem_helper.c:121
#: drivers/gpu/drm/drm_gem_shmem_helper.c:138
#: drivers/gpu/drm/drm_gem_shmem_helper.c:505
#: drivers/gpu/drm/drm_gem_shmem_helper.c:779
#: drivers/gpu/drm/drm_gem_shmem_helper.c:814 ../../../gpu/drm-mm:391:
#: drivers/gpu/drm/drm_gem_vram_helper.c:171
#: drivers/gpu/drm/drm_gem_vram_helper.c:415
#: drivers/gpu/drm/drm_gem_vram_helper.c:534
#: drivers/gpu/drm/drm_gem_vram_helper.c:930
#: drivers/gpu/drm/drm_gem_vram_helper.c:986 ../../../gpu/drm-mm:400:
#: drivers/gpu/drm/drm_gem_ttm_helper.c:124 ../../../gpu/drm-mm:442:
#: drivers/gpu/drm/drm_prime.c:237 drivers/gpu/drm/drm_prime.c:288
#: drivers/gpu/drm/drm_prime.c:419 drivers/gpu/drm/drm_prime.c:507
#: drivers/gpu/drm/drm_prime.c:848 drivers/gpu/drm/drm_prime.c:945
#: drivers/gpu/drm/drm_prime.c:964 drivers/gpu/drm/drm_prime.c:1033
msgid "``struct drm_device *dev``"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:121
#: drivers/gpu/drm/drm_gem.c:155 drivers/gpu/drm/drm_gem.c:171
msgid "drm_device the object should be initialized for"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:122
#: drivers/gpu/drm/drm_gem.c:156 drivers/gpu/drm/drm_gem.c:172
msgid "drm_gem_object to initialize"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:124
#: drivers/gpu/drm/drm_gem.c:158 drivers/gpu/drm/drm_gem.c:174
#: drivers/gpu/drm/drm_gem.c:539 ../../../gpu/drm-mm:367:
#: drivers/gpu/drm/drm_gem_dma_helper.c:120 ../../../gpu/drm-mm:379:
#: drivers/gpu/drm/drm_gem_shmem_helper.c:118
#: drivers/gpu/drm/drm_gem_shmem_helper.c:136 ../../../gpu/drm-mm:391:
#: drivers/gpu/drm/drm_gem_vram_helper.c:168
msgid "``size_t size``"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:123
#: drivers/gpu/drm/drm_gem.c:157 drivers/gpu/drm/drm_gem.c:173
msgid "object size"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:125
#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:137
msgid "``struct vfsmount *gemfs``"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:124
msgid ""
"tmpfs mount where the GEM object will be created. If NULL, use the usual "
"tmpfs mountpoint (`shm_mnt`)."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:126
#: drivers/gpu/drm/drm_gem.c:158
msgid ""
"Initialize an already allocated GEM object of the specified size with shmfs "
"backing store."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:154
msgid "initialize an allocated shmem-backed GEM object"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:170
msgid "initialize an allocated private GEM object"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:174
msgid ""
"Initialize an already allocated GEM object of the specified size with no GEM "
"provided backing store. Instead the caller is responsible for backing the "
"object and handling it."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:203
msgid "Finalize a failed drm_gem_object"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:204
msgid "drm_gem_object"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:205
msgid "Uninitialize an already allocated GEM object when it initialized failed"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:344
msgid "deletes the given file-private handle"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:350
#: drivers/gpu/drm/drm_gem.c:767 drivers/gpu/drm/drm_gem.c:822
msgid "``struct drm_file *filp``"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:345
msgid "drm file-private structure to use for the handle look up"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:347
#: drivers/gpu/drm/drm_gem.c:382 drivers/gpu/drm/drm_gem.c:819
#: drivers/gpu/drm/drm_gem.c:841 ../../../gpu/drm-mm:535:
#: drivers/gpu/drm/drm_syncobj.c:244 drivers/gpu/drm/drm_syncobj.c:425
#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:943
msgid "``u32 handle``"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:346
msgid "userspace handle to delete"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:347
msgid ""
"Removes the GEM handle from the **filp** lookup table which has been added "
"with drm_gem_handle_create(). If this is the last handle also cleans up "
"linked resources like GEM names."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:378
msgid "return the fake mmap offset for a gem object"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:384
#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:508
#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:416
#: drivers/gpu/drm/drm_gem_vram_helper.c:537 ../../../gpu/drm-mm:400:
#: drivers/gpu/drm/drm_gem_ttm_helper.c:127 ../../../gpu/drm-mm:571:
#: drivers/gpu/drm/scheduler/sched_main.c:942
msgid "``struct drm_file *file``"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:379
msgid "drm file-private structure containing the gem object"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:380
msgid "corresponding drm_device"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:381
msgid "gem object handle"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:383
msgid "``u64 *offset``"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:382
msgid "return location for the fake mmap offset"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:383
msgid ""
"This implements the :c:type:`drm_driver.dumb_map_offset <drm_driver>` kms "
"driver callback for drivers which use gem to manage their backing storage."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:387
#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:126
#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:262
#: drivers/gpu/drm/drm_gem_dma_helper.c:299
#: drivers/gpu/drm/drm_gem_dma_helper.c:519 ../../../gpu/drm-mm:376:
#: include/drm/drm_gem_shmem_helper.h:270 ../../../gpu/drm-mm:379:
#: drivers/gpu/drm/drm_gem_shmem_helper.c:285
#: drivers/gpu/drm/drm_gem_shmem_helper.c:515
#: drivers/gpu/drm/drm_gem_shmem_helper.c:617
msgid "0 on success or a negative error code on failure."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:493
msgid "create a gem handle for an object"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:499
#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:258
#: drivers/gpu/drm/drm_gem_dma_helper.c:291 ../../../gpu/drm-mm:442:
#: drivers/gpu/drm/drm_prime.c:285 drivers/gpu/drm/drm_prime.c:416
#: drivers/gpu/drm/drm_prime.c:504
msgid "``struct drm_file *file_priv``"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:494
msgid "drm file-private structure to register the handle for"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:495
msgid "object to register"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:497
msgid "``u32 *handlep``"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:496
msgid "pointer to return the created handle to the caller"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:497
msgid ""
"Create a handle for this object. This adds a handle reference to the object, "
"which includes a regular reference count. Callers will likely want to "
"dereference the object afterwards."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:501
msgid ""
"Since this publishes **obj** to userspace it must be fully set up by this "
"point, drivers must call this last in their buffer object creation callbacks."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:517
msgid "release a fake mmap offset for an object"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:519
msgid ""
"This routine frees fake offsets allocated by drm_gem_create_mmap_offset()."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:521
msgid ""
"Note that drm_gem_object_release() already calls this function, so drivers "
"don't have to take care of releasing the mmap offset themselves when freeing "
"the GEM object."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:536
#: drivers/gpu/drm/drm_gem.c:563
msgid "create a fake mmap offset for an object"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:538
msgid "the virtual size"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:539
#: drivers/gpu/drm/drm_gem.c:565
msgid ""
"GEM memory mapping works by handing back to userspace a fake mmap offset it "
"can use in a subsequent mmap(2) call.  The DRM core code then looks up the "
"object based on the offset and sets up the various memory mapping structures."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:544
msgid ""
"This routine allocates and attaches a fake offset for **obj**, in cases "
"where the virtual size differs from the physical size (ie. :c:type:"
"`drm_gem_object.size <drm_gem_object>`). Otherwise just use "
"drm_gem_create_mmap_offset()."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:548
msgid ""
"This function is idempotent and handles an already allocated mmap offset "
"transparently. Drivers do not need to check for this case."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:570
msgid "This routine allocates and attaches a fake offset for **obj**."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:572
msgid ""
"Drivers can call drm_gem_free_mmap_offset() before freeing **obj** to "
"release the fake offset again."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:594
msgid "helper to allocate backing pages for a GEM object from shmem"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:597
msgid ""
"This reads the page-array of the shmem-backing storage of the given gem "
"object. An array of pages is returned. If a page is not allocated or swapped-"
"out, this will allocate/swap-in the required pages. Note that the whole "
"object is covered by the page-array and pinned in memory."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:602
msgid "Use drm_gem_put_pages() to release the array and unpin all pages."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:604
msgid ""
"This uses the GFP-mask set on the shmem-mapping (see "
"mapping_set_gfp_mask()). If you require other GFP-masks, you have to do "
"those allocations yourself."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:607
msgid ""
"Note that you are not allowed to change gfp-zones during runtime. That is, "
"shmem_read_mapping_page_gfp() must be called with the same gfp_zone(gfp) as "
"set during initialization. If you have special zone constraints, set them "
"after drm_gem_object_init() via mapping_set_gfp_mask(). shmem-core takes "
"care to keep pages in the required zone during swap-in."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:613
msgid ""
"This function is only valid on objects initialized with "
"drm_gem_object_init(), but not for those initialized with "
"drm_gem_private_object_init() only."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:687
msgid "helper to free backing pages for a GEM object"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:690
#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:845
#: drivers/gpu/drm/drm_prime.c:1050
msgid "``struct page **pages``"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:689
msgid "pages to free"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:691
msgid "``bool dirty``"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:690
msgid "if true, pages will be marked as dirty"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:692
msgid "``bool accessed``"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:691
msgid "if true, the pages will be marked as accessed"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:761
msgid "look up GEM objects from an array of handles"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:762
#: drivers/gpu/drm/drm_gem.c:817 drivers/gpu/drm/drm_gem.c:839
msgid "DRM file private date"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:764
msgid "``void __user *bo_handles``"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:763
msgid "user pointer to array of userspace handle"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:765
#: drivers/gpu/drm/drm_gem.c:1318
msgid "``int count``"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:764
msgid "size of handle array"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:766
msgid "``struct drm_gem_object ***objs_out``"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:765
msgid "returned pointer to array of drm_gem_object pointers"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:766
msgid ""
"Takes an array of userspace handles and returns a newly allocated array of "
"GEM objects."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:769
msgid "For a single handle lookup, use drm_gem_object_lookup()."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:772
msgid ""
"**objs** filled in with GEM object pointers. Returned GEM objects need to be "
"released with drm_gem_object_put(). -ENOENT is returned on a lookup failure. "
"0 is returned on success."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:816
msgid "look up a GEM object from its handle"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:818
#: drivers/gpu/drm/drm_gem.c:840
msgid "userspace handle"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:819
msgid "If looking up an array of handles, use drm_gem_objects_lookup()."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:822
msgid ""
"A reference to the object named by the handle if such exists on **filp**, "
"NULL otherwise."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:837
msgid ""
"Wait on GEM object's reservation's objects shared and/or exclusive fences."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:843
msgid "``struct drm_file *filep``"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:842
msgid "``bool wait_all``"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:841
msgid "if true, wait on all fences, else wait on just exclusive fence"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:843
msgid "``unsigned long timeout``"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:842
msgid "timeout value in jiffies or zero to return immediately"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:844
msgid ""
"Returns -ERESTARTSYS if interrupted, 0 if the wait timed out, or greater "
"than 0 on success."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1024
msgid "release GEM buffer object resources"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1026
msgid ""
"This releases any structures and resources used by **obj** and is the "
"inverse of drm_gem_object_init()."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1044
msgid "free a GEM object"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1050
#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:527
msgid "``struct kref *kref``"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1045
msgid "kref of the object to free"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1046
msgid "Called after the last reference to the object has been lost."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1048
msgid "Frees the object"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1065
msgid "vma->ops->open implementation for GEM"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1071
#: drivers/gpu/drm/drm_gem.c:1086 drivers/gpu/drm/drm_gem.c:1099
#: drivers/gpu/drm/drm_gem.c:1166 ../../../gpu/drm-mm:364:
#: include/drm/drm_gem_dma_helper.h:122 ../../../gpu/drm-mm:367:
#: drivers/gpu/drm/drm_gem_dma_helper.c:514 ../../../gpu/drm-mm:376:
#: include/drm/drm_gem_shmem_helper.h:266 ../../../gpu/drm-mm:379:
#: drivers/gpu/drm/drm_gem_shmem_helper.c:613 ../../../gpu/drm-mm:400:
#: drivers/gpu/drm/drm_gem_ttm_helper.c:96 ../../../gpu/drm-mm:442:
#: drivers/gpu/drm/drm_prime.c:758 drivers/gpu/drm/drm_prime.c:816
msgid "``struct vm_area_struct *vma``"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1066
#: drivers/gpu/drm/drm_gem.c:1081
msgid "VM area structure"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1067
msgid ""
"This function implements the #vm_operations_struct open() callback for GEM "
"drivers. This must be used together with drm_gem_vm_close()."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1080
msgid "vma->ops->close implementation for GEM"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1082
msgid ""
"This function implements the #vm_operations_struct close() callback for GEM "
"drivers. This must be used together with drm_gem_vm_open()."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1095
msgid "memory map a GEM object"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1096
msgid "the GEM object to map"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1098
msgid "``unsigned long obj_size``"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1097
msgid "the object size to be mapped, in bytes"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1098
#: drivers/gpu/drm/drm_gem.c:1165 ../../../gpu/drm-mm:364:
#: include/drm/drm_gem_dma_helper.h:121 ../../../gpu/drm-mm:367:
#: drivers/gpu/drm/drm_gem_dma_helper.c:513 ../../../gpu/drm-mm:376:
#: include/drm/drm_gem_shmem_helper.h:265 ../../../gpu/drm-mm:379:
#: drivers/gpu/drm/drm_gem_shmem_helper.c:612
msgid "VMA for the area to be mapped"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1099
msgid ""
"Set up the VMA to prepare mapping of the GEM object using the GEM object's "
"vm_ops. Depending on their requirements, GEM objects can either provide a "
"fault handler in their vm_ops (in which case any accesses to the object will "
"be trapped, to perform migration, GTT binding, surface register allocation, "
"or performance monitoring), or mmap the buffer memory synchronously after "
"calling drm_gem_mmap_obj."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1106
msgid ""
"This function is mainly intended to implement the DMABUF mmap operation, "
"when the GEM object is not looked up based on its fake offset. To implement "
"the DRM mmap operation, drivers should use the drm_gem_mmap() function."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1110
msgid ""
"drm_gem_mmap_obj() assumes the user is granted access to the buffer while "
"drm_gem_mmap() prevents unprivileged users from mapping random objects. So "
"callers must verify access restrictions before calling this helper."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1114
msgid ""
"Return 0 or success or -EINVAL if the object size is smaller than the VMA "
"size, or if no vm_ops are provided."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1163
msgid "memory map routine for GEM objects"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1169
#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:331
msgid "``struct file *filp``"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1164
msgid "DRM file pointer"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1166
msgid ""
"If a driver supports GEM object mapping, mmap calls on the DRM file "
"descriptor will end up here."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1169
msgid ""
"Look up the GEM object based on the offset passed in (vma->vm_pgoff will "
"contain the fake offset we created when the GTT map ioctl was called on the "
"object) and map it with a call to drm_gem_mmap_obj()."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1173
msgid ""
"If the caller is not granted access to the buffer object, the mmap will fail "
"with EACCES. Please see the vma manager for more information."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1309
msgid ""
"Sets up the ww context and acquires the lock on an array of GEM objects."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1315
#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1299
msgid "``struct drm_gem_object **objs``"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1316
msgid "drm_gem_objects to lock"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1317
msgid "Number of objects in **objs**"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1319
msgid "``struct ww_acquire_ctx *acquire_ctx``"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1318
msgid ""
"struct ww_acquire_ctx that will be initialized as part of tracking this set "
"of locked reservations."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1310
msgid ""
"Once you've locked your reservations, you'll want to set up space for your "
"shared fences (if applicable), submit your job, then "
"drm_gem_unlock_reservations()."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1387
msgid "initialize a LRU"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1393
#: drivers/gpu/drm/drm_gem.c:1438 drivers/gpu/drm/drm_gem.c:1460
#: drivers/gpu/drm/drm_gem.c:1479
msgid "``struct drm_gem_lru *lru``"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1389
msgid "The LRU to initialize"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1391
msgid "``struct mutex *lock``"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1390
msgid "The lock protecting the LRU"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1411
msgid "remove object from whatever LRU it is in"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1415
msgid "The GEM object to remove from current LRU"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1412
msgid "If the object is currently in any LRU, remove it."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1432
#: drivers/gpu/drm/drm_gem.c:1454
msgid "move the object to the tail of the LRU"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1436
#: drivers/gpu/drm/drm_gem.c:1460
msgid "The LRU to move the object into."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1437
#: drivers/gpu/drm/drm_gem.c:1461
msgid "The GEM object to move into this LRU"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1433
msgid "Like :c:type:`drm_gem_lru_move_tail` but lru lock must be held"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1455
msgid ""
"If the object is already in this LRU it will be moved to the tail.  "
"Otherwise it will be removed from whichever other LRU it is in (if any) and "
"moved into this LRU."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1473
msgid "helper to implement shrinker.scan_objects"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1482
msgid "The LRU to scan"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1484
msgid "``unsigned int nr_to_scan``"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1483
msgid "The number of pages to try to reclaim"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1485
msgid "``unsigned long *remaining``"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1484
msgid "The number of pages left to reclaim, should be initialized by caller"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1486
msgid ""
"``bool (*shrink)(struct drm_gem_object *obj, struct ww_acquire_ctx *ticket)``"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1485
msgid "Callback to try to shrink/reclaim the object."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1487
msgid "``struct ww_acquire_ctx *ticket``"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1486
msgid "Optional ww_acquire_ctx context to use for locking"
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1474
msgid ""
"If the shrink callback succeeds, it is expected that the driver move the "
"object out of this LRU."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1477
msgid ""
"If the LRU possibly contain active buffers, it is the responsibility of the "
"shrink callback to check for this (ie. dma_resv_test_signaled()) or if "
"necessary block until the buffer becomes idle."
msgstr ""

#: ../../../gpu/drm-mm:355: drivers/gpu/drm/drm_gem.c:1577
msgid "helper to evict backing pages for a GEM object"
msgstr ""

#: ../../../gpu/drm-mm.rst:359
msgid "GEM DMA Helper Functions Reference"
msgstr ""

#: ../../../gpu/drm-mm:361: drivers/gpu/drm/drm_gem_dma_helper.c:27
msgid ""
"The DRM GEM/DMA helpers are a means to provide buffer objects that are "
"presented to the device as a contiguous chunk of memory. This is useful for "
"devices that do not support scatter-gather DMA (either directly or by using "
"an intimately attached IOMMU)."
msgstr ""

#: ../../../gpu/drm-mm:361: drivers/gpu/drm/drm_gem_dma_helper.c:32
msgid ""
"For devices that access the memory bus through an (external) IOMMU then the "
"buffer objects are allocated using a traditional page-based allocator and "
"may be scattered through physical memory. However they are contiguous in the "
"IOVA space so appear contiguous to devices using them."
msgstr ""

#: ../../../gpu/drm-mm:361: drivers/gpu/drm/drm_gem_dma_helper.c:38
msgid ""
"For other devices then the helpers rely on CMA to provide buffer objects "
"that are physically contiguous in memory."
msgstr ""

#: ../../../gpu/drm-mm:361: drivers/gpu/drm/drm_gem_dma_helper.c:41
msgid ""
"For GEM callback helpers in struct :c:type:`drm_gem_object` functions, see "
"likewise named functions with an _object_ infix (e.g., "
"drm_gem_dma_object_vmap() wraps drm_gem_dma_vmap()). These helpers perform "
"the necessary type conversion."
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:12
msgid "GEM object backed by DMA memory allocations"
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:14
msgid "base GEM object"
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:14
msgid "``dma_addr``"
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:15
msgid "DMA address of the backing memory"
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:15
#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:70
msgid "``sgt``"
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:16
msgid ""
"scatter/gather table for imported PRIME buffers. The table can have more "
"than one entry but they are guaranteed to have contiguous DMA addresses."
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:18
#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:75
msgid "``vaddr``"
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:19
msgid "kernel virtual address of the backing memory"
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:19
msgid "``map_noncoherent``"
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:20
msgid "if true, the GEM object is backed by non-coherent memory"
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:52
msgid "GEM object function for drm_gem_dma_free()"
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:53
#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:153
msgid "GEM object to free"
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:54
msgid ""
"This function wraps drm_gem_dma_free_object(). Drivers that employ the DMA "
"helpers should use it as their :c:type:`drm_gem_object_funcs.free "
"<drm_gem_object_funcs>` handler."
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:66
#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:395
msgid "Print :c:type:`drm_gem_dma_object` info for debugfs"
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:67
#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:397
#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:167
#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:664
#: ../../../gpu/drm-mm:400: drivers/gpu/drm/drm_gem_ttm_helper.c:19
msgid "DRM printer"
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:69
#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:399
#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:169
#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:666
#: ../../../gpu/drm-mm:400: drivers/gpu/drm/drm_gem_ttm_helper.c:21
msgid "``unsigned int indent``"
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:68
#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:398
#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:168
#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:665
#: ../../../gpu/drm-mm:400: drivers/gpu/drm/drm_gem_ttm_helper.c:20
msgid "Tab indentation level"
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:69
#: include/drm/drm_gem_dma_helper.h:84 include/drm/drm_gem_dma_helper.h:120
#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:169
#: include/drm/drm_gem_shmem_helper.h:184
#: include/drm/drm_gem_shmem_helper.h:198
#: include/drm/drm_gem_shmem_helper.h:212
#: include/drm/drm_gem_shmem_helper.h:264 ../../../gpu/drm-mm:400:
#: drivers/gpu/drm/drm_gem_ttm_helper.c:21 ../../../gpu/drm-mm:442:
#: drivers/gpu/drm/drm_prime.c:756
msgid "GEM object"
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:70
msgid ""
"This function wraps drm_gem_dma_print_info(). Drivers that employ the DMA "
"helpers should use this function as their :c:type:`drm_gem_object_funcs."
"print_info <drm_gem_object_funcs>` handler."
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:83
msgid "GEM object function for drm_gem_dma_get_sg_table()"
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:85
msgid ""
"This function wraps drm_gem_dma_get_sg_table(). Drivers that employ the DMA "
"helpers should use it as their :c:type:`drm_gem_object_funcs.get_sg_table "
"<drm_gem_object_funcs>` handler."
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:89
#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:418
msgid ""
"A pointer to the scatter/gather table of pinned pages or NULL on failure."
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:119
msgid "GEM object function for drm_gem_dma_mmap()"
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:122
msgid ""
"This function wraps drm_gem_dma_mmap(). Drivers that employ the dma helpers "
"should use it as their :c:type:`drm_gem_object_funcs.mmap "
"<drm_gem_object_funcs>` handler."
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:158
msgid "``DRM_GEM_DMA_DRIVER_OPS_WITH_DUMB_CREATE (dumb_create_func)``"
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:156
#: include/drm/drm_gem_dma_helper.h:173
msgid "DMA GEM driver operations"
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:162
#: include/drm/drm_gem_dma_helper.h:194
msgid "``dumb_create_func``"
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:157
#: include/drm/drm_gem_dma_helper.h:191
msgid "callback function for .dumb_create"
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:158
#: include/drm/drm_gem_dma_helper.h:174
msgid ""
"This macro provides a shortcut for setting the default GEM operations in "
"the :c:type:`drm_driver` structure."
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:161
msgid ""
"This macro is a variant of DRM_GEM_DMA_DRIVER_OPS for drivers that override "
"the default implementation of :c:type:`struct rm_driver <rm_driver>`."
"dumb_create. Use DRM_GEM_DMA_DRIVER_OPS if possible. Drivers that require a "
"virtual address on imported buffers should use "
"DRM_GEM_DMA_DRIVER_OPS_VMAP_WITH_DUMB_CREATE() instead."
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:175
msgid "``DRM_GEM_DMA_DRIVER_OPS``"
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:177
msgid ""
"Drivers that come with their own implementation of :c:type:`struct "
"drm_driver <drm_driver>`.dumb_create should use "
"DRM_GEM_DMA_DRIVER_OPS_WITH_DUMB_CREATE() instead. Use "
"DRM_GEM_DMA_DRIVER_OPS if possible. Drivers that require a virtual address "
"on imported buffers should use DRM_GEM_DMA_DRIVER_OPS_VMAP instead."
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:190
msgid "``DRM_GEM_DMA_DRIVER_OPS_VMAP_WITH_DUMB_CREATE (dumb_create_func)``"
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:188
#: include/drm/drm_gem_dma_helper.h:208
msgid "DMA GEM driver operations ensuring a virtual address on the buffer"
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:192
#: include/drm/drm_gem_dma_helper.h:209
msgid ""
"This macro provides a shortcut for setting the default GEM operations in "
"the :c:type:`drm_driver` structure for drivers that need the virtual address "
"also on imported buffers."
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:196
msgid ""
"This macro is a variant of DRM_GEM_DMA_DRIVER_OPS_VMAP for drivers that "
"override the default implementation of :c:type:`struct drm_driver "
"<drm_driver>`.dumb_create. Use DRM_GEM_DMA_DRIVER_OPS_VMAP if possible. "
"Drivers that do not require a virtual address on imported buffers should use "
"DRM_GEM_DMA_DRIVER_OPS_WITH_DUMB_CREATE() instead."
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:210
msgid "``DRM_GEM_DMA_DRIVER_OPS_VMAP``"
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:213
msgid ""
"Drivers that come with their own implementation of :c:type:`struct "
"drm_driver <drm_driver>`.dumb_create should use "
"DRM_GEM_DMA_DRIVER_OPS_VMAP_WITH_DUMB_CREATE() instead. Use "
"DRM_GEM_DMA_DRIVER_OPS_VMAP if possible. Drivers that do not require a "
"virtual address on imported buffers should use DRM_GEM_DMA_DRIVER_OPS "
"instead."
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:249
msgid "``DEFINE_DRM_GEM_DMA_FOPS (name)``"
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:247
msgid "macro to generate file operations for DMA drivers"
msgstr ""

#: ../../../gpu/drm-mm:364: include/drm/drm_gem_dma_helper.h:249
msgid ""
"This macro autogenerates a suitable :c:type:`struct file_operations "
"<file_operations>` for DMA based drivers, which can be assigned to :c:type:"
"`drm_driver.fops <drm_driver>`. Note that this structure cannot be shared "
"between drivers, because it contains a reference to the current module using "
"THIS_MODULE."
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:117
msgid "allocate an object with the given size"
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:123
#: drivers/gpu/drm/drm_gem_dma_helper.c:255
#: drivers/gpu/drm/drm_gem_dma_helper.c:288 ../../../gpu/drm-mm:506:
#: drivers/gpu/drm/drm_gpuvm.c:960 drivers/gpu/drm/drm_gpuvm.c:985
msgid "``struct drm_device *drm``"
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:118
#: drivers/gpu/drm/drm_gem_dma_helper.c:254
#: drivers/gpu/drm/drm_gem_dma_helper.c:287
#: drivers/gpu/drm/drm_gem_dma_helper.c:556 ../../../gpu/drm-mm:379:
#: drivers/gpu/drm/drm_gem_shmem_helper.c:116
#: drivers/gpu/drm/drm_gem_shmem_helper.c:134
#: drivers/gpu/drm/drm_gem_shmem_helper.c:504 ../../../gpu/drm-mm:442:
#: drivers/gpu/drm/drm_prime.c:843
msgid "DRM device"
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:119
msgid "size of the object to allocate"
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:120
msgid ""
"This function creates a DMA GEM object and allocates memory as backing "
"store. The allocated memory will occupy a contiguous chunk of bus address "
"space."
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:123
msgid ""
"For devices that are directly connected to the memory bus then the allocated "
"memory will be physically contiguous. For devices that access through an "
"IOMMU, then the allocated memory is not expected to be physically contiguous "
"because having contiguous IOVAs is sufficient to meet a devices DMA "
"requirements."
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:130
msgid ""
"A struct drm_gem_dma_object * on success or an ERR_PTR()-encoded negative "
"error code on failure."
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:219
msgid "free resources associated with a DMA GEM object"
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:225
#: drivers/gpu/drm/drm_gem_dma_helper.c:417
#: drivers/gpu/drm/drm_gem_dma_helper.c:494
#: drivers/gpu/drm/drm_gem_dma_helper.c:517
msgid "``struct drm_gem_dma_object *dma_obj``"
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:220
msgid "DMA GEM object to free"
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:221
msgid ""
"This function frees the backing memory of the DMA GEM object, cleans up the "
"GEM object state and frees the memory used to store the object itself. If "
"the buffer is imported and the virtual address is set, it is released."
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:252
#: drivers/gpu/drm/drm_gem_dma_helper.c:285
msgid "create a dumb buffer object"
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:253
#: drivers/gpu/drm/drm_gem_dma_helper.c:286
msgid "DRM file-private structure to create the dumb buffer for"
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:256
#: drivers/gpu/drm/drm_gem_dma_helper.c:289 ../../../gpu/drm-mm:379:
#: drivers/gpu/drm/drm_gem_shmem_helper.c:506 ../../../gpu/drm-mm:391:
#: drivers/gpu/drm/drm_gem_vram_helper.c:418
#: drivers/gpu/drm/drm_gem_vram_helper.c:535
msgid "``struct drm_mode_create_dumb *args``"
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:255
#: drivers/gpu/drm/drm_gem_dma_helper.c:288 ../../../gpu/drm-mm:379:
#: drivers/gpu/drm/drm_gem_shmem_helper.c:505
msgid "IOCTL data"
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:256
msgid ""
"This aligns the pitch and size arguments to the minimum required. This is an "
"internal helper that can be wrapped by a driver to account for hardware with "
"more specific alignment requirements. It should not be used directly as "
"their :c:type:`drm_driver.dumb_create <drm_driver>` callback."
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:289
#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:506
msgid ""
"This function computes the pitch of the dumb buffer and rounds it up to an "
"integer number of bytes per pixel. Drivers for hardware that doesn't have "
"any additional restrictions on the pitch can directly use this function as "
"their :c:type:`drm_driver.dumb_create <drm_driver>` callback."
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:294
msgid ""
"For hardware with additional restrictions, drivers can adjust the fields set "
"up by userspace and pass the IOCTL data along to the "
"drm_gem_dma_dumb_create_internal() function."
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:325
msgid "propose address for mapping in noMMU cases"
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:326
msgid "file object"
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:328
msgid "``unsigned long addr``"
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:327
msgid "memory address"
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:329
#: ../../../gpu/drm-mm:521: drivers/gpu/drm/drm_cache.c:298
msgid "``unsigned long len``"
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:328
msgid "buffer size"
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:330
msgid "``unsigned long pgoff``"
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:329
msgid "page offset"
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:331
#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:1016
msgid "``unsigned long flags``"
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:330
msgid "memory flags"
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:331
msgid ""
"This function is used in noMMU platforms to propose address mapping for a "
"given buffer. It's intended to be used as a direct handler for the struct :c:"
"type:`file_operations.get_unmapped_area <file_operations>` operation."
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:337
msgid "mapping address on success or a negative error code on failure."
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:401
msgid "``const struct drm_gem_dma_object *dma_obj``"
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:396
#: drivers/gpu/drm/drm_gem_dma_helper.c:413
#: drivers/gpu/drm/drm_gem_dma_helper.c:490
#: drivers/gpu/drm/drm_gem_dma_helper.c:512
msgid "DMA GEM object"
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:399
msgid "This function prints dma_addr and vaddr for use in e.g. debugfs output."
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:411
msgid "provide a scatter/gather table of pinned pages for a DMA GEM object"
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:414
msgid ""
"This function exports a scatter/gather table by calling the standard DMA "
"mapping API."
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:445
msgid ""
"produce a DMA GEM object from another driver's scatter/gather table of "
"pinned pages"
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:447
msgid "device to import into"
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:449
#: drivers/gpu/drm/drm_gem_dma_helper.c:558 ../../../gpu/drm-mm:379:
#: drivers/gpu/drm/drm_gem_shmem_helper.c:777 ../../../gpu/drm-mm:442:
#: drivers/gpu/drm/drm_prime.c:597 drivers/gpu/drm/drm_prime.c:635
#: drivers/gpu/drm/drm_prime.c:664 drivers/gpu/drm/drm_prime.c:705
msgid "``struct dma_buf_attachment *attach``"
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:448
#: drivers/gpu/drm/drm_gem_dma_helper.c:557 ../../../gpu/drm-mm:379:
#: drivers/gpu/drm/drm_gem_shmem_helper.c:776
msgid "DMA-BUF attachment"
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:450
#: drivers/gpu/drm/drm_gem_dma_helper.c:559 ../../../gpu/drm-mm:379:
#: drivers/gpu/drm/drm_gem_shmem_helper.c:778 ../../../gpu/drm-mm:442:
#: drivers/gpu/drm/drm_prime.c:702 drivers/gpu/drm/drm_prime.c:886
#: drivers/gpu/drm/drm_prime.c:1053 drivers/gpu/drm/drm_prime.c:1081
msgid "``struct sg_table *sgt``"
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:449
msgid "scatter/gather table of pinned pages"
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:450
msgid ""
"This function imports a scatter/gather table exported via DMA-BUF by another "
"driver. Imported buffers must be physically contiguous in memory (i.e. the "
"scatter/gather table must contain a single entry). Drivers that use the DMA "
"helpers should set this as their :c:type:`drm_driver."
"gem_prime_import_sg_table <drm_driver>` callback."
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:457
#: drivers/gpu/drm/drm_gem_dma_helper.c:569 ../../../gpu/drm-mm:379:
#: drivers/gpu/drm/drm_gem_shmem_helper.c:783
msgid ""
"A pointer to a newly created GEM object or an ERR_PTR-encoded negative error "
"code on failure."
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:488
msgid "map a DMA GEM object into the kernel's virtual address space"
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:492
#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:337
#: drivers/gpu/drm/drm_gem_vram_helper.c:380 ../../../gpu/drm-mm:400:
#: drivers/gpu/drm/drm_gem_ttm_helper.c:59
#: drivers/gpu/drm/drm_gem_ttm_helper.c:79 ../../../gpu/drm-mm:442:
#: drivers/gpu/drm/drm_prime.c:723 drivers/gpu/drm/drm_prime.c:742
msgid "``struct iosys_map *map``"
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:491
msgid ""
"Returns the kernel virtual address of the DMA GEM object's backing store."
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:493
msgid ""
"This function maps a buffer into the kernel's virtual address space. Since "
"the DMA buffers are already mapped into the kernel virtual address space "
"this simply returns the cached virtual address."
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:498
#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:345
#: drivers/gpu/drm/drm_gem_vram_helper.c:425
#: drivers/gpu/drm/drm_gem_vram_helper.c:540
msgid "0 on success, or a negative error code otherwise."
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:511
msgid "memory-map an exported DMA GEM object"
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:514
msgid ""
"This function maps a buffer into a userspace process's address space. In "
"addition to the usual GEM VMA setup it immediately faults in the entire "
"object instead of using on-demand faulting."
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:554
msgid ""
"PRIME import another driver's scatter/gather table and get the virtual "
"address of the buffer"
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:558
#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:777
msgid "Scatter/gather table of pinned pages"
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:559
msgid ""
"This function imports a scatter/gather table using "
"drm_gem_dma_prime_import_sg_table() and uses dma_buf_vmap() to get the "
"kernel virtual address. This ensures that a DMA GEM object always has its "
"virtual address set. This address is released when the object is freed."
msgstr ""

#: ../../../gpu/drm-mm:367: drivers/gpu/drm/drm_gem_dma_helper.c:564
msgid ""
"This function can be used as the :c:type:`drm_driver."
"gem_prime_import_sg_table <drm_driver>` callback. The :c:type:"
"`DRM_GEM_DMA_DRIVER_OPS_VMAP` macro provides a shortcut to set the necessary "
"DRM driver operations."
msgstr ""

#: ../../../gpu/drm-mm.rst:371
msgid "GEM SHMEM Helper Function Reference"
msgstr ""

#: ../../../gpu/drm-mm:373: drivers/gpu/drm/drm_gem_shmem_helper.c:28
msgid ""
"This library provides helpers for GEM objects backed by shmem buffers "
"allocated using anonymous pageable memory."
msgstr ""

#: ../../../gpu/drm-mm:373: drivers/gpu/drm/drm_gem_shmem_helper.c:31
msgid ""
"Functions that operate on the GEM object receive struct :c:type:"
"`drm_gem_shmem_object`. For GEM callback helpers in struct :c:type:"
"`drm_gem_object` functions, see likewise named functions with an _object_ "
"infix (e.g., drm_gem_shmem_object_vmap() wraps drm_gem_shmem_vmap()). These "
"helpers perform the necessary type conversion."
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:21
msgid "GEM object backed by shmem"
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:26
msgid "Base GEM object"
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:31
msgid "Page table"
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:35
msgid "``pages_use_count``"
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:36
msgid ""
"Reference count on the pages table. The pages are put when the count reaches "
"zero."
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:43
msgid "``pages_pin_count``"
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:44
msgid "Reference count on the pinned pages table."
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:46
msgid ""
"Pages are hard-pinned and reside in memory if count greater than zero. "
"Otherwise, when count is zero, the pages are allowed to be evicted and "
"purged by memory shrinker."
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:54
msgid "``madv``"
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:55
msgid "State for madvise"
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:57
msgid ""
"0 is active/inuse. A negative value is the object is purged. Positive values "
"are driver specific and not used by the helpers."
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:63
msgid "``madv_list``"
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:64
msgid "List entry for madvise tracking"
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:66
msgid "Typically used by drivers to track purgeable objects"
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:71
msgid "Scatter/gather table for imported PRIME buffers"
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:76
msgid "Kernel virtual address of the backing memory"
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:80
#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:56
msgid "``vmap_use_count``"
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:81
#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:57
msgid ""
"Reference count on the virtual address. The address are un-mapped when the "
"count reaches zero."
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:88
msgid "``pages_mark_dirty_on_put``"
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:89
msgid "Mark pages as dirty when they are put."
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:95
msgid "``pages_mark_accessed_on_put``"
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:96
msgid "Mark pages as accessed when they are put."
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:102
msgid "``map_wc``"
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:103
msgid "map object write-combined (instead of using shmem defaults)."
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:152
msgid "GEM object function for drm_gem_shmem_free()"
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:154
msgid ""
"This function wraps drm_gem_shmem_free(). Drivers that employ the shmem "
"helpers should use it as their :c:type:`drm_gem_object_funcs.free "
"<drm_gem_object_funcs>` handler."
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:166
#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:662
msgid "Print :c:type:`drm_gem_shmem_object` info for debugfs"
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:170
msgid ""
"This function wraps drm_gem_shmem_print_info(). Drivers that employ the "
"shmem helpers should use this function as their :c:type:"
"`drm_gem_object_funcs.print_info <drm_gem_object_funcs>` handler."
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:183
msgid "GEM object function for drm_gem_shmem_pin()"
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:185
msgid ""
"This function wraps drm_gem_shmem_pin(). Drivers that employ the shmem "
"helpers should use it as their :c:type:`drm_gem_object_funcs.pin "
"<drm_gem_object_funcs>` handler."
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:197
msgid "GEM object function for drm_gem_shmem_unpin()"
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:199
msgid ""
"This function wraps drm_gem_shmem_unpin(). Drivers that employ the shmem "
"helpers should use it as their :c:type:`drm_gem_object_funcs.unpin "
"<drm_gem_object_funcs>` handler."
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:211
msgid "GEM object function for drm_gem_shmem_get_sg_table()"
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:213
msgid ""
"This function wraps drm_gem_shmem_get_sg_table(). Drivers that employ the "
"shmem helpers should use it as their :c:type:`drm_gem_object_funcs."
"get_sg_table <drm_gem_object_funcs>` handler."
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:217
#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:691
msgid ""
"A pointer to the scatter/gather table of pinned pages or error pointer on "
"failure."
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:263
msgid "GEM object function for drm_gem_shmem_mmap()"
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:266
msgid ""
"This function wraps drm_gem_shmem_mmap(). Drivers that employ the shmem "
"helpers should use it as their :c:type:`drm_gem_object_funcs.mmap "
"<drm_gem_object_funcs>` handler."
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:296
msgid "``DRM_GEM_SHMEM_DRIVER_OPS``"
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:294
msgid "Default shmem GEM operations"
msgstr ""

#: ../../../gpu/drm-mm:376: include/drm/drm_gem_shmem_helper.h:295
msgid ""
"This macro provides a shortcut for setting the shmem GEM operations in the :"
"c:type:`drm_driver` structure. Drivers that do not require an s/g table for "
"imported buffers should use this."
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:115
msgid "Allocate an object with the given size"
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:117
#: drivers/gpu/drm/drm_gem_shmem_helper.c:135
msgid "Size of the object to allocate"
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:118
msgid "This function creates a shmem GEM object."
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:121
#: drivers/gpu/drm/drm_gem_shmem_helper.c:140
msgid ""
"A struct drm_gem_shmem_object * on success or an ERR_PTR()-encoded negative "
"error code on failure."
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:132
msgid "Allocate an object with the given size in a given mountpoint"
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:136
msgid "tmpfs mount where the GEM object will be created"
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:137
msgid "This function creates a shmem GEM object in a given tmpfs mountpoint."
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:153
msgid "Free resources associated with a shmem GEM object"
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:159
#: drivers/gpu/drm/drm_gem_shmem_helper.c:285
#: drivers/gpu/drm/drm_gem_shmem_helper.c:315
#: drivers/gpu/drm/drm_gem_shmem_helper.c:616
#: drivers/gpu/drm/drm_gem_shmem_helper.c:687
#: drivers/gpu/drm/drm_gem_shmem_helper.c:748
msgid "``struct drm_gem_shmem_object *shmem``"
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:154
msgid "shmem GEM object to free"
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:155
msgid ""
"This function cleans up the GEM object state and frees the memory used to "
"store the object itself."
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:279
msgid "Pin backing pages for a shmem GEM object"
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:280
#: drivers/gpu/drm/drm_gem_shmem_helper.c:310
#: drivers/gpu/drm/drm_gem_shmem_helper.c:611
#: drivers/gpu/drm/drm_gem_shmem_helper.c:663
#: drivers/gpu/drm/drm_gem_shmem_helper.c:683
#: drivers/gpu/drm/drm_gem_shmem_helper.c:744
msgid "shmem GEM object"
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:281
msgid ""
"This function makes sure the backing pages are pinned in memory while the "
"buffer is exported."
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:309
msgid "Unpin backing pages for a shmem GEM object"
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:311
msgid ""
"This function removes the requirement that the backing pages are pinned in "
"memory."
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:502
msgid "Create a dumb shmem buffer object"
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:503
msgid "DRM file structure to create the dumb buffer for"
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:511
msgid ""
"For hardware with additional restrictions, drivers can adjust the fields set "
"up by userspace before calling into this function."
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:610
msgid "Memory-map a shmem GEM object"
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:613
msgid ""
"This function implements an augmented version of the GEM DRM file mmap "
"operation for shmem objects."
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:668
msgid "``const struct drm_gem_shmem_object *shmem``"
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:681
msgid "Provide a scatter/gather table of pinned pages for a shmem GEM object"
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:684
msgid ""
"This function exports a scatter/gather table suitable for PRIME usage by "
"calling the standard DMA mapping API."
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:687
msgid ""
"Drivers who need to acquire an scatter/gather table for objects need to call "
"drm_gem_shmem_get_pages_sgt() instead."
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:742
msgid ""
"Pin pages, dma map them, and return a scatter/gather table for a shmem GEM "
"object."
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:745
msgid ""
"This function returns a scatter/gather table suitable for driver usage. If "
"the sg table doesn't exist, the pages are pinned, dma-mapped, and a sg table "
"created."
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:749
msgid ""
"This is the main function for drivers to get at backing storage, and it "
"hides and difference between dma-buf imported and natively allocated "
"objects. drm_gem_shmem_get_sg_table() should not be directly called by "
"drivers."
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:754
msgid ""
"A pointer to the scatter/gather table of pinned pages or errno on failure."
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:773
msgid ""
"Produce a shmem GEM object from another driver's scatter/gather table of "
"pinned pages"
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:775
#: drivers/gpu/drm/drm_gem_shmem_helper.c:809
msgid "Device to import into"
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:778
msgid ""
"This function imports a scatter/gather table exported via DMA-BUF by another "
"driver. Drivers that use the shmem helpers should set this as their :c:type:"
"`drm_driver.gem_prime_import_sg_table <drm_driver>` callback."
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:808
msgid "Import dmabuf without mapping its sg_table"
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:811
#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:267
#: drivers/gpu/drm/drm_prime.c:600 drivers/gpu/drm/drm_prime.c:638
#: drivers/gpu/drm/drm_prime.c:726 drivers/gpu/drm/drm_prime.c:745
#: drivers/gpu/drm/drm_prime.c:819 drivers/gpu/drm/drm_prime.c:943
#: drivers/gpu/drm/drm_prime.c:961 drivers/gpu/drm/drm_prime.c:1030
msgid "``struct dma_buf *dma_buf``"
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:810
#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:942
#: drivers/gpu/drm/drm_prime.c:960 drivers/gpu/drm/drm_prime.c:1029
msgid "dma-buf object to import"
msgstr ""

#: ../../../gpu/drm-mm:379: drivers/gpu/drm/drm_gem_shmem_helper.c:811
msgid ""
"Drivers that use the shmem helpers but also wants to import dmabuf without "
"mapping its sg_table can use this as their :c:type:`drm_driver."
"gem_prime_import <drm_driver>` implementation."
msgstr ""

#: ../../../gpu/drm-mm.rst:383
msgid "GEM VRAM Helper Functions Reference"
msgstr ""

#: ../../../gpu/drm-mm:385: drivers/gpu/drm/drm_gem_vram_helper.c:27
msgid ""
"This library provides :c:type:`struct drm_gem_vram_object "
"<drm_gem_vram_object>` (GEM VRAM), a GEM buffer object that is backed by "
"video RAM (VRAM). It can be used for framebuffer devices with dedicated "
"memory."
msgstr ""

#: ../../../gpu/drm-mm:385: drivers/gpu/drm/drm_gem_vram_helper.c:31
msgid ""
"The data structure :c:type:`struct drm_vram_mm <drm_vram_mm>` and its "
"helpers implement a memory manager for simple framebuffer devices with "
"dedicated video memory. GEM VRAM buffer objects are either placed in the "
"video memory or remain evicted to system memory."
msgstr ""

#: ../../../gpu/drm-mm:385: drivers/gpu/drm/drm_gem_vram_helper.c:36
msgid ""
"With the GEM interface userspace applications create, manage and destroy "
"graphics buffers, such as an on-screen framebuffer. GEM does not provide an "
"implementation of these interfaces. It's up to the DRM driver to provide an "
"implementation that suits the hardware. If the hardware device contains "
"dedicated video memory, the DRM driver can use the VRAM helper library. Each "
"active buffer object is stored in video RAM. Active buffer are used for "
"drawing the current frame, typically something like the frame's scanout "
"buffer or the cursor image. If there's no more space left in VRAM, inactive "
"GEM objects can be moved to system memory."
msgstr ""

#: ../../../gpu/drm-mm:385: drivers/gpu/drm/drm_gem_vram_helper.c:46
msgid ""
"To initialize the VRAM helper library call drmm_vram_helper_init(). The "
"function allocates and initializes an instance of :c:type:`struct "
"drm_vram_mm <drm_vram_mm>` in :c:type:`struct drm_device <drm_device>`."
"vram_mm . Use :c:type:`DRM_GEM_VRAM_DRIVER` to initialize :c:type:`struct "
"drm_driver <drm_driver>` and  :c:type:`DRM_VRAM_MM_FILE_OPERATIONS` to "
"initialize :c:type:`struct file_operations <file_operations>`; as "
"illustrated below."
msgstr ""

#: ../../../gpu/drm-mm:385: drivers/gpu/drm/drm_gem_vram_helper.c:80
msgid ""
"This creates an instance of :c:type:`struct drm_vram_mm <drm_vram_mm>`, "
"exports DRM userspace interfaces for GEM buffer management and initializes "
"file operations to allow for accessing created GEM buffers. With this setup, "
"the DRM driver manages an area of video RAM with VRAM MM and provides GEM "
"VRAM objects to userspace."
msgstr ""

#: ../../../gpu/drm-mm:385: drivers/gpu/drm/drm_gem_vram_helper.c:86
msgid ""
"You don't have to clean up the instance of VRAM MM. drmm_vram_helper_init() "
"is a managed interface that installs a clean-up handler to run during the "
"DRM device's release."
msgstr ""

#: ../../../gpu/drm-mm:385: drivers/gpu/drm/drm_gem_vram_helper.c:90
msgid ""
"A buffer object that is pinned in video RAM has a fixed address within that "
"memory region. Call drm_gem_vram_offset() to retrieve this value. Typically "
"it's used to program the hardware's scanout engine for framebuffers, set the "
"cursor overlay's image for a mouse cursor, or use it as input to the "
"hardware's drawing engine."
msgstr ""

#: ../../../gpu/drm-mm:385: drivers/gpu/drm/drm_gem_vram_helper.c:96
msgid ""
"To access a buffer object's memory from the DRM driver, call "
"drm_gem_vram_vmap(). It maps the buffer into kernel address space and "
"returns the memory address. Use drm_gem_vram_vunmap() to release the mapping."
msgstr ""

#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:32
msgid "GEM object backed by VRAM"
msgstr ""

#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:34
msgid "TTM buffer object"
msgstr ""

#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:34
#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:962
msgid "``map``"
msgstr ""

#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:35
msgid "Mapping information for **bo**"
msgstr ""

#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:36
msgid ""
"TTM placement information. Supported placements are ``TTM_PL_VRAM`` and "
"``TTM_PL_SYSTEM``"
msgstr ""

#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:37
msgid "``placements``"
msgstr ""

#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:38
msgid "TTM placement information."
msgstr ""

#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:38
msgid ""
"The type struct drm_gem_vram_object represents a GEM object that is backed "
"by VRAM. It can be used for simple framebuffer devices with dedicated "
"memory. The buffer object can be evicted to system memory if video memory "
"becomes scarce."
msgstr ""

#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:43
msgid ""
"GEM VRAM objects perform reference counting for pin and mapping operations. "
"So a buffer object that has been pinned N times with drm_gem_vram_pin() must "
"be unpinned N times with drm_gem_vram_unpin(). The same applies to pairs of "
"drm_gem_vram_kmap() and drm_gem_vram_kunmap(), as well as pairs of "
"drm_gem_vram_vmap() and drm_gem_vram_vunmap()."
msgstr ""

#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:69
msgid ""
"Returns the container of type :c:type:`struct drm_gem_vram_object "
"<drm_gem_vram_object>` for field bo."
msgstr ""

#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:71
msgid "the VRAM buffer object"
msgstr ""

#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:72
#: include/drm/drm_gem_vram_helper.h:84
msgid "The containing GEM VRAM object"
msgstr ""

#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:81
msgid ""
"Returns the container of type :c:type:`struct drm_gem_vram_object "
"<drm_gem_vram_object>` for field gem."
msgstr ""

#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:87
#: ../../../gpu/drm-mm:400: drivers/gpu/drm/drm_gem_ttm_helper.c:62
#: drivers/gpu/drm/drm_gem_ttm_helper.c:82
#: drivers/gpu/drm/drm_gem_ttm_helper.c:99
msgid "``struct drm_gem_object *gem``"
msgstr ""

#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:128
msgid "``DRM_GEM_VRAM_PLANE_HELPER_FUNCS``"
msgstr ""

#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:126
msgid "Initializes struct drm_plane_helper_funcs for VRAM handling"
msgstr ""

#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:127
msgid ""
"Drivers may use GEM BOs as VRAM helpers for the framebuffer memory. This "
"macro initializes struct drm_plane_helper_funcs to use the respective helper "
"functions."
msgstr ""

#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:140
msgid "``DRM_GEM_VRAM_DRIVER``"
msgstr ""

#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:138
msgid "default callback functions for :c:type:`struct drm_driver <drm_driver>`"
msgstr ""

#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:139
msgid ""
"Drivers that use VRAM MM and GEM VRAM can use this macro to initialize :c:"
"type:`struct drm_driver <drm_driver>` with default functions."
msgstr ""

#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:154
msgid "An instance of VRAM MM"
msgstr ""

#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:155
msgid "``vram_base``"
msgstr ""

#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:156
msgid "Base address of the managed video memory"
msgstr ""

#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:156
msgid "``vram_size``"
msgstr ""

#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:157
msgid "Size of the managed video memory in bytes"
msgstr ""

#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:158
msgid "The TTM BO device."
msgstr ""

#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:158
msgid ""
"The fields :c:type:`struct drm_vram_mm <drm_vram_mm>`.vram_base and :c:type:"
"`struct drm_vram_mm <drm_vram_mm>`.vrm_size are managed by VRAM MM, but are "
"available for public read access. Use the field :c:type:`struct drm_vram_mm "
"<drm_vram_mm>`.bdev to access the TTM BO device."
msgstr ""

#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:172
msgid ""
"Returns the container of type :c:type:`struct ttm_device <ttm_device>` for "
"field bdev."
msgstr ""

#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:174
msgid "the TTM BO device"
msgstr ""

#: ../../../gpu/drm-mm:388: include/drm/drm_gem_vram_helper.h:176
msgid "The containing instance of :c:type:`struct drm_vram_mm <drm_vram_mm>`"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:165
msgid "Creates a VRAM-backed GEM object"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:166
#: drivers/gpu/drm/drm_gem_vram_helper.c:414
#: drivers/gpu/drm/drm_gem_vram_helper.c:533
#: drivers/gpu/drm/drm_gem_vram_helper.c:926
#: drivers/gpu/drm/drm_gem_vram_helper.c:982
msgid "the DRM device"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:167
msgid "the buffer size in bytes"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:169
#: drivers/gpu/drm/drm_gem_vram_helper.c:416
msgid "``unsigned long pg_align``"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:168
#: drivers/gpu/drm/drm_gem_vram_helper.c:415
msgid "the buffer's alignment in multiples of the page size"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:169
msgid ""
"GEM objects are allocated by calling struct drm_driver.gem_create_object, if "
"set. Otherwise kzalloc() will be used. Drivers can set their own GEM object "
"functions in struct drm_driver.gem_create_object. If no functions are set, "
"the new GEM object will use the default functions from GEM VRAM helpers."
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:176
msgid ""
"A new instance of :c:type:`struct drm_gem_vram_object <drm_gem_vram_object>` "
"on success, or an ERR_PTR()-encoded error code otherwise."
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:234
msgid "Releases a reference to a VRAM-backed GEM object"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:240
#: drivers/gpu/drm/drm_gem_vram_helper.c:262
#: drivers/gpu/drm/drm_gem_vram_helper.c:339
#: drivers/gpu/drm/drm_gem_vram_helper.c:383
msgid "``struct drm_gem_vram_object *gbo``"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:235
#: drivers/gpu/drm/drm_gem_vram_helper.c:257
msgid "the GEM VRAM object"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:236
msgid "See ttm_bo_put() for more information."
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:256
msgid "Returns a GEM VRAM object's offset in video memory"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:258
msgid ""
"This function returns the buffer object's offset in the device's video "
"memory. The buffer object has to be pinned to ``TTM_PL_VRAM``."
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:262
msgid ""
"The buffer object's offset in video memory on success, or a negative errno "
"code otherwise."
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:333
msgid "Pins and maps a GEM VRAM object into kernel address space"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:335
msgid "The GEM VRAM object to map"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:336
msgid ""
"Returns the kernel virtual address of the VRAM GEM object's backing store."
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:338
msgid ""
"The vmap function pins a GEM VRAM object to its current location, either "
"system or video memory, and maps its buffer into kernel address space. As "
"pinned object cannot be relocated, you should avoid pinning objects "
"permanently. Call drm_gem_vram_vunmap() with the returned address to unmap "
"and unpin the GEM VRAM object."
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:377
msgid "Unmaps and unpins a GEM VRAM object"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:378
msgid "The GEM VRAM object to unmap"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:379
msgid "Kernel virtual address where the VRAM GEM object was mapped"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:380
msgid ""
"A call to drm_gem_vram_vunmap() unmaps and unpins a GEM VRAM buffer. See the "
"documentation for drm_gem_vram_vmap() for more information."
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:410
msgid ""
"Helper for implementing :c:type:`struct drm_driver <drm_driver>`.dumb_create"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:413
#: drivers/gpu/drm/drm_gem_vram_helper.c:532
msgid "the DRM file"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:417
msgid "``unsigned long pitch_align``"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:416
msgid "the scanline's alignment in powers of 2"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:417
#: drivers/gpu/drm/drm_gem_vram_helper.c:534
msgid ""
"the arguments as provided to :c:type:`struct drm_driver <drm_driver>`."
"dumb_create"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:419
msgid ""
"This helper function fills :c:type:`struct drm_mode_create_dumb "
"<drm_mode_create_dumb>`, which is used by :c:type:`struct drm_driver "
"<drm_driver>`.dumb_create. Implementations of this interface should forwards "
"their arguments to this helper, plus the driver-specific parameters."
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:531
msgid "Implements :c:type:`struct drm_driver <drm_driver>`.dumb_create"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:536
msgid ""
"This function requires the driver to use **drm_device.vram_mm** for its "
"instance of VRAM MM."
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:578
msgid ""
"Implements :c:type:`struct drm_plane_helper_funcs <drm_plane_helper_funcs>`."
"prepare_fb"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:584
#: drivers/gpu/drm/drm_gem_vram_helper.c:635
msgid "``struct drm_plane *plane``"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:580
#: drivers/gpu/drm/drm_gem_vram_helper.c:631
msgid "a DRM plane"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:582
msgid "``struct drm_plane_state *new_state``"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:581
msgid "the plane's new state"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:582
msgid ""
"During plane updates, this function sets the plane's fence and pins the GEM "
"VRAM objects of the plane's new framebuffer to VRAM. Call "
"drm_gem_vram_plane_helper_cleanup_fb() to unpin them."
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:587
#: drivers/gpu/drm/drm_gem_vram_helper.c:935 ../../../gpu/drm-mm:400:
#: drivers/gpu/drm/drm_gem_ttm_helper.c:63
#: drivers/gpu/drm/drm_gem_ttm_helper.c:133
msgid "0 on success, or a negative errno code otherwise."
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:629
msgid ""
"Implements :c:type:`struct drm_plane_helper_funcs <drm_plane_helper_funcs>`."
"cleanup_fb"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:633
msgid "``struct drm_plane_state *old_state``"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:632
msgid "the plane's old state"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:633
msgid ""
"During plane updates, this function unpins the GEM VRAM objects of the "
"plane's old framebuffer from VRAM. Complements "
"drm_gem_vram_plane_helper_prepare_fb()."
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:839
msgid "Register VRAM MM debugfs file."
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:845
msgid "``struct drm_minor *minor``"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:841
msgid "drm minor device."
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:924
msgid ""
"Initializes a device's instance of :c:type:`struct drm_vram_mm <drm_vram_mm>`"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:928
msgid "``uint64_t vram_base``"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:927
msgid "the base address of the video memory"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:929
msgid "``size_t vram_size``"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:928
msgid "the size of the video memory in bytes"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:929
msgid ""
"Creates a new instance of :c:type:`struct drm_vram_mm <drm_vram_mm>` and "
"stores it in struct :c:type:`drm_device.vram_mm <drm_device>`. The instance "
"is auto-managed and cleaned up as part of device cleanup. Calling this "
"function multiple times will generate an error message."
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:980
msgid ""
"Tests if a display mode's framebuffer fits into the available video memory."
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:984
msgid "``const struct drm_display_mode *mode``"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:983
msgid "the mode to test"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:984
msgid ""
"This function tests if enough video memory is available for using the "
"specified display mode. Atomic modesetting requires importing the designated "
"framebuffer into video memory before evicting the active one. Hence, any "
"framebuffer may consume at most half of the available VRAM. Display modes "
"that require a larger framebuffer can not be used, even if the CRTC does "
"support them. Each framebuffer is assumed to have 32-bit color depth."
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:992
#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:119
#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1174
#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:608
#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:565
msgid "**Note**"
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:993
msgid ""
"The function can only test if the display mode is supported in general. If "
"there are too many framebuffers pinned to video memory, a display mode may "
"still not be usable in practice. The color depth of 32-bit fits all current "
"use case. A more flexible test can be added when necessary."
msgstr ""

#: ../../../gpu/drm-mm:391: drivers/gpu/drm/drm_gem_vram_helper.c:1000
msgid ""
"MODE_OK if the display mode is supported, or an error code of type enum "
"drm_mode_status otherwise."
msgstr ""

#: ../../../gpu/drm-mm.rst:395
msgid "GEM TTM Helper Functions Reference"
msgstr ""

#: ../../../gpu/drm-mm:397: drivers/gpu/drm/drm_gem_ttm_helper.c:11
msgid "This library provides helper functions for gem objects backed by ttm."
msgstr ""

#: ../../../gpu/drm-mm:400: drivers/gpu/drm/drm_gem_ttm_helper.c:18
msgid "Print :c:type:`ttm_buffer_object` info for debugfs"
msgstr ""

#: ../../../gpu/drm-mm:400: drivers/gpu/drm/drm_gem_ttm_helper.c:22
msgid "``const struct drm_gem_object *gem``"
msgstr ""

#: ../../../gpu/drm-mm:400: drivers/gpu/drm/drm_gem_ttm_helper.c:22
msgid ""
"This function can be used as :c:type:`drm_gem_object_funcs.print_info "
"<drm_gem_object_funcs>` callback."
msgstr ""

#: ../../../gpu/drm-mm:400: drivers/gpu/drm/drm_gem_ttm_helper.c:56
msgid "vmap :c:type:`ttm_buffer_object`"
msgstr ""

#: ../../../gpu/drm-mm:400: drivers/gpu/drm/drm_gem_ttm_helper.c:57
#: drivers/gpu/drm/drm_gem_ttm_helper.c:77
#: drivers/gpu/drm/drm_gem_ttm_helper.c:94
msgid "GEM object."
msgstr ""

#: ../../../gpu/drm-mm:400: drivers/gpu/drm/drm_gem_ttm_helper.c:58
msgid "[out] returns the dma-buf mapping."
msgstr ""

#: ../../../gpu/drm-mm:400: drivers/gpu/drm/drm_gem_ttm_helper.c:59
msgid ""
"Maps a GEM object with ttm_bo_vmap(). This function can be used as :c:type:"
"`drm_gem_object_funcs.vmap <drm_gem_object_funcs>` callback."
msgstr ""

#: ../../../gpu/drm-mm:400: drivers/gpu/drm/drm_gem_ttm_helper.c:76
msgid "vunmap :c:type:`ttm_buffer_object`"
msgstr ""

#: ../../../gpu/drm-mm:400: drivers/gpu/drm/drm_gem_ttm_helper.c:78
msgid "dma-buf mapping."
msgstr ""

#: ../../../gpu/drm-mm:400: drivers/gpu/drm/drm_gem_ttm_helper.c:79
msgid ""
"Unmaps a GEM object with ttm_bo_vunmap(). This function can be used as :c:"
"type:`drm_gem_object_funcs.vmap <drm_gem_object_funcs>` callback."
msgstr ""

#: ../../../gpu/drm-mm:400: drivers/gpu/drm/drm_gem_ttm_helper.c:93
msgid "mmap :c:type:`ttm_buffer_object`"
msgstr ""

#: ../../../gpu/drm-mm:400: drivers/gpu/drm/drm_gem_ttm_helper.c:95
msgid "vm area."
msgstr ""

#: ../../../gpu/drm-mm:400: drivers/gpu/drm/drm_gem_ttm_helper.c:96
msgid ""
"This function can be used as :c:type:`drm_gem_object_funcs.mmap "
"<drm_gem_object_funcs>` callback."
msgstr ""

#: ../../../gpu/drm-mm:400: drivers/gpu/drm/drm_gem_ttm_helper.c:121
msgid "Implements struct :c:type:`drm_driver.dumb_map_offset <drm_driver>`"
msgstr ""

#: ../../../gpu/drm-mm:400: drivers/gpu/drm/drm_gem_ttm_helper.c:122
msgid "DRM file pointer."
msgstr ""

#: ../../../gpu/drm-mm:400: drivers/gpu/drm/drm_gem_ttm_helper.c:123
msgid "DRM device."
msgstr ""

#: ../../../gpu/drm-mm:400: drivers/gpu/drm/drm_gem_ttm_helper.c:125
#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:417
#: drivers/gpu/drm/drm_prime.c:505
msgid "``uint32_t handle``"
msgstr ""

#: ../../../gpu/drm-mm:400: drivers/gpu/drm/drm_gem_ttm_helper.c:124
msgid "GEM handle"
msgstr ""

#: ../../../gpu/drm-mm:400: drivers/gpu/drm/drm_gem_ttm_helper.c:126
msgid "``uint64_t *offset``"
msgstr ""

#: ../../../gpu/drm-mm:400: drivers/gpu/drm/drm_gem_ttm_helper.c:125
msgid "Returns the mapping's memory offset on success"
msgstr ""

#: ../../../gpu/drm-mm:400: drivers/gpu/drm/drm_gem_ttm_helper.c:126
msgid ""
"Provides an implementation of struct :c:type:`drm_driver.dumb_map_offset "
"<drm_driver>` for TTM-based GEM drivers. TTM allocates the offset internally "
"and drm_gem_ttm_dumb_map_offset() returns it for dumb-buffer implementations."
msgstr ""

#: ../../../gpu/drm-mm:400: drivers/gpu/drm/drm_gem_ttm_helper.c:130
msgid "See struct :c:type:`drm_driver.dumb_map_offset <drm_driver>`."
msgstr ""

#: ../../../gpu/drm-mm.rst:404
msgid "VMA Offset Manager"
msgstr ""

#: ../../../gpu/drm-mm:406: drivers/gpu/drm/drm_vma_manager.c:38
msgid ""
"The vma-manager is responsible to map arbitrary driver-dependent memory "
"regions into the linear user address-space. It provides offsets to the "
"caller which can then be used on the address_space of the drm-device. It "
"takes care to not overlap regions, size them appropriately and to not "
"confuse mm-core by inconsistent fake vm_pgoff fields. Drivers shouldn't use "
"this for object placement in VMEM. This manager should only be used to "
"manage mappings into linear user-space VMs."
msgstr ""

#: ../../../gpu/drm-mm:406: drivers/gpu/drm/drm_vma_manager.c:46
msgid ""
"We use drm_mm as backend to manage object allocations. But it is highly "
"optimized for alloc/free calls, not lookups. Hence, we use an rb-tree to "
"speed up offset lookups."
msgstr ""

#: ../../../gpu/drm-mm:406: drivers/gpu/drm/drm_vma_manager.c:50
msgid ""
"You must not use multiple offset managers on a single address_space. "
"Otherwise, mm-core will be unable to tear down memory mappings as the VM "
"will no longer be linear."
msgstr ""

#: ../../../gpu/drm-mm:406: drivers/gpu/drm/drm_vma_manager.c:54
msgid ""
"This offset manager works on page-based addresses. That is, every argument "
"and return code (with the exception of drm_vma_node_offset_addr()) is given "
"in number of pages, not number of bytes. That means, object sizes and "
"offsets must always be page-aligned (as usual). If you want to get a valid "
"byte-based user-space address for a given offset, please see "
"drm_vma_node_offset_addr()."
msgstr ""

#: ../../../gpu/drm-mm:406: drivers/gpu/drm/drm_vma_manager.c:61
msgid ""
"Additionally to offset management, the vma offset manager also handles "
"access management. For every open-file context that is allowed to access a "
"given node, you must call drm_vma_node_allow(). Otherwise, an mmap() call on "
"this open-file with the offset of the node will fail with -EACCES. To revoke "
"access again, use drm_vma_node_revoke(). However, the caller is responsible "
"for destroying already existing mappings, if required."
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:84
msgid "Look up node by exact address"
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:90
#: include/drm/drm_vma_manager.h:113 include/drm/drm_vma_manager.h:134
#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:78
#: drivers/gpu/drm/drm_vma_manager.c:101 drivers/gpu/drm/drm_vma_manager.c:118
#: drivers/gpu/drm/drm_vma_manager.c:186 drivers/gpu/drm/drm_vma_manager.c:226
msgid "``struct drm_vma_offset_manager *mgr``"
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:85
#: include/drm/drm_vma_manager.h:108 include/drm/drm_vma_manager.h:129
#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:73
#: drivers/gpu/drm/drm_vma_manager.c:96 drivers/gpu/drm/drm_vma_manager.c:113
#: drivers/gpu/drm/drm_vma_manager.c:181 drivers/gpu/drm/drm_vma_manager.c:221
msgid "Manager object"
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:87
#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:115
msgid "``unsigned long start``"
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:86
msgid "Start address (page-based, not byte-based)"
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:88
#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:116
#: drivers/gpu/drm/drm_vma_manager.c:184
msgid "``unsigned long pages``"
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:87
#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:115
msgid "Size of object (page-based)"
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:88
msgid ""
"Same as drm_vma_offset_lookup_locked() but does not allow any offset into "
"the node. It only returns the exact object with the given start address."
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:92
msgid "Node at exact start address **start**."
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:107
msgid "Lock lookup for extended private use"
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:109
msgid ""
"Lock VMA manager for extended lookups. Only locked VMA function calls are "
"allowed while holding this lock. All other contexts are blocked from VMA "
"until the lock is released via drm_vma_offset_unlock_lookup()."
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:113
msgid ""
"Use this if you need to take a reference to the objects returned by "
"drm_vma_offset_lookup_locked() before releasing this lock again."
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:116
msgid ""
"This lock must not be used for anything else than extended lookups. You must "
"not call any other VMA helpers while holding this lock."
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:120
msgid "You're in atomic-context while holding this lock!"
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:128
msgid "Unlock lookup for extended private use"
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:130
msgid ""
"Release lookup-lock. See drm_vma_offset_lock_lookup() for more information."
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:139
msgid "Initialize or reset node object"
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:145
#: include/drm/drm_vma_manager.h:181 include/drm/drm_vma_manager.h:198
#: include/drm/drm_vma_manager.h:215 include/drm/drm_vma_manager.h:236
#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:183
#: drivers/gpu/drm/drm_vma_manager.c:223 drivers/gpu/drm/drm_vma_manager.c:301
#: drivers/gpu/drm/drm_vma_manager.c:327 drivers/gpu/drm/drm_vma_manager.c:352
#: drivers/gpu/drm/drm_vma_manager.c:393
msgid "``struct drm_vma_offset_node *node``"
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:140
msgid "Node to initialize or reset"
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:141
msgid ""
"Reset a node to its initial state. This must be called before using it with "
"any VMA offset manager."
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:144
msgid ""
"This must not be called on an already allocated node, or you will leak "
"memory."
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:156
msgid "Return start address for page-based addressing"
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:162
msgid "``const struct drm_vma_offset_node *node``"
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:157
#: include/drm/drm_vma_manager.h:176
msgid "Node to inspect"
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:158
msgid ""
"Return the start address of the given node. This can be used as offset into "
"the linear VM space that is provided by the VMA offset manager. Note that "
"this can only be used for page-based addressing. If you need a proper offset "
"for user-space mappings, you must apply \"<< PAGE_SHIFT\" or use the "
"drm_vma_node_offset_addr() helper instead."
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:165
msgid ""
"Start address of **node** for page-based addressing. 0 if the node does not "
"have an offset allocated."
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:175
msgid "Return size (page-based)"
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:177
msgid ""
"Return the size as number of pages for the given node. This is the same size "
"that was passed to drm_vma_offset_add(). If no offset is allocated for the "
"node, this is 0."
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:182
msgid ""
"Size of **node** as number of pages. 0 if the node does not have an offset "
"allocated."
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:192
msgid "Return sanitized offset for user-space mmaps"
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:193
msgid "Linked offset node"
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:194
msgid ""
"Same as drm_vma_node_start() but returns the address as a valid offset that "
"can be used for user-space mappings during mmap(). This must not be called "
"on unlinked nodes."
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:199
msgid ""
"Offset of **node** for byte-based addressing. 0 if the node does not have an "
"object allocated."
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:209
msgid "Unmap offset node"
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:210
#: include/drm/drm_vma_manager.h:231
msgid "Offset node"
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:212
msgid "``struct address_space *file_mapping``"
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:211
msgid "Address space to unmap **node** from"
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:212
msgid ""
"Unmap all userspace mappings for a given offset node. The mappings must be "
"associated with the **file_mapping** address-space. If no offset exists "
"nothing is done."
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:216
msgid ""
"This call is unlocked. The caller must guarantee that "
"drm_vma_offset_remove() is not called on this node concurrently."
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:230
msgid "Access verification helper for TTM"
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:233
#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:298
#: drivers/gpu/drm/drm_vma_manager.c:324 drivers/gpu/drm/drm_vma_manager.c:349
#: drivers/gpu/drm/drm_vma_manager.c:390
msgid "``struct drm_file *tag``"
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:232
msgid "Tag of file to check"
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:233
msgid ""
"This checks whether **tag** is granted access to **node**. It is the same as "
"drm_vma_node_is_allowed() but suitable as drop-in helper for TTM "
"verify_access() callbacks."
msgstr ""

#: ../../../gpu/drm-mm:409: include/drm/drm_vma_manager.h:238
msgid "0 if access is granted, -EACCES otherwise."
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:72
msgid "Initialize new offset-manager"
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:75
msgid "``unsigned long page_offset``"
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:74
msgid "Offset of available memory area (page-based)"
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:76
msgid "``unsigned long size``"
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:75
msgid "Size of available address space range (page-based)"
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:76
msgid ""
"Initialize a new offset-manager. The offset and area size available for the "
"manager are given as **page_offset** and **size**. Both are interpreted as "
"page-numbers, not bytes."
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:80
msgid ""
"Adding/removing nodes from the manager is locked internally and protected "
"against concurrent access. However, node allocation and destruction is left "
"for the caller. While calling into the vma-manager, a given node must always "
"be guaranteed to be referenced."
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:95
msgid "Destroy offset manager"
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:97
msgid ""
"Destroy an object manager which was previously created via "
"drm_vma_offset_manager_init(). The caller must remove all allocated nodes "
"before destroying the manager. Otherwise, drm_mm will refuse to free the "
"requested resources."
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:102
msgid "The manager must not be accessed after this function is called."
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:112
msgid "Find node in offset space"
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:114
msgid "Start address for object (page-based)"
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:116
msgid ""
"Find a node given a start address and object size. This returns the _best_ "
"match for the given node. That is, **start** may point somewhere into a "
"valid region and the given node will be returned, as long as the node spans "
"the whole requested area (given the size in number of pages as **pages**)."
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:121
msgid ""
"Note that before lookup the vma offset manager lookup lock must be acquired "
"with drm_vma_offset_lock_lookup(). See there for an example. This can then "
"be used to implement weakly referenced lookups using kref_get_unless_zero()."
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:125
msgid "**Example**"
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:136
msgid ""
"Returns NULL if no suitable node can be found. Otherwise, the best match is "
"returned. It's the caller's responsibility to make sure the node doesn't get "
"destroyed before the caller can access it."
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:180
msgid "Add offset node to manager"
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:182
msgid "Node to be added"
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:183
msgid "Allocation size visible to user-space (in number of pages)"
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:184
msgid ""
"Add a node to the offset-manager. If the node was already added, this does "
"nothing and return 0. **pages** is the size of the object given in number of "
"pages. After this call succeeds, you can access the offset of the node until "
"it is removed again."
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:190
msgid ""
"If this call fails, it is safe to retry the operation or call "
"drm_vma_offset_remove(), anyway. However, no cleanup is required in that "
"case."
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:194
msgid ""
"**pages** is not required to be the same size as the underlying memory "
"object that you want to map. It only limits the size that user-space can map "
"into their address space."
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:220
msgid "Remove offset node from manager"
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:222
msgid "Node to be removed"
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:223
msgid ""
"Remove a node from the offset manager. If the node wasn't added before, this "
"does nothing. After this call returns, the offset and size will be 0 until a "
"new offset is allocated via drm_vma_offset_add() again. Helper functions "
"like drm_vma_node_start() and drm_vma_node_offset_addr() will return 0 if no "
"offset is allocated."
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:295
#: drivers/gpu/drm/drm_vma_manager.c:321
msgid "Add open-file to list of allowed users"
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:296
#: drivers/gpu/drm/drm_vma_manager.c:322 drivers/gpu/drm/drm_vma_manager.c:347
msgid "Node to modify"
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:297
#: drivers/gpu/drm/drm_vma_manager.c:323 drivers/gpu/drm/drm_vma_manager.c:348
#: drivers/gpu/drm/drm_vma_manager.c:389
msgid "Tag of file to remove"
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:298
msgid ""
"Add **tag** to the list of allowed open-files for this node. If **tag** is "
"already on this list, the ref-count is incremented."
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:301
#: drivers/gpu/drm/drm_vma_manager.c:326
msgid ""
"The list of allowed-users is preserved across drm_vma_offset_add() and "
"drm_vma_offset_remove() calls. You may even call it if the node is currently "
"not added to any offset-manager."
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:305
msgid ""
"You must remove all open-files the same number of times as you added them "
"before destroying the node. Otherwise, you will leak memory."
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:308
#: drivers/gpu/drm/drm_vma_manager.c:333 drivers/gpu/drm/drm_vma_manager.c:353
#: drivers/gpu/drm/drm_vma_manager.c:393
msgid "This is locked against concurrent access internally."
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:311
#: drivers/gpu/drm/drm_vma_manager.c:336
msgid "0 on success, negative error code on internal failure (out-of-mem)"
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:324
msgid "Add **tag** to the list of allowed open-files for this node."
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:330
msgid ""
"This is not ref-counted unlike drm_vma_node_allow() hence "
"drm_vma_node_revoke() should only be called once after this."
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:346
msgid "Remove open-file from list of allowed users"
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:349
msgid ""
"Decrement the ref-count of **tag** in the list of allowed open-files on "
"**node**. If the ref-count drops to zero, remove **tag** from the list. You "
"must call this once for every drm_vma_node_allow() on **tag**."
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:355
msgid "If **tag** is not on the list, nothing is done."
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:387
msgid "Check whether an open-file is granted access"
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:388
msgid "Node to check"
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:390
msgid ""
"Search the list in **node** whether **tag** is currently on the list of "
"allowed open-files (see drm_vma_node_allow())."
msgstr ""

#: ../../../gpu/drm-mm:412: drivers/gpu/drm/drm_vma_manager.c:396
msgid "true if **filp** is on the list"
msgstr ""

#: ../../../gpu/drm-mm.rst:418
msgid "PRIME Buffer Sharing"
msgstr ""

#: ../../../gpu/drm-mm.rst:420
msgid ""
"PRIME is the cross device buffer sharing framework in drm, originally "
"created for the OPTIMUS range of multi-gpu platforms. To userspace PRIME "
"buffers are dma-buf based file descriptors."
msgstr ""

#: ../../../gpu/drm-mm.rst:425
msgid "Overview and Lifetime Rules"
msgstr ""

#: ../../../gpu/drm-mm:427: drivers/gpu/drm/drm_prime.c:46
msgid ""
"Similar to GEM global names, PRIME file descriptors are also used to share "
"buffer objects across processes. They offer additional security: as file "
"descriptors must be explicitly sent over UNIX domain sockets to be shared "
"between applications, they can't be guessed like the globally unique GEM "
"names."
msgstr ""

#: ../../../gpu/drm-mm:427: drivers/gpu/drm/drm_prime.c:52
msgid ""
"Drivers that support the PRIME API implement the drm_gem_object_funcs.export "
"and :c:type:`drm_driver.gem_prime_import <drm_driver>` hooks. :c:type:"
"`dma_buf_ops` implementations for drivers are all individually exported for "
"drivers which need to overwrite or reimplement some of them."
msgstr ""

#: ../../../gpu/drm-mm:427: drivers/gpu/drm/drm_prime.c:60
msgid ""
"On the export the :c:type:`dma_buf` holds a reference to the exported buffer "
"object, usually a :c:type:`drm_gem_object`. It takes this reference in the "
"PRIME_HANDLE_TO_FD IOCTL, when it first calls :c:type:`drm_gem_object_funcs."
"export <drm_gem_object_funcs>` and stores the exporting GEM object in the :c:"
"type:`dma_buf.priv <dma_buf>` field. This reference needs to be released "
"when the final reference to the :c:type:`dma_buf` itself is dropped and its :"
"c:type:`dma_buf_ops.release <dma_buf_ops>` function is called.  For GEM-"
"based drivers, the :c:type:`dma_buf` should be exported using "
"drm_gem_dmabuf_export() and then released by drm_gem_dmabuf_release()."
msgstr ""

#: ../../../gpu/drm-mm:427: drivers/gpu/drm/drm_prime.c:69
msgid ""
"Thus the chain of references always flows in one direction, avoiding loops: "
"importing GEM object -> dma-buf -> exported GEM bo. A further complication "
"are the lookup caches for import and export. These are required to guarantee "
"that any given object will always have only one unique userspace handle. "
"This is required to allow userspace to detect duplicated imports, since some "
"GEM drivers do fail command submissions if a given buffer object is listed "
"more than once. These import and export caches in :c:type:"
"`drm_prime_file_private` only retain a weak reference, which is cleaned up "
"when the corresponding object is released."
msgstr ""

#: ../../../gpu/drm-mm:427: drivers/gpu/drm/drm_prime.c:79
msgid ""
"Self-importing: If userspace is using PRIME as a replacement for flink then "
"it will get a fd->handle request for a GEM object that it created.  Drivers "
"should detect this situation and return back the underlying object from the "
"dma-buf private. For GEM based drivers this is handled in "
"drm_gem_prime_import() already."
msgstr ""

#: ../../../gpu/drm-mm.rst:431
msgid "PRIME Helper Functions"
msgstr ""

#: ../../../gpu/drm-mm:433: drivers/gpu/drm/drm_prime.c:555
msgid ""
"Drivers can implement :c:type:`drm_gem_object_funcs.export "
"<drm_gem_object_funcs>` and :c:type:`drm_driver.gem_prime_import "
"<drm_driver>` in terms of simpler APIs by using the helper functions "
"drm_gem_prime_export() and drm_gem_prime_import(). These functions implement "
"dma-buf support in terms of some lower-level helpers, which are again "
"exported for drivers to use individually:"
msgstr ""

#: ../../../gpu/drm-mm:433: drivers/gpu/drm/drm_prime.c:564
msgid ""
"Optional pinning of buffers is handled at dma-buf attach and detach time in "
"drm_gem_map_attach() and drm_gem_map_detach(). Backing storage itself is "
"handled by drm_gem_map_dma_buf() and drm_gem_unmap_dma_buf(), which relies "
"on :c:type:`drm_gem_object_funcs.get_sg_table <drm_gem_object_funcs>`. If :c:"
"type:`drm_gem_object_funcs.get_sg_table <drm_gem_object_funcs>` is "
"unimplemented, exports into another device are rejected."
msgstr ""

#: ../../../gpu/drm-mm:433: drivers/gpu/drm/drm_prime.c:570
msgid ""
"For kernel-internal access there's drm_gem_dmabuf_vmap() and "
"drm_gem_dmabuf_vunmap(). Userspace mmap support is provided by "
"drm_gem_dmabuf_mmap()."
msgstr ""

#: ../../../gpu/drm-mm:433: drivers/gpu/drm/drm_prime.c:574
msgid ""
"Note that these export helpers can only be used if the underlying backing "
"storage is fully coherent and either permanently pinned, or it is safe to "
"pin it indefinitely."
msgstr ""

#: ../../../gpu/drm-mm:433: drivers/gpu/drm/drm_prime.c:578
msgid "FIXME: The underlying helper functions are named rather inconsistently."
msgstr ""

#: ../../../gpu/drm-mm:433: drivers/gpu/drm/drm_prime.c:583
msgid ""
"Importing dma-bufs using drm_gem_prime_import() relies on :c:type:"
"`drm_driver.gem_prime_import_sg_table <drm_driver>`."
msgstr ""

#: ../../../gpu/drm-mm:433: drivers/gpu/drm/drm_prime.c:586
msgid ""
"Note that similarly to the export helpers this permanently pins the "
"underlying backing storage. Which is ok for scanout, but is not the best "
"option for sharing lots of buffers for rendering."
msgstr ""

#: ../../../gpu/drm-mm.rst:437
msgid "PRIME Function References"
msgstr ""

#: ../../../gpu/drm-mm:439: include/drm/drm_prime.h:40
msgid "per-file tracking for PRIME"
msgstr ""

#: ../../../gpu/drm-mm:439: include/drm/drm_prime.h:41
msgid ""
"This just contains the internal :c:type:`struct dma_buf <dma_buf>` and "
"handle caches for each :c:type:`struct drm_file <drm_file>` used by the "
"PRIME core code."
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:231
msgid ":c:type:`dma_buf` export implementation for GEM"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:232
msgid "parent device for the exported dmabuf"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:234
msgid "``struct dma_buf_export_info *exp_info``"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:233
msgid "the export information used by dma_buf_export()"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:234
msgid ""
"This wraps dma_buf_export() for use by generic GEM drivers that are using "
"drm_gem_dmabuf_release(). In addition to calling dma_buf_export(), we take a "
"reference to the :c:type:`drm_device` and the exported :c:type:"
"`drm_gem_object` (stored in :c:type:`dma_buf_export_info.priv "
"<dma_buf_export_info>`) which is released by drm_gem_dmabuf_release()."
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:239
msgid "Returns the new dmabuf."
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:261
msgid ":c:type:`dma_buf` release implementation for GEM"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:262
msgid "buffer to be released"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:263
msgid ""
"Generic release function for dma_bufs exported as PRIME buffers. GEM drivers "
"must use this in their :c:type:`dma_buf_ops` structure as the release "
"callback. drm_gem_dmabuf_release() should be used in conjunction with "
"drm_gem_dmabuf_export()."
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:282
msgid "PRIME import function for GEM drivers"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:283
#: drivers/gpu/drm/drm_prime.c:959 drivers/gpu/drm/drm_prime.c:1028
msgid "drm_device to import into"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:284
#: drivers/gpu/drm/drm_prime.c:415 drivers/gpu/drm/drm_prime.c:503
msgid "drm file-private structure"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:286
msgid "``int prime_fd``"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:285
msgid "fd id of the dma-buf which should be imported"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:287
msgid "``uint32_t *handle``"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:286
msgid "pointer to storage for the handle of the imported buffer object"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:287
msgid ""
"This is the PRIME import function which must be used mandatorily by GEM "
"drivers to ensure correct lifetime management of the underlying GEM object. "
"The actual importing of GEM object from the dma-buf is done through the :c:"
"type:`drm_driver.gem_prime_import <drm_driver>` driver callback."
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:292
#: drivers/gpu/drm/drm_prime.c:819 ../../../gpu/drm-mm:574:
#: drivers/gpu/drm/scheduler/sched_entity.c:55
msgid "Returns 0 on success or a negative error code on failure."
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:413
#: drivers/gpu/drm/drm_prime.c:501
msgid "PRIME export function for GEM drivers"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:414
#: drivers/gpu/drm/drm_prime.c:502
msgid "dev to export the buffer from"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:416
#: drivers/gpu/drm/drm_prime.c:504
msgid "buffer handle to export"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:418
#: drivers/gpu/drm/drm_prime.c:506 ../../../gpu/drm-mm:535:
#: drivers/gpu/drm/drm_syncobj.c:546
msgid "``uint32_t flags``"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:417
#: drivers/gpu/drm/drm_prime.c:505
msgid "flags like DRM_CLOEXEC"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:418
#: drivers/gpu/drm/drm_prime.c:507
msgid ""
"This is the PRIME export function which must be used mandatorily by GEM "
"drivers to ensure correct lifetime management of the underlying GEM object. "
"The actual exporting from GEM object to a dma-buf is done through the :c:"
"type:`drm_gem_object_funcs.export <drm_gem_object_funcs>` callback."
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:423
msgid ""
"Unlike drm_gem_prime_handle_to_fd(), it returns the struct dma_buf it has "
"created, without attaching it to any file descriptors.  The difference "
"between those two is similar to that between anon_inode_getfile() and "
"anon_inode_getfd(); insertion into descriptor table is something you can not "
"revert if any cleanup is needed, so the descriptor-returning variants should "
"only be used when you are past the last failure exit and the only thing left "
"is passing the new file descriptor to userland. When all you need is the "
"object itself or when you need to do something else that might fail, use "
"that one instead."
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:507
msgid "``int *prime_fd``"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:506
msgid "pointer to storage for the fd id of the create dma-buf"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:594
msgid "dma_buf attach implementation for GEM"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:595
msgid "buffer to attach device to"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:596
msgid "buffer attachment data"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:597
msgid ""
"Calls :c:type:`drm_gem_object_funcs.pin <drm_gem_object_funcs>` for device "
"specific handling. This can be used as the :c:type:`dma_buf_ops.attach "
"<dma_buf_ops>` callback. Must be used together with drm_gem_map_detach()."
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:601
msgid "Returns 0 on success, negative error code on failure."
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:632
msgid "dma_buf detach implementation for GEM"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:633
msgid "buffer to detach from"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:634
msgid "attachment to be detached"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:635
msgid ""
"Calls :c:type:`drm_gem_object_funcs.pin <drm_gem_object_funcs>` for device "
"specific handling.  Cleans up :c:type:`dma_buf_attachment` from "
"drm_gem_map_attach(). This can be used as the :c:type:`dma_buf_ops.detach "
"<dma_buf_ops>` callback."
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:658
msgid "map_dma_buf implementation for GEM"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:659
msgid "attachment whose scatterlist is to be returned"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:661
#: drivers/gpu/drm/drm_prime.c:703
msgid "``enum dma_data_direction dir``"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:660
#: drivers/gpu/drm/drm_prime.c:702
msgid "direction of DMA transfer"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:661
msgid ""
"Calls :c:type:`drm_gem_object_funcs.get_sg_table <drm_gem_object_funcs>` and "
"then maps the scatterlist. This can be used as the :c:type:`dma_buf_ops."
"map_dma_buf <dma_buf_ops>` callback. Should be used together with "
"drm_gem_unmap_dma_buf()."
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:666
msgid ""
"sg_table containing the scatterlist to be returned; returns ERR_PTR on "
"error. May return -EINTR if it is interrupted by a signal."
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:699
msgid "unmap_dma_buf implementation for GEM"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:700
msgid "attachment to unmap buffer from"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:701
msgid "scatterlist info of the buffer to unmap"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:703
msgid ""
"This can be used as the :c:type:`dma_buf_ops.unmap_dma_buf <dma_buf_ops>` "
"callback."
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:720
msgid "dma_buf vmap implementation for GEM"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:721
#: drivers/gpu/drm/drm_prime.c:814
msgid "buffer to be mapped"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:722
#: drivers/gpu/drm/drm_prime.c:741
msgid "the virtual address of the buffer"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:723
msgid ""
"Sets up a kernel virtual mapping. This can be used as the :c:type:"
"`dma_buf_ops.vmap <dma_buf_ops>` callback. Calls into :c:type:"
"`drm_gem_object_funcs.vmap <drm_gem_object_funcs>` for device specific "
"handling. The kernel virtual address is returned in map."
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:727
msgid "Returns 0 on success or a negative errno code otherwise."
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:739
msgid "dma_buf vunmap implementation for GEM"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:740
msgid "buffer to be unmapped"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:742
msgid ""
"Releases a kernel virtual mapping. This can be used as the :c:type:"
"`dma_buf_ops.vunmap <dma_buf_ops>` callback. Calls into :c:type:"
"`drm_gem_object_funcs.vunmap <drm_gem_object_funcs>` for device specific "
"handling."
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:755
msgid "PRIME mmap function for GEM drivers"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:757
msgid "Virtual address range"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:758
msgid ""
"This function sets up a userspace mapping for PRIME exported buffers using "
"the same codepath that is used for regular GEM buffer mapping on the DRM fd. "
"The fake GEM offset is added to vma->vm_pgoff and :c:type:`drm_driver->fops "
"<drm_driver>`->mmap is called to set up the mapping."
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:813
msgid "dma_buf mmap implementation for GEM"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:815
msgid "virtual address range"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:816
msgid ""
"Provides memory mapping for the buffer. This can be used as the :c:type:"
"`dma_buf_ops.mmap <dma_buf_ops>` callback. It just forwards to "
"drm_gem_prime_mmap()."
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:842
msgid "converts a page array into an sg list"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:844
msgid "pointer to the array of page pointers to convert"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:846
msgid "``unsigned int nr_pages``"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:845
msgid "length of the page vector"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:846
msgid ""
"This helper creates an sg table object from a set of pages the driver is "
"responsible for mapping the pages into the importers address space for use "
"with dma_buf itself."
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:850
msgid ""
"This is useful for implementing :c:type:`drm_gem_object_funcs.get_sg_table "
"<drm_gem_object_funcs>`."
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:880
msgid "returns the contiguous size of the buffer"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:881
msgid "sg_table describing the buffer to check"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:882
msgid ""
"This helper calculates the contiguous size in the DMA address space of the "
"buffer described by the provided sg_table."
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:885
msgid ""
"This is useful for implementing :c:type:`drm_gem_object_funcs."
"gem_prime_import_sg_table <drm_gem_object_funcs>`."
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:911
msgid "helper library implementation of the export callback"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:912
msgid "GEM object to export"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:914
msgid "``int flags``"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:913
msgid "flags like DRM_CLOEXEC and DRM_RDWR"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:914
msgid ""
"This is the implementation of the :c:type:`drm_gem_object_funcs.export "
"<drm_gem_object_funcs>` functions for GEM drivers using the PRIME helpers. "
"It is used as the default in drm_gem_prime_handle_to_fd()."
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:939
msgid ""
"checks if the DMA-BUF was exported from a GEM object belonging to **dev**."
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:941
msgid "drm_device to check against"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:944
msgid ""
"true if the DMA-BUF was exported from a GEM object belonging to **dev**, "
"false otherwise."
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:958
msgid "core implementation of the import callback"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:962
msgid "``struct device *attach_dev``"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:961
msgid "struct device to dma_buf attach"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:962
msgid ""
"This is the core of drm_gem_prime_import(). It's designed to be called by "
"drivers who want to use a different device structure than :c:type:"
"`drm_device.dev <drm_device>` for attaching via dma_buf. This function "
"calls :c:type:`drm_driver.gem_prime_import_sg_table <drm_driver>` internally."
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:967
#: drivers/gpu/drm/drm_prime.c:1035
msgid ""
"Drivers must arrange to call drm_prime_gem_destroy() from their :c:type:"
"`drm_gem_object_funcs.free <drm_gem_object_funcs>` hook when using this "
"function."
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:1027
msgid "helper library implementation of the import callback"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:1030
msgid ""
"This is the implementation of the gem_prime_import functions for GEM drivers "
"using the PRIME helpers. Drivers can use this as their :c:type:`drm_driver."
"gem_prime_import <drm_driver>` implementation. It is used as the default "
"implementation in drm_gem_prime_fd_to_handle()."
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:1047
msgid "convert an sg table into a page array"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:1048
#: drivers/gpu/drm/drm_prime.c:1076
msgid "scatter-gather table to convert"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:1049
msgid "array of page pointers to store the pages in"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:1051
#: drivers/gpu/drm/drm_prime.c:1079
msgid "``int max_entries``"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:1050
msgid "size of the passed-in array"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:1051
msgid "Exports an sg table into an array of pages."
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:1053
msgid ""
"This function is deprecated and strongly discouraged to be used. The page "
"array is only useful for page faults and those can corrupt fields in the "
"struct page if they are not handled by the exporting driver."
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:1075
msgid "convert an sg table into a dma addr array"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:1078
msgid "``dma_addr_t *addrs``"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:1077
msgid "array to store the dma bus address of each page"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:1078
msgid "size of both the passed-in arrays"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:1079
msgid "Exports an sg table into an array of addresses."
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:1081
msgid ""
"Drivers should use this in their :c:type:`drm_driver."
"gem_prime_import_sg_table <drm_driver>` implementation."
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:1101
msgid "helper to clean up a PRIME-imported GEM object"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:1102
msgid "GEM object which was created from a dma-buf"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:1104
msgid "``struct sg_table *sg``"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:1103
msgid "the sg-table which was pinned at import time"
msgstr ""

#: ../../../gpu/drm-mm:442: drivers/gpu/drm/drm_prime.c:1104
msgid ""
"This is the cleanup functions which GEM drivers need to call when they use "
"drm_gem_prime_import() or drm_gem_prime_import_dev() to import dma-bufs."
msgstr ""

#: ../../../gpu/drm-mm.rst:446
msgid "DRM MM Range Allocator"
msgstr ""

#: ../../../gpu/drm-mm.rst:449 ../../../gpu/drm-mm.rst:475
#: ../../../gpu/drm-mm.rst:554
msgid "Overview"
msgstr ""

#: ../../../gpu/drm-mm:451: drivers/gpu/drm/drm_mm.c:54
msgid ""
"drm_mm provides a simple range allocator. The drivers are free to use the "
"resource allocator from the linux core if it suits them, the upside of "
"drm_mm is that it's in the DRM core. Which means that it's easier to extend "
"for some of the crazier special purpose needs of gpus."
msgstr ""

#: ../../../gpu/drm-mm:451: drivers/gpu/drm/drm_mm.c:59
msgid ""
"The main data struct is :c:type:`drm_mm`, allocations are tracked in :c:type:"
"`drm_mm_node`. Drivers are free to embed either of them into their own "
"suitable datastructures. drm_mm itself will not do any memory allocations of "
"its own, so if drivers choose not to embed nodes they need to still allocate "
"them themselves."
msgstr ""

#: ../../../gpu/drm-mm:451: drivers/gpu/drm/drm_mm.c:65
msgid ""
"The range allocator also supports reservation of preallocated blocks. This "
"is useful for taking over initial mode setting configurations from the "
"firmware, where an object needs to be created which exactly matches the "
"firmware's scanout target. As long as the range is still free it can be "
"inserted anytime after the allocator is initialized, which helps with "
"avoiding looped dependencies in the driver load sequence."
msgstr ""

#: ../../../gpu/drm-mm:451: drivers/gpu/drm/drm_mm.c:72
msgid ""
"drm_mm maintains a stack of most recently freed holes, which of all "
"simplistic datastructures seems to be a fairly decent approach to clustering "
"allocations and avoiding too much fragmentation. This means free space "
"searches are O(num_holes). Given that all the fancy features drm_mm supports "
"something better would be fairly complex and since gfx thrashing is a fairly "
"steep cliff not a real concern. Removing a node again is O(1)."
msgstr ""

#: ../../../gpu/drm-mm:451: drivers/gpu/drm/drm_mm.c:79
msgid ""
"drm_mm supports a few features: Alignment and range restrictions can be "
"supplied. Furthermore every :c:type:`drm_mm_node` has a color value (which "
"is just an opaque unsigned long) which in conjunction with a driver callback "
"can be used to implement sophisticated placement restrictions. The i915 DRM "
"driver uses this to implement guard pages between incompatible caching "
"domains in the graphics TT."
msgstr ""

#: ../../../gpu/drm-mm:451: drivers/gpu/drm/drm_mm.c:86
msgid ""
"Two behaviors are supported for searching and allocating: bottom-up and top-"
"down. The default is bottom-up. Top-down allocation can be used if the "
"memory area has different restrictions, or just to reduce fragmentation."
msgstr ""

#: ../../../gpu/drm-mm:451: drivers/gpu/drm/drm_mm.c:90
msgid ""
"Finally iteration helpers to walk all nodes and all holes are provided as "
"are some basic allocator dumpers for debugging."
msgstr ""

#: ../../../gpu/drm-mm:451: drivers/gpu/drm/drm_mm.c:93
msgid ""
"Note that this range allocator is not thread-safe, drivers need to protect "
"modifications with their own locking. The idea behind this is that for a "
"full memory manager additional data needs to be protected anyway, hence "
"internal locking would be fully redundant."
msgstr ""

#: ../../../gpu/drm-mm.rst:455
msgid "LRU Scan/Eviction Support"
msgstr ""

#: ../../../gpu/drm-mm:457: drivers/gpu/drm/drm_mm.c:652
msgid ""
"Very often GPUs need to have continuous allocations for a given object. When "
"evicting objects to make space for a new one it is therefore not most "
"efficient when we simply start to select all objects from the tail of an LRU "
"until there's a suitable hole: Especially for big objects or nodes that "
"otherwise have special allocation constraints there's a good chance we evict "
"lots of (smaller) objects unnecessarily."
msgstr ""

#: ../../../gpu/drm-mm:457: drivers/gpu/drm/drm_mm.c:659
msgid ""
"The DRM range allocator supports this use-case through the scanning "
"interfaces. First a scan operation needs to be initialized with "
"drm_mm_scan_init() or drm_mm_scan_init_with_range(). The driver adds objects "
"to the roster, probably by walking an LRU list, but this can be freely "
"implemented. Eviction candidates are added using drm_mm_scan_add_block() "
"until a suitable hole is found or there are no further evictable objects. "
"Eviction roster metadata is tracked in :c:type:`struct drm_mm_scan "
"<drm_mm_scan>`."
msgstr ""

#: ../../../gpu/drm-mm:457: drivers/gpu/drm/drm_mm.c:668
msgid ""
"The driver must walk through all objects again in exactly the reverse order "
"to restore the allocator state. Note that while the allocator is used in the "
"scan mode no other operation is allowed."
msgstr ""

#: ../../../gpu/drm-mm:457: drivers/gpu/drm/drm_mm.c:672
msgid ""
"Finally the driver evicts all objects selected (drm_mm_scan_remove_block() "
"reported true) in the scan, and any overlapping nodes after color adjustment "
"(drm_mm_scan_color_evict()). Adding and removing an object is O(1), and "
"since freeing a node is also O(1) the overall complexity is "
"O(scanned_objects). So like the free stack which needs to be walked before a "
"scan operation even begins this is linear in the number of objects. It "
"doesn't seem to hurt too badly."
msgstr ""

#: ../../../gpu/drm-mm.rst:461
msgid "DRM MM Range Allocator Function References"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:60
msgid "control search and allocation behaviour"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:66
msgid "``DRM_MM_INSERT_BEST``"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:67
msgid ""
"Search for the smallest hole (within the search range) that fits the desired "
"node."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:70 include/drm/drm_mm.h:76
#: include/drm/drm_mm.h:92
msgid "Allocates the node from the bottom of the found hole."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:72
msgid "``DRM_MM_INSERT_LOW``"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:73
msgid ""
"Search for the lowest hole (address closest to 0, within the search range) "
"that fits the desired node."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:78
msgid "``DRM_MM_INSERT_HIGH``"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:79
msgid ""
"Search for the highest hole (address closest to U64_MAX, within the search "
"range) that fits the desired node."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:82
msgid ""
"Allocates the node from the *top* of the found hole. The specified alignment "
"for the node is applied to the base of the node (:c:type:`drm_mm_node.start "
"<drm_mm_node>`)."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:86
msgid "``DRM_MM_INSERT_EVICT``"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:87
msgid ""
"Search for the most recently evicted hole (within the search range) that "
"fits the desired node. This is appropriate for use immediately after "
"performing an eviction scan (see drm_mm_scan_init()) and removing the "
"selected nodes to form a hole."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:94
msgid "``DRM_MM_INSERT_ONCE``"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:95
msgid ""
"Only check the first hole for suitablity and report -ENOSPC immediately "
"otherwise, rather than check every hole until a suitable one is found. Can "
"only be used in conjunction with another search method such as "
"DRM_MM_INSERT_HIGH or DRM_MM_INSERT_LOW."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:100
msgid "``DRM_MM_INSERT_HIGHEST``"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:101
msgid ""
"Only check the highest hole (the hole with the largest address) and insert "
"the node at the top of the hole or report -ENOSPC if unsuitable."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:105 include/drm/drm_mm.h:112
msgid "Does not search all holes."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:107
msgid "``DRM_MM_INSERT_LOWEST``"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:108
msgid ""
"Only check the lowest hole (the hole with the smallest address) and insert "
"the node at the bottom of the hole or report -ENOSPC if unsuitable."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:61
msgid ""
"The :c:type:`struct drm_mm <drm_mm>` range manager supports finding a "
"suitable modes using a number of search trees. These trees are oranised by "
"size, by address and in most recent eviction order. This allows the user to "
"find either the smallest hole to reuse, the lowest or highest address to "
"reuse, or simply reuse the most recent eviction that fits. When allocating "
"the :c:type:`drm_mm_node` from within the hole, the :c:type:"
"`drm_mm_insert_mode` also dictate whether to allocate the lowest matching "
"address or the highest."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:149
msgid "allocated block in the DRM allocator"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:158
msgid "``color``"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:159
msgid "Opaque driver-private tag."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:161
msgid "Start address of the allocated block."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:163
msgid "Size of the allocated block."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:150
msgid ""
"This represents an allocated block in a :c:type:`drm_mm` allocator. Except "
"for pre-reserved nodes inserted using drm_mm_reserve_node() the structure is "
"entirely opaque and should only be accessed through the provided funcions. "
"Since allocation of these nodes is entirely handled by the driver they can "
"be embedded."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:183
msgid "DRM allocator"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:192
msgid "``color_adjust``"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:193
msgid ""
"Optional driver callback to further apply restrictions on a hole. The node "
"argument points at the node containing the hole from which the block would "
"be allocated (see drm_mm_hole_follows() and friends). The other arguments "
"are the size of the block to be allocated. The driver can adjust the start "
"and end as needed to e.g. insert guard pages."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:184
msgid ""
"DRM range allocator with a few special functions and features geared towards "
"managing GPU memory. Except for the **color_adjust** callback the structure "
"is entirely opaque and should only be accessed through the provided "
"functions and macros. This structure can be embedded into larger driver "
"structures."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:219
msgid "DRM allocator eviction roaster data"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:220
msgid ""
"This structure tracks data needed for the eviction roaster set up using "
"drm_mm_scan_init(), and used with drm_mm_scan_add_block() and "
"drm_mm_scan_remove_block(). The structure is entirely opaque and should only "
"be accessed through the provided functions and macros. It is meant to be "
"allocated temporarily by the driver on the stack."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:246
msgid "checks whether a node is allocated"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:252 include/drm/drm_mm.h:288
msgid "``const struct drm_mm_node *node``"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:247 include/drm/drm_mm.h:283
msgid "drm_mm_node to check"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:248
msgid ""
"Drivers are required to clear a node prior to using it with the drm_mm range "
"manager."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:251 include/drm/drm_mm.h:269
msgid ""
"Drivers should use this helper for proper encapsulation of drm_mm internals."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:255
msgid "True if the **node** is allocated."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:264
msgid "checks whether an allocator is initialized"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:270 include/drm/drm_mm.h:476
#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:990
msgid "``const struct drm_mm *mm``"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:265
msgid "drm_mm to check"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:266
msgid ""
"Drivers should clear the struct drm_mm prior to initialisation if they want "
"to use this function."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:273
msgid "True if the **mm** is initialized."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:282
msgid "checks whether a hole follows this node"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:284
msgid ""
"Holes are embedded into the drm_mm using the tail of a drm_mm_node. If you "
"wish to know whether a hole follows this particular node, query this "
"function. See also drm_mm_hole_node_start() and drm_mm_hole_node_end()."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:290
msgid "True if a hole follows the **node**."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:304
msgid "computes the start of the hole following **node**"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:310 include/drm/drm_mm.h:332
msgid "``const struct drm_mm_node *hole_node``"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:305 include/drm/drm_mm.h:327
msgid "drm_mm_node which implicitly tracks the following hole"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:306
msgid ""
"This is useful for driver-specific debug dumpers. Otherwise drivers should "
"not inspect holes themselves. Drivers must check first whether a hole indeed "
"follows by looking at drm_mm_hole_follows()"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:311
msgid "Start of the subsequent hole."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:326
msgid "computes the end of the hole following **node**"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:328
msgid ""
"This is useful for driver-specific debug dumpers. Otherwise drivers should "
"not inspect holes themselves. Drivers must check first whether a hole indeed "
"follows by looking at drm_mm_hole_follows()."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:333
msgid "End of the subsequent hole."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:344
msgid "``drm_mm_nodes (mm)``"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:342
msgid "list of nodes under the drm_mm range manager"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:348 include/drm/drm_mm.h:359
#: include/drm/drm_mm.h:371 include/drm/drm_mm.h:382
msgid "``mm``"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:343
msgid "the struct drm_mm range manager"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:344
msgid ""
"As the drm_mm range manager hides its node_list deep with its structure, "
"extracting it looks painful and repetitive. This is not expected to be used "
"outside of the drm_mm_for_each_node() macros and similar internal functions."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:350
msgid "The node list, may be empty."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:358
msgid "``drm_mm_for_each_node (entry, mm)``"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:356 include/drm/drm_mm.h:367
msgid "iterator to walk over all allocated nodes"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:362 include/drm/drm_mm.h:373
#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:948
msgid "``entry``"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:357 include/drm/drm_mm.h:368
msgid ""
":c:type:`struct drm_mm_node <drm_mm_node>` to assign to in each iteration "
"step"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:358 include/drm/drm_mm.h:370
#: include/drm/drm_mm.h:381
msgid ":c:type:`drm_mm` allocator to walk"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:359
msgid ""
"This iterator walks over all nodes in the range allocator. It is implemented "
"with list_for_each(), so not save against removal of elements."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:369
msgid "``drm_mm_for_each_node_safe (entry, next, mm)``"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:370 ../../../gpu/drm-mm:503:
#: include/drm/drm_gpuvm.h:916 include/drm/drm_gpuvm.h:1006
msgid "``next``"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:369
msgid ":c:type:`struct drm_mm_node <drm_mm_node>` to store the next step"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:371
msgid ""
"This iterator walks over all nodes in the range allocator. It is implemented "
"with list_for_each_safe(), so save against removal of elements."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:381
msgid "``drm_mm_for_each_hole (pos, mm, hole_start, hole_end)``"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:379
msgid "iterator to walk over all holes"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:380
msgid ":c:type:`drm_mm_node` used internally to track progress"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:383
msgid "``hole_start``"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:382
msgid "ulong variable to assign the hole start to on each iteration"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:384
msgid "``hole_end``"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:383
msgid "ulong variable to assign the hole end to on each iteration"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:384
msgid ""
"This iterator walks over all holes in the range allocator. It is implemented "
"with list_for_each(), so not save against removal of elements. **entry** is "
"used internally and will not reflect a real drm_mm_node for the very first "
"hole. Hence users of this iterator may not access it."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:389
msgid ""
"Implementation Note: We need to inline list_for_each_entry in order to be "
"able to set hole_start and hole_end on each iteration while keeping the "
"macro sane."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:417 include/drm/drm_mm.h:445
msgid "search for space and insert **node**"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:423 include/drm/drm_mm.h:451
#: include/drm/drm_mm.h:517 ../../../gpu/drm-mm:466:
#: drivers/gpu/drm/drm_mm.c:443 drivers/gpu/drm/drm_mm.c:687
#: drivers/gpu/drm/drm_mm.c:927 drivers/gpu/drm/drm_mm.c:962
msgid "``struct drm_mm *mm``"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:418 include/drm/drm_mm.h:446
#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:500
msgid "drm_mm to allocate from"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:420 include/drm/drm_mm.h:448
#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:440
#: drivers/gpu/drm/drm_mm.c:626 drivers/gpu/drm/drm_mm.c:738
#: drivers/gpu/drm/drm_mm.c:821
msgid "``struct drm_mm_node *node``"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:419 include/drm/drm_mm.h:447
#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:501
msgid "preallocate node to insert"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:421 include/drm/drm_mm.h:449
#: include/drm/drm_mm.h:518 ../../../gpu/drm-mm:466:
#: drivers/gpu/drm/drm_mm.c:503 drivers/gpu/drm/drm_mm.c:688
#: drivers/gpu/drm/drm_mm.c:925 ../../../gpu/drm-mm:515:
#: drivers/gpu/drm/drm_buddy.c:230 drivers/gpu/drm/drm_buddy.c:1013
msgid "``u64 size``"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:420 include/drm/drm_mm.h:448
#: include/drm/drm_mm.h:517 ../../../gpu/drm-mm:466:
#: drivers/gpu/drm/drm_mm.c:502 drivers/gpu/drm/drm_mm.c:687
msgid "size of the allocation"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:422 include/drm/drm_mm.h:519
#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:504
#: drivers/gpu/drm/drm_mm.c:689
msgid "``u64 alignment``"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:421 include/drm/drm_mm.h:518
#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:503
#: drivers/gpu/drm/drm_mm.c:688 ../../../gpu/drm-mm:515:
#: drivers/gpu/drm/drm_buddy.c:1013
msgid "alignment of the allocation"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:423 include/drm/drm_mm.h:520
#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:505
#: drivers/gpu/drm/drm_mm.c:690
msgid "``unsigned long color``"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:422 ../../../gpu/drm-mm:466:
#: drivers/gpu/drm/drm_mm.c:504
msgid "opaque tag value to use for this node"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:424 include/drm/drm_mm.h:521
#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:508
#: drivers/gpu/drm/drm_mm.c:693
msgid "``enum drm_mm_insert_mode mode``"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:423 include/drm/drm_mm.h:520
#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:507
#: drivers/gpu/drm/drm_mm.c:692
msgid "fine-tune the allocation search and placement"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:424
msgid ""
"This is a simplified version of drm_mm_insert_node_in_range() with no range "
"restrictions applied."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:427 include/drm/drm_mm.h:452
msgid "The preallocated node must be cleared to 0."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:430 include/drm/drm_mm.h:455
#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:511
msgid "0 on success, -ENOSPC if there's no suitable hole."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:449
msgid ""
"This is a simplified version of drm_mm_insert_node_generic() with **color** "
"set to 0."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:470
msgid "checks whether an allocator is clean"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:471
msgid "drm_mm allocator to check"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:473
msgid ""
"True if the allocator is completely free, false if there's still a node "
"allocated in it."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:488
msgid "``drm_mm_for_each_node_in_range (node__, mm__, start__, end__)``"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:486
msgid "iterator to walk over a range of allocated nodes"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:492
msgid "``node__``"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:488
msgid "drm_mm_node structure to assign to in each iteration step"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:490
msgid "``mm__``"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:489
msgid "drm_mm allocator to walk"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:491 ../../../gpu/drm-mm:503:
#: include/drm/drm_gpuvm.h:441 include/drm/drm_gpuvm.h:464
msgid "``start__``"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:490
msgid "starting offset, the first node will overlap this"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:492 ../../../gpu/drm-mm:503:
#: include/drm/drm_gpuvm.h:442 include/drm/drm_gpuvm.h:465
msgid "``end__``"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:491
msgid "ending offset, the last node will start before this (but may overlap)"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:492
msgid ""
"This iterator walks over all nodes in the range allocator that lie between "
"**start** and **end**. It is implemented similarly to list_for_each(), but "
"using the internal interval tree to accelerate the search for the starting "
"node, and so not safe against removal of elements. It assumes that **end** "
"is within (or is the upper limit of) the drm_mm allocator. If [**start**, "
"**end**] are beyond the range of the drm_mm, the iterator may walk over the "
"special _unallocated_ :c:type:`drm_mm.head_node <drm_mm>`, and may even "
"continue indefinitely."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:514
msgid "initialize lru scanning"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:520 ../../../gpu/drm-mm:466:
#: drivers/gpu/drm/drm_mm.c:690 drivers/gpu/drm/drm_mm.c:741
#: drivers/gpu/drm/drm_mm.c:824 drivers/gpu/drm/drm_mm.c:873
msgid "``struct drm_mm_scan *scan``"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:515 ../../../gpu/drm-mm:466:
#: drivers/gpu/drm/drm_mm.c:685
msgid "scan state"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:516 ../../../gpu/drm-mm:466:
#: drivers/gpu/drm/drm_mm.c:686
msgid "drm_mm to scan"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:519 ../../../gpu/drm-mm:466:
#: drivers/gpu/drm/drm_mm.c:689
msgid "opaque tag value to use for the allocation"
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:521
msgid ""
"This is a simplified version of drm_mm_scan_init_with_range() with no range "
"restrictions applied."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:524 ../../../gpu/drm-mm:466:
#: drivers/gpu/drm/drm_mm.c:693
msgid ""
"This simply sets up the scanning routines with the parameters for the "
"desired hole."
msgstr ""

#: ../../../gpu/drm-mm:463: include/drm/drm_mm.h:527 ../../../gpu/drm-mm:466:
#: drivers/gpu/drm/drm_mm.c:696
msgid ""
"Warning: As long as the scan list is non-empty, no other operations than "
"adding/removing nodes to/from the scan list are allowed."
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:437
msgid "insert an pre-initialized node"
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:438
msgid "drm_mm allocator to insert **node** into"
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:439
msgid "drm_mm_node to insert"
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:440
msgid ""
"This functions inserts an already set-up :c:type:`drm_mm_node` into the "
"allocator, meaning that start, size and color must be set by the caller. All "
"other fields must be cleared to 0. This is useful to initialize the "
"allocator with preallocated objects which must be set-up before the range "
"allocator can be set-up, e.g. when taking over a firmware framebuffer."
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:447
msgid "0 on success, -ENOSPC if there's no hole where **node** is."
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:499
msgid "ranged search for space and insert **node**"
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:505
msgid "``struct drm_mm * const mm``"
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:502
msgid "``struct drm_mm_node * const node``"
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:506
msgid "``u64 range_start``"
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:505
msgid "start of the allowed range for this node"
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:507
msgid "``u64 range_end``"
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:506
msgid "end of the allowed range for this node"
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:508
msgid "The preallocated **node** must be cleared to 0."
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:620
msgid "Remove a memory node from the allocator."
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:621
#: drivers/gpu/drm/drm_mm.c:820
msgid "drm_mm_node to remove"
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:622
msgid ""
"This just removes a node from its drm_mm allocator. The node does not need "
"to be cleared again before it can be re-inserted into this or any other "
"drm_mm allocator. It is a bug to call this function on a unallocated node."
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:684
msgid "initialize range-restricted lru scanning"
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:691
#: drivers/gpu/drm/drm_mm.c:924 ../../../gpu/drm-mm:506:
#: drivers/gpu/drm/drm_gpuvm.c:1934 ../../../gpu/drm-mm:515:
#: drivers/gpu/drm/drm_buddy.c:1011
msgid "``u64 start``"
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:690
msgid "start of the allowed range for the allocation"
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:692
#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1956
#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:1012
msgid "``u64 end``"
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:691
msgid "end of the allowed range for the allocation"
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:735
msgid "add a node to the scan list"
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:736
#: drivers/gpu/drm/drm_mm.c:819
msgid "the active drm_mm scanner"
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:737
msgid "drm_mm_node to add"
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:738
msgid ""
"Add a node to the scan list that might be freed to make space for the "
"desired hole."
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:742
msgid "True if a hole has been found, false otherwise."
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:818
msgid "remove a node from the scan list"
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:821
msgid ""
"Nodes **must** be removed in exactly the reverse order from the scan list as "
"they have been added (e.g. using list_add() as they are added and then "
"list_for_each() over that eviction list to remove), otherwise the internal "
"state of the memory manager will be corrupted."
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:826
msgid ""
"When the scan list is empty, the selected memory nodes can be freed. An "
"immediately following drm_mm_insert_node_in_range_generic() or one of the "
"simpler versions of that function with !DRM_MM_SEARCH_BEST will then return "
"the just freed block (because it's at the top of the free_stack list)."
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:832
msgid ""
"True if this block should be evicted, false otherwise. Will always return "
"false when no hole has been found."
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:867
msgid "evict overlapping nodes on either side of hole"
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:868
msgid "drm_mm scan with target hole"
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:869
msgid ""
"After completing an eviction scan and removing the selected nodes, we may "
"need to remove a few more nodes from either side of the target hole if mm."
"color_adjust is being used."
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:874
msgid "A node to evict, or NULL if there are no overlapping nodes."
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:921
msgid "initialize a drm-mm allocator"
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:922
msgid "the drm_mm structure to initialize"
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:923
msgid "start of the range managed by **mm**"
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:924
msgid "end of the range managed by **mm**"
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:925
msgid "Note that **mm** must be cleared to 0 before calling this function."
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:956
msgid "clean up a drm_mm allocator"
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:957
msgid "drm_mm allocator to clean up"
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:958
msgid ""
"Note that it is a bug to call this function on an allocator which is not "
"clean."
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:984
#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:1191
msgid "print allocator state"
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:985
msgid "drm_mm allocator to print"
msgstr ""

#: ../../../gpu/drm-mm:466: drivers/gpu/drm/drm_mm.c:986
#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:1177
#: drivers/gpu/drm/drm_buddy.c:1194
msgid "DRM printer to use"
msgstr ""

#: ../../../gpu/drm-mm.rst:472
msgid "DRM GPUVM"
msgstr ""

#: ../../../gpu/drm-mm:477: drivers/gpu/drm/drm_gpuvm.c:35
msgid ""
"The DRM GPU VA Manager, represented by struct drm_gpuvm keeps track of a "
"GPU's virtual address (VA) space and manages the corresponding virtual "
"mappings represented by :c:type:`drm_gpuva` objects. It also keeps track of "
"the mapping's backing :c:type:`drm_gem_object` buffers."
msgstr ""

#: ../../../gpu/drm-mm:477: drivers/gpu/drm/drm_gpuvm.c:40
msgid ""
":c:type:`drm_gem_object` buffers maintain a list of :c:type:`drm_gpuva` "
"objects representing all existing GPU VA mappings using this :c:type:"
"`drm_gem_object` as backing buffer."
msgstr ""

#: ../../../gpu/drm-mm:477: drivers/gpu/drm/drm_gpuvm.c:43
msgid ""
"GPU VAs can be flagged as sparse, such that drivers may use GPU VAs to also "
"keep track of sparse PTEs in order to support Vulkan 'Sparse Resources'."
msgstr ""

#: ../../../gpu/drm-mm:477: drivers/gpu/drm/drm_gpuvm.c:46
msgid ""
"The GPU VA manager internally uses a rb-tree to manage the :c:type:"
"`drm_gpuva` mappings within a GPU's virtual address space."
msgstr ""

#: ../../../gpu/drm-mm:477: drivers/gpu/drm/drm_gpuvm.c:49
msgid ""
"The :c:type:`drm_gpuvm` structure contains a special :c:type:`drm_gpuva` "
"representing the portion of VA space reserved by the kernel. This node is "
"initialized together with the GPU VA manager instance and removed when the "
"GPU VA manager is destroyed."
msgstr ""

#: ../../../gpu/drm-mm:477: drivers/gpu/drm/drm_gpuvm.c:54
msgid ""
"In a typical application drivers would embed struct drm_gpuvm and struct "
"drm_gpuva within their own driver specific structures, there won't be any "
"memory allocations of its own nor memory allocations of :c:type:`drm_gpuva` "
"entries."
msgstr ""

#: ../../../gpu/drm-mm:477: drivers/gpu/drm/drm_gpuvm.c:59
msgid ""
"The data structures needed to store :c:type:`drm_gpuvas` within the :c:type:"
"`drm_gpuvm` are contained within struct drm_gpuva already. Hence, for "
"inserting :c:type:`drm_gpuva` entries from within dma-fence signalling "
"critical sections it is enough to pre-allocate the :c:type:`drm_gpuva` "
"structures."
msgstr ""

#: ../../../gpu/drm-mm:477: drivers/gpu/drm/drm_gpuvm.c:64
msgid ""
":c:type:`drm_gem_objects` which are private to a single VM can share a "
"common :c:type:`dma_resv` in order to improve locking efficiency (e.g. with :"
"c:type:`drm_exec`). For this purpose drivers must pass a :c:type:"
"`drm_gem_object` to drm_gpuvm_init(), in the following called 'resv object', "
"which serves as the container of the GPUVM's shared :c:type:`dma_resv`. This "
"resv object can be a driver specific :c:type:`drm_gem_object`, such as the :"
"c:type:`drm_gem_object` containing the root page table, but it can also be a "
"'dummy' object, which can be allocated with drm_gpuvm_resv_object_alloc()."
msgstr ""

#: ../../../gpu/drm-mm:477: drivers/gpu/drm/drm_gpuvm.c:73
msgid ""
"In order to connect a struct drm_gpuva to its backing :c:type:"
"`drm_gem_object` each :c:type:`drm_gem_object` maintains a list of :c:type:"
"`drm_gpuvm_bo` structures, and each :c:type:`drm_gpuvm_bo` contains a list "
"of :c:type:`drm_gpuva` structures."
msgstr ""

#: ../../../gpu/drm-mm:477: drivers/gpu/drm/drm_gpuvm.c:77
msgid ""
"A :c:type:`drm_gpuvm_bo` is an abstraction that represents a combination of "
"a :c:type:`drm_gpuvm` and a :c:type:`drm_gem_object`. Every such combination "
"should be unique. This is ensured by the API through drm_gpuvm_bo_obtain() "
"and drm_gpuvm_bo_obtain_prealloc() which first look into the corresponding :"
"c:type:`drm_gem_object` list of :c:type:`drm_gpuvm_bos` for an existing "
"instance of this particular combination. If not present, a new instance is "
"created and linked to the :c:type:`drm_gem_object`."
msgstr ""

#: ../../../gpu/drm-mm:477: drivers/gpu/drm/drm_gpuvm.c:85
msgid ""
":c:type:`drm_gpuvm_bo` structures, since unique for a given :c:type:"
"`drm_gpuvm`, are also used as entry for the :c:type:`drm_gpuvm`'s lists of "
"external and evicted objects. Those lists are maintained in order to "
"accelerate locking of dma-resv locks and validation of evicted objects bound "
"in a :c:type:`drm_gpuvm`. For instance, all :c:type:`drm_gem_object`'s :c:"
"type:`dma_resv` of a given :c:type:`drm_gpuvm` can be locked by calling "
"drm_gpuvm_exec_lock(). Once locked drivers can call drm_gpuvm_validate() in "
"order to validate all evicted :c:type:`drm_gem_objects`. It is also possible "
"to lock additional :c:type:`drm_gem_objects` by providing the corresponding "
"parameters to drm_gpuvm_exec_lock() as well as open code the :c:type:"
"`drm_exec` loop while making use of helper functions such as "
"drm_gpuvm_prepare_range() or drm_gpuvm_prepare_objects()."
msgstr ""

#: ../../../gpu/drm-mm:477: drivers/gpu/drm/drm_gpuvm.c:97
msgid ""
"Every bound :c:type:`drm_gem_object` is treated as external object when its :"
"c:type:`dma_resv` structure is different than the :c:type:`drm_gpuvm`'s "
"common :c:type:`dma_resv` structure."
msgstr ""

#: ../../../gpu/drm-mm.rst:481
msgid "Split and Merge"
msgstr ""

#: ../../../gpu/drm-mm:483: drivers/gpu/drm/drm_gpuvm.c:104
msgid ""
"Besides its capability to manage and represent a GPU VA space, the GPU VA "
"manager also provides functions to let the :c:type:`drm_gpuvm` calculate a "
"sequence of operations to satisfy a given map or unmap request."
msgstr ""

#: ../../../gpu/drm-mm:483: drivers/gpu/drm/drm_gpuvm.c:108
msgid ""
"Therefore the DRM GPU VA manager provides an algorithm implementing "
"splitting and merging of existing GPU VA mappings with the ones that are "
"requested to be mapped or unmapped. This feature is required by the Vulkan "
"API to implement Vulkan 'Sparse Memory Bindings' - drivers UAPIs often refer "
"to this as VM BIND."
msgstr ""

#: ../../../gpu/drm-mm:483: drivers/gpu/drm/drm_gpuvm.c:114
msgid ""
"Drivers can call drm_gpuvm_sm_map() to receive a sequence of callbacks "
"containing map, unmap and remap operations for a given newly requested "
"mapping. The sequence of callbacks represents the set of operations to "
"execute in order to integrate the new mapping cleanly into the current state "
"of the GPU VA space."
msgstr ""

#: ../../../gpu/drm-mm:483: drivers/gpu/drm/drm_gpuvm.c:120
msgid ""
"Depending on how the new GPU VA mapping intersects with the existing "
"mappings of the GPU VA space the :c:type:`drm_gpuvm_ops` callbacks contain "
"an arbitrary amount of unmap operations, a maximum of two remap operations "
"and a single map operation. The caller might receive no callback at all if "
"no operation is required, e.g. if the requested mapping already exists in "
"the exact same way."
msgstr ""

#: ../../../gpu/drm-mm:483: drivers/gpu/drm/drm_gpuvm.c:126
msgid ""
"The single map operation represents the original map operation requested by "
"the caller."
msgstr ""

#: ../../../gpu/drm-mm:483: drivers/gpu/drm/drm_gpuvm.c:129
msgid ""
":c:type:`drm_gpuva_op_unmap` contains a 'keep' field, which indicates "
"whether the :c:type:`drm_gpuva` to unmap is physically contiguous with the "
"original mapping request. Optionally, if 'keep' is set, drivers may keep the "
"actual page table entries for this :c:type:`drm_gpuva`, adding the missing "
"page table entries only and update the :c:type:`drm_gpuvm`'s view of things "
"accordingly."
msgstr ""

#: ../../../gpu/drm-mm:483: drivers/gpu/drm/drm_gpuvm.c:135
msgid ""
"Drivers may do the same optimization, namely delta page table updates, also "
"for remap operations. This is possible since :c:type:`drm_gpuva_op_remap` "
"consists of one unmap operation and one or two map operations, such that "
"drivers can derive the page table update delta accordingly."
msgstr ""

#: ../../../gpu/drm-mm:483: drivers/gpu/drm/drm_gpuvm.c:140
msgid ""
"Note that there can't be more than two existing mappings to split up, one at "
"the beginning and one at the end of the new mapping, hence there is a "
"maximum of two remap operations."
msgstr ""

#: ../../../gpu/drm-mm:483: drivers/gpu/drm/drm_gpuvm.c:144
msgid ""
"Analogous to drm_gpuvm_sm_map() drm_gpuvm_sm_unmap() uses :c:type:"
"`drm_gpuvm_ops` to call back into the driver in order to unmap a range of "
"GPU VA space. The logic behind this function is way simpler though: For all "
"existing mappings enclosed by the given range unmap operations are created. "
"For mappings which are only partially located within the given range, remap "
"operations are created such that those mappings are split up and re-mapped "
"partially."
msgstr ""

#: ../../../gpu/drm-mm:483: drivers/gpu/drm/drm_gpuvm.c:151
msgid ""
"As an alternative to drm_gpuvm_sm_map() and drm_gpuvm_sm_unmap(), "
"drm_gpuvm_sm_map_ops_create() and drm_gpuvm_sm_unmap_ops_create() can be "
"used to directly obtain an instance of struct drm_gpuva_ops containing a "
"list of :c:type:`drm_gpuva_op`, which can be iterated with "
"drm_gpuva_for_each_op(). This list contains the :c:type:`drm_gpuva_ops` "
"analogous to the callbacks one would receive when calling drm_gpuvm_sm_map() "
"or drm_gpuvm_sm_unmap(). While this way requires more memory (to allocate "
"the :c:type:`drm_gpuva_ops`), it provides drivers a way to iterate the :c:"
"type:`drm_gpuva_op` multiple times, e.g. once in a context where memory "
"allocations are possible (e.g. to allocate GPU page tables) and once in the "
"dma-fence signalling critical path."
msgstr ""

#: ../../../gpu/drm-mm:483: drivers/gpu/drm/drm_gpuvm.c:162
msgid ""
"To update the :c:type:`drm_gpuvm`'s view of the GPU VA space "
"drm_gpuva_insert() and drm_gpuva_remove() may be used. These functions can "
"safely be used from :c:type:`drm_gpuvm_ops` callbacks originating from "
"drm_gpuvm_sm_map() or drm_gpuvm_sm_unmap(). However, it might be more "
"convenient to use the provided helper functions drm_gpuva_map(), "
"drm_gpuva_remap() and drm_gpuva_unmap() instead."
msgstr ""

#: ../../../gpu/drm-mm:483: drivers/gpu/drm/drm_gpuvm.c:169
msgid ""
"The following diagram depicts the basic relationships of existing GPU VA "
"mappings, a newly requested mapping and the resulting mappings as "
"implemented by drm_gpuvm_sm_map() - it doesn't cover any arbitrary "
"combinations of these."
msgstr ""

#: ../../../gpu/drm-mm:483: drivers/gpu/drm/drm_gpuvm.c:173
msgid ""
"Requested mapping is identical. Replace it, but indicate the backing PTEs "
"could be kept."
msgstr ""

#: ../../../gpu/drm-mm:483: drivers/gpu/drm/drm_gpuvm.c:188
msgid ""
"Requested mapping is identical, except for the BO offset, hence replace the "
"mapping."
msgstr ""

#: ../../../gpu/drm-mm:483: drivers/gpu/drm/drm_gpuvm.c:203
msgid ""
"Requested mapping is identical, except for the backing BO, hence replace the "
"mapping."
msgstr ""

#: ../../../gpu/drm-mm:483: drivers/gpu/drm/drm_gpuvm.c:218
msgid ""
"Existent mapping is a left aligned subset of the requested one, hence "
"replace the existing one."
msgstr ""

#: ../../../gpu/drm-mm:483: drivers/gpu/drm/drm_gpuvm.c:233
#: drivers/gpu/drm/drm_gpuvm.c:253
msgid ""
"We expect to see the same result for a request with a different BO and/or "
"non-contiguous BO offset."
msgstr ""

#: ../../../gpu/drm-mm:483: drivers/gpu/drm/drm_gpuvm.c:237
msgid ""
"Requested mapping's range is a left aligned subset of the existing one, but "
"backed by a different BO. Hence, map the requested mapping and split the "
"existing one adjusting its BO offset."
msgstr ""

#: ../../../gpu/drm-mm:483: drivers/gpu/drm/drm_gpuvm.c:257
#: drivers/gpu/drm/drm_gpuvm.c:288
msgid ""
"Existent mapping is a superset of the requested mapping. Split it up, but "
"indicate that the backing PTEs could be kept."
msgstr ""

#: ../../../gpu/drm-mm:483: drivers/gpu/drm/drm_gpuvm.c:272
msgid ""
"Requested mapping's range is a right aligned subset of the existing one, but "
"backed by a different BO. Hence, map the requested mapping and split the "
"existing one, without adjusting the BO offset."
msgstr ""

#: ../../../gpu/drm-mm:483: drivers/gpu/drm/drm_gpuvm.c:303
msgid ""
"Existent mapping is overlapped at the end by the requested mapping backed by "
"a different BO. Hence, map the requested mapping and split up the existing "
"one, without adjusting the BO offset."
msgstr ""

#: ../../../gpu/drm-mm:483: drivers/gpu/drm/drm_gpuvm.c:319
msgid ""
"Existent mapping is overlapped by the requested mapping, both having the "
"same backing BO with a contiguous offset. Indicate the backing PTEs of the "
"old mapping could be kept."
msgstr ""

#: ../../../gpu/drm-mm:483: drivers/gpu/drm/drm_gpuvm.c:335
msgid ""
"Requested mapping's range is a centered subset of the existing one having a "
"different backing BO. Hence, map the requested mapping and split up the "
"existing one in two mappings, adjusting the BO offset of the right one "
"accordingly."
msgstr ""

#: ../../../gpu/drm-mm:483: drivers/gpu/drm/drm_gpuvm.c:352
msgid ""
"Requested mapping is a contiguous subset of the existing one. Split it up, "
"but indicate that the backing PTEs could be kept."
msgstr ""

#: ../../../gpu/drm-mm:483: drivers/gpu/drm/drm_gpuvm.c:367
msgid ""
"Existent mapping is a right aligned subset of the requested one, hence "
"replace the existing one."
msgstr ""

#: ../../../gpu/drm-mm:483: drivers/gpu/drm/drm_gpuvm.c:382
#: drivers/gpu/drm/drm_gpuvm.c:401
msgid ""
"We expect to see the same result for a request with a different bo and/or "
"non-contiguous bo_offset."
msgstr ""

#: ../../../gpu/drm-mm:483: drivers/gpu/drm/drm_gpuvm.c:386
msgid ""
"Existent mapping is a centered subset of the requested one, hence replace "
"the existing one."
msgstr ""

#: ../../../gpu/drm-mm:483: drivers/gpu/drm/drm_gpuvm.c:405
msgid ""
"Existent mappings is overlapped at the beginning by the requested mapping "
"backed by a different BO. Hence, map the requested mapping and split up the "
"existing one, adjusting its BO offset accordingly."
msgstr ""

#: ../../../gpu/drm-mm.rst:489
msgid "Locking"
msgstr ""

#: ../../../gpu/drm-mm:491: drivers/gpu/drm/drm_gpuvm.c:424
msgid ""
"In terms of managing :c:type:`drm_gpuva` entries DRM GPUVM does not take "
"care of locking itself, it is the drivers responsibility to take care about "
"locking. Drivers might want to protect the following operations: inserting, "
"removing and iterating :c:type:`drm_gpuva` objects as well as generating all "
"kinds of operations, such as split / merge or prefetch."
msgstr ""

#: ../../../gpu/drm-mm:491: drivers/gpu/drm/drm_gpuvm.c:430
msgid ""
"DRM GPUVM also does not take care of the locking of the backing :c:type:"
"`drm_gem_object` buffers GPU VA lists and :c:type:`drm_gpuvm_bo` "
"abstractions by itself; drivers are responsible to enforce mutual exclusion "
"using either the GEMs dma_resv lock or alternatively a driver specific "
"external lock. For the latter see also drm_gem_gpuva_set_lock()."
msgstr ""

#: ../../../gpu/drm-mm:491: drivers/gpu/drm/drm_gpuvm.c:436
msgid ""
"However, DRM GPUVM contains lockdep checks to ensure callers of its API hold "
"the corresponding lock whenever the :c:type:`drm_gem_objects` GPU VA list is "
"accessed by functions such as drm_gpuva_link() or drm_gpuva_unlink(), but "
"also drm_gpuvm_bo_obtain() and drm_gpuvm_bo_put()."
msgstr ""

#: ../../../gpu/drm-mm:491: drivers/gpu/drm/drm_gpuvm.c:441
msgid ""
"The latter is required since on creation and destruction of a :c:type:"
"`drm_gpuvm_bo` the :c:type:`drm_gpuvm_bo` is attached / removed from the :c:"
"type:`drm_gem_objects` gpuva list. Subsequent calls to drm_gpuvm_bo_obtain() "
"for the same :c:type:`drm_gpuvm` and :c:type:`drm_gem_object` must be able "
"to observe previous creations and destructions of :c:type:`drm_gpuvm_bos` in "
"order to keep instances unique."
msgstr ""

#: ../../../gpu/drm-mm:491: drivers/gpu/drm/drm_gpuvm.c:447
msgid ""
"The :c:type:`drm_gpuvm`'s lists for keeping track of external and evicted "
"objects are protected against concurrent insertion / removal and iteration "
"internally."
msgstr ""

#: ../../../gpu/drm-mm:491: drivers/gpu/drm/drm_gpuvm.c:450
msgid ""
"However, drivers still need ensure to protect concurrent calls to functions "
"iterating those lists, namely drm_gpuvm_prepare_objects() and "
"drm_gpuvm_validate()."
msgstr ""

#: ../../../gpu/drm-mm:491: drivers/gpu/drm/drm_gpuvm.c:454
msgid ""
"Alternatively, drivers can set the :c:type:`DRM_GPUVM_RESV_PROTECTED` flag "
"to indicate that the corresponding :c:type:`dma_resv` locks are held in "
"order to protect the lists. If :c:type:`DRM_GPUVM_RESV_PROTECTED` is set, "
"internal locking is disabled and the corresponding lockdep checks are "
"enabled. This is an optimization for drivers which are capable of taking the "
"corresponding :c:type:`dma_resv` locks and hence do not require internal "
"locking."
msgstr ""

#: ../../../gpu/drm-mm.rst:495
msgid "Examples"
msgstr ""

#: ../../../gpu/drm-mm:497: drivers/gpu/drm/drm_gpuvm.c:465
msgid ""
"This section gives two examples on how to let the DRM GPUVA Manager "
"generate :c:type:`drm_gpuva_op` in order to satisfy a given map or unmap "
"request and how to make use of them."
msgstr ""

#: ../../../gpu/drm-mm:497: drivers/gpu/drm/drm_gpuvm.c:469
msgid ""
"The below code is strictly limited to illustrate the generic usage pattern. "
"To maintain simplicity, it doesn't make use of any abstractions for common "
"code, different (asynchronous) stages with fence signalling critical paths, "
"any other helpers or error handling in terms of freeing memory and dropping "
"previously taken locks."
msgstr ""

#: ../../../gpu/drm-mm:497: drivers/gpu/drm/drm_gpuvm.c:475
msgid "Obtain a list of :c:type:`drm_gpuva_op` to create a new mapping::"
msgstr ""

#: ../../../gpu/drm-mm:497: drivers/gpu/drm/drm_gpuvm.c:566
msgid ""
"Receive a callback for each :c:type:`drm_gpuva_op` to create a new mapping::"
msgstr ""

#: ../../../gpu/drm-mm.rst:501
msgid "DRM GPUVM Function References"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:42
msgid "flags for struct drm_gpuva"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:48
msgid "``DRM_GPUVA_INVALIDATED``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:49
msgid ""
"Flag indicating that the :c:type:`drm_gpuva`'s backing GEM is invalidated."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:51
msgid "``DRM_GPUVA_SPARSE``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:52
msgid "Flag indicating that the :c:type:`drm_gpuva` is a sparse mapping."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:54
msgid "``DRM_GPUVA_USERBITS``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:55
#: include/drm/drm_gpuvm.h:210
msgid "user defined bits"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:66
msgid "structure to track a GPU VA mapping"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:75
#: include/drm/drm_gpuvm.h:524 include/drm/drm_gpuvm.h:647
msgid "``vm``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:76
msgid "the :c:type:`drm_gpuvm` this object is associated with"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:80
msgid "``vm_bo``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:81
msgid ""
"the :c:type:`drm_gpuvm_bo` abstraction for the mapped :c:type:"
"`drm_gem_object`"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:87
msgid "the :c:type:`drm_gpuva_flags` for this mapping"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:91
#: include/drm/drm_gpuvm.h:830 include/drm/drm_gpuvm.h:869
#: include/drm/drm_gpuvm.h:934
msgid "``va``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:92
msgid "structure containing the address and range of the :c:type:`drm_gpuva`"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:95
#: include/drm/drm_gpuvm.h:835
msgid "``va.addr``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:96
msgid "the start address"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:106
#: include/drm/drm_gpuvm.h:846
msgid "``gem``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:107
#: include/drm/drm_gpuvm.h:847
msgid "structure containing the :c:type:`drm_gem_object` and its offset"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:110
#: include/drm/drm_gpuvm.h:850
msgid "``gem.offset``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:111
#: include/drm/drm_gpuvm.h:851 ../../../gpu/drm-mm:506:
#: drivers/gpu/drm/drm_gpuvm.c:2309 drivers/gpu/drm/drm_gpuvm.c:2427
#: drivers/gpu/drm/drm_gpuvm.c:2617
msgid "the offset within the :c:type:`drm_gem_object`"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:115
#: include/drm/drm_gpuvm.h:855
msgid "``gem.obj``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:116
msgid "the mapped :c:type:`drm_gem_object`"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:120
msgid "``gem.entry``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:121
msgid ""
"the :c:type:`list_head` to attach this object to a :c:type:`drm_gpuvm_bo`"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:126
#: include/drm/drm_gpuvm.h:253
msgid "``rb``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:127
msgid "structure containing data to store :c:type:`drm_gpuvas` in a rb-tree"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:130
msgid "``rb.node``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:131
msgid "the rb-tree node"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:135
msgid "``rb.entry``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:136
msgid ""
"The :c:type:`list_head` to additionally connect :c:type:`drm_gpuvas` in the "
"same order they appear in the interval tree. This is useful to keep "
"iterating :c:type:`drm_gpuvas` from a start node found through the rb-tree "
"while doing modifications on the rb-tree itself."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:144
msgid "``rb.__subtree_last``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:145
msgid "needed by the interval tree, holding last-in-subtree"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:67
msgid ""
"This structure represents a GPU VA mapping and is associated with a :c:type:"
"`drm_gpuvm`."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:70
msgid "Typically, this structure is embedded in bigger driver structures."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:173
msgid "sets whether the backing GEM of this :c:type:`drm_gpuva` is invalidated"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:179
#: include/drm/drm_gpuvm.h:193 ../../../gpu/drm-mm:506:
#: drivers/gpu/drm/drm_gpuvm.c:1748 drivers/gpu/drm/drm_gpuvm.c:1796
#: drivers/gpu/drm/drm_gpuvm.c:1822 drivers/gpu/drm/drm_gpuvm.c:1855
#: drivers/gpu/drm/drm_gpuvm.c:1995
msgid "``struct drm_gpuva *va``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:175
msgid "the :c:type:`drm_gpuva` to set the invalidate flag for"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:177
msgid "``bool invalidate``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:176
msgid "indicates whether the :c:type:`drm_gpuva` is invalidated"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:187
msgid ""
"indicates whether the backing BO of this :c:type:`drm_gpuva` is invalidated"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:189
msgid "the :c:type:`drm_gpuva` to check"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:191
msgid "``true`` if the GPU VA is invalidated, ``false`` otherwise"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:199
msgid "flags for struct drm_gpuvm"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:205
msgid "``DRM_GPUVM_RESV_PROTECTED``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:206
msgid "GPUVM is protected externally by the GPUVM's :c:type:`dma_resv` lock"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:209
msgid "``DRM_GPUVM_USERBITS``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:215
msgid "DRM GPU VA Manager"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:229
msgid "the name of the DRM GPU VA space"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:234
msgid "the :c:type:`drm_gpuvm_flags` of this GPUVM"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:238
msgid "``drm``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:239
msgid "the :c:type:`drm_device` this VM lives in"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:243
msgid "``mm_start``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:244
msgid "start of the VA space"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:248
msgid "``mm_range``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:249
msgid "length of the VA space"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:254
msgid "structures to track :c:type:`drm_gpuva` entries"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:257
msgid "``rb.tree``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:258
msgid "the rb-tree to track GPU VA mappings"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:262
msgid "``rb.list``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:263
msgid "the :c:type:`list_head` to track GPU VA mappings"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:268
#: include/drm/drm_gpuvm.h:665
msgid "``kref``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:269
msgid "reference count of this object"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:273
msgid "``kernel_alloc_node``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:274
msgid ""
":c:type:`drm_gpuva` representing the address space cutout reserved for the "
"kernel"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:281
#: include/drm/drm_gpuvm.h:997 include/drm/drm_gpuvm.h:1007
#: include/drm/drm_gpuvm.h:1018 include/drm/drm_gpuvm.h:1029
#: include/drm/drm_gpuvm.h:1042 include/drm/drm_gpuvm.h:1049
#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:541
#: include/drm/gpu_scheduler.h:603
msgid "``ops``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:282
msgid ":c:type:`drm_gpuvm_ops` providing the split/merge steps to drivers"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:286
msgid "``r_obj``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:287
msgid "Resv GEM object; representing the GPUVM's common :c:type:`dma_resv`."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:291
msgid "``extobj``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:292
msgid "structure holding the extobj list"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:295
msgid "``extobj.list``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:296
msgid ""
":c:type:`list_head` storing :c:type:`drm_gpuvm_bos` serving as external "
"object"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:301
msgid "``extobj.local_list``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:302
msgid ""
"pointer to the local list temporarily storing entries from the external "
"object list"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:307
msgid "``extobj.lock``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:308
msgid "spinlock to protect the extobj list"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:314
msgid "structure holding the evict list and evict list lock"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:317
msgid "``evict.list``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:318
msgid ""
":c:type:`list_head` storing :c:type:`drm_gpuvm_bos` currently being evicted"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:323
msgid "``evict.local_list``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:324
msgid ""
"pointer to the local list temporarily storing entries from the evicted "
"object list"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:329
msgid "``evict.lock``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:330
msgid "spinlock to protect the evict list"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:216
msgid ""
"The DRM GPU VA Manager keeps track of a GPU's virtual address space by "
"using :c:type:`maple_tree` structures. Typically, this structure is embedded "
"in bigger driver structures."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:220
msgid ""
"Drivers can pass addresses and ranges in an arbitrary unit, e.g. bytes or "
"pages."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:223
msgid "There should be one manager instance per GPU virtual address space."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:344
msgid "acquire a struct drm_gpuvm reference"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:350
#: include/drm/drm_gpuvm.h:375 include/drm/drm_gpuvm.h:418
#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:928
#: drivers/gpu/drm/drm_gpuvm.c:986 drivers/gpu/drm/drm_gpuvm.c:1080
#: drivers/gpu/drm/drm_gpuvm.c:1104 drivers/gpu/drm/drm_gpuvm.c:1169
#: drivers/gpu/drm/drm_gpuvm.c:1205 drivers/gpu/drm/drm_gpuvm.c:1413
#: drivers/gpu/drm/drm_gpuvm.c:1438 drivers/gpu/drm/drm_gpuvm.c:1466
#: drivers/gpu/drm/drm_gpuvm.c:1575 drivers/gpu/drm/drm_gpuvm.c:1597
#: drivers/gpu/drm/drm_gpuvm.c:1751 drivers/gpu/drm/drm_gpuvm.c:1890
#: drivers/gpu/drm/drm_gpuvm.c:1908 drivers/gpu/drm/drm_gpuvm.c:1937
#: drivers/gpu/drm/drm_gpuvm.c:1959 drivers/gpu/drm/drm_gpuvm.c:1981
#: drivers/gpu/drm/drm_gpuvm.c:1997 drivers/gpu/drm/drm_gpuvm.c:2309
#: drivers/gpu/drm/drm_gpuvm.c:2359 drivers/gpu/drm/drm_gpuvm.c:2426
#: drivers/gpu/drm/drm_gpuvm.c:2504 drivers/gpu/drm/drm_gpuvm.c:2618
#: drivers/gpu/drm/drm_gpuvm.c:2685 drivers/gpu/drm/drm_gpuvm.c:2747
#: drivers/gpu/drm/drm_gpuvm.c:2855
msgid "``struct drm_gpuvm *gpuvm``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:345
msgid "the :c:type:`drm_gpuvm` to acquire the reference of"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:346
msgid ""
"This function acquires an additional reference to **gpuvm**. It is illegal "
"to call this without already holding a reference. No locks required."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:350
msgid "the :c:type:`struct drm_gpuvm <drm_gpuvm>` pointer"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:369
msgid "indicates whether :c:type:`DRM_GPUVM_RESV_PROTECTED` is set"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:371
#: include/drm/drm_gpuvm.h:383 include/drm/drm_gpuvm.h:392
#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1099
#: drivers/gpu/drm/drm_gpuvm.c:1164 drivers/gpu/drm/drm_gpuvm.c:1200
#: drivers/gpu/drm/drm_gpuvm.c:1993
msgid "the :c:type:`drm_gpuvm`"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:373
msgid "true if :c:type:`DRM_GPUVM_RESV_PROTECTED` is set, false otherwise."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:384
msgid "``drm_gpuvm_resv (gpuvm__)``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:382
msgid "returns the :c:type:`drm_gpuvm`'s :c:type:`dma_resv`"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:388
#: include/drm/drm_gpuvm.h:396 include/drm/drm_gpuvm.h:440
#: include/drm/drm_gpuvm.h:463 include/drm/drm_gpuvm.h:485
#: include/drm/drm_gpuvm.h:497
msgid "``gpuvm__``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:385
msgid "a pointer to the :c:type:`drm_gpuvm`'s shared :c:type:`dma_resv`"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:392
msgid "``drm_gpuvm_resv_obj (gpuvm__)``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:390
msgid ""
"returns the :c:type:`drm_gem_object` holding the :c:type:`drm_gpuvm`'s :c:"
"type:`dma_resv`"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:394
msgid ""
"a pointer to the :c:type:`drm_gem_object` holding the :c:type:`drm_gpuvm`'s "
"shared :c:type:`dma_resv`"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:412
msgid ""
"indicates whether the given :c:type:`drm_gem_object` is an external object"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:414
msgid "the :c:type:`drm_gpuvm` to check"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:415
msgid "the :c:type:`drm_gem_object` to check"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:417
msgid ""
"true if the :c:type:`drm_gem_object` :c:type:`dma_resv` differs from the :c:"
"type:`drm_gpuvms` :c:type:`dma_resv`, false otherwise"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:439
msgid "``drm_gpuvm_for_each_va_range (va__, gpuvm__, start__, end__)``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:437
msgid "iterate over a range of :c:type:`drm_gpuvas`"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:443
#: include/drm/drm_gpuvm.h:464 include/drm/drm_gpuvm.h:488
#: include/drm/drm_gpuvm.h:499 include/drm/drm_gpuvm.h:768
#: include/drm/drm_gpuvm.h:781
msgid "``va__``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:438
#: include/drm/drm_gpuvm.h:763 include/drm/drm_gpuvm.h:777
msgid ":c:type:`drm_gpuva` structure to assign to in each iteration step"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:439
#: include/drm/drm_gpuvm.h:462 include/drm/drm_gpuvm.h:484
#: include/drm/drm_gpuvm.h:496
msgid ":c:type:`drm_gpuvm` to walk over"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:440
#: include/drm/drm_gpuvm.h:463
msgid "starting offset, the first gpuva will overlap this"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:441
#: include/drm/drm_gpuvm.h:464
msgid "ending offset, the last gpuva will start before this (but may overlap)"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:443
msgid ""
"This iterator walks over all :c:type:`drm_gpuvas` in the :c:type:`drm_gpuvm` "
"that lie between **start__** and **end__**. It is implemented similarly to "
"list_for_each(), but is using the :c:type:`drm_gpuvm`'s internal interval "
"tree to accelerate the search for the starting :c:type:`drm_gpuva`, and "
"hence isn't safe against removal of elements. It assumes that **end__** is "
"within (or is the upper limit of) the :c:type:`drm_gpuvm`. This iterator "
"does not skip over the :c:type:`drm_gpuvm`'s **kernel_alloc_node**."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:460
msgid ""
"``drm_gpuvm_for_each_va_range_safe (va__, next__, gpuvm__, start__, end__)``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:458
msgid "safely iterate over a range of :c:type:`drm_gpuvas`"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:460
#: include/drm/drm_gpuvm.h:483 include/drm/drm_gpuvm.h:494
msgid ":c:type:`drm_gpuva` to assign to in each iteration step"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:461
#: include/drm/drm_gpuvm.h:495
msgid "another :c:type:`drm_gpuva` to use as temporary storage"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:466
msgid ""
"This iterator walks over all :c:type:`drm_gpuvas` in the :c:type:`drm_gpuvm` "
"that lie between **start__** and **end__**. It is implemented similarly to "
"list_for_each_safe(), but is using the :c:type:`drm_gpuvm`'s internal "
"interval tree to accelerate the search for the starting :c:type:`drm_gpuva`, "
"and hence is safe against removal of elements. It assumes that **end__** is "
"within (or is the upper limit of) the :c:type:`drm_gpuvm`. This iterator "
"does not skip over the :c:type:`drm_gpuvm`'s **kernel_alloc_node**."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:484
msgid "``drm_gpuvm_for_each_va (va__, gpuvm__)``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:482
msgid "iterate over all :c:type:`drm_gpuvas`"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:485
msgid ""
"This iterator walks over all :c:type:`drm_gpuva` structures associated with "
"the given :c:type:`drm_gpuvm`."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:495
msgid "``drm_gpuvm_for_each_va_safe (va__, next__, gpuvm__)``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:493
msgid "safely iterate over all :c:type:`drm_gpuvas`"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:497
msgid ""
"This iterator walks over all :c:type:`drm_gpuva` structures associated with "
"the given :c:type:`drm_gpuvm`. It is implemented with "
"list_for_each_entry_safe(), and hence safe against the removal of elements."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:506
msgid ":c:type:`drm_gpuvm` abstraction of :c:type:`drm_exec`"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:514
#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:75 include/drm/drm_exec.h:86
#: include/drm/drm_exec.h:101 include/drm/drm_exec.h:120
msgid "``exec``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:515
msgid "the :c:type:`drm_exec` structure"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:520
msgid "the flags for the struct drm_exec"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:525
msgid "the :c:type:`drm_gpuvm` to lock its DMA reservations"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:529
msgid "``num_fences``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:530
msgid ""
"the number of fences to reserve for the :c:type:`dma_resv` of the locked :c:"
"type:`drm_gem_objects`"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:535
msgid "``extra``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:536
msgid ""
"Callback and corresponding private data for the driver to lock arbitrary "
"additional :c:type:`drm_gem_objects`."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:540
msgid "``extra.fn``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:541
msgid "The driver callback to lock additional :c:type:`drm_gem_objects`."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:546
msgid "``extra.priv``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:547
msgid "driver private data for the **fn** callback"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:507
msgid ""
"This structure should be created on the stack as :c:type:`drm_exec` should "
"be."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:509
msgid ""
"Optionally, **extra** can be set in order to lock additional :c:type:"
"`drm_gem_objects`."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:575
msgid "lock all dma-resv of all assoiciated BOs"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:581
#: include/drm/drm_gpuvm.h:603 include/drm/drm_gpuvm.h:622
#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1239
#: drivers/gpu/drm/drm_gpuvm.c:1302 drivers/gpu/drm/drm_gpuvm.c:1333
msgid "``struct drm_gpuvm_exec *vm_exec``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:576
#: include/drm/drm_gpuvm.h:598 include/drm/drm_gpuvm.h:617
#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1234
#: drivers/gpu/drm/drm_gpuvm.c:1297 drivers/gpu/drm/drm_gpuvm.c:1328
msgid "the :c:type:`drm_gpuvm_exec` wrapper"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:577
msgid ""
"Releases all dma-resv locks of all :c:type:`drm_gem_objects` previously "
"acquired through drm_gpuvm_exec_lock() or its variants."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:597
msgid "add fence to private and all extobj"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:600
#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1437
#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:329
#: drivers/gpu/drm/drm_syncobj.c:369 drivers/gpu/drm/drm_syncobj.c:547
#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:896
#: drivers/gpu/drm/scheduler/sched_main.c:1029
msgid "``struct dma_fence *fence``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:599
#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1436
msgid "fence to add"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:601
#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1438
msgid "``enum dma_resv_usage private_usage``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:600
#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1437
msgid "private dma-resv usage"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:602
#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1439
msgid "``enum dma_resv_usage extobj_usage``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:601
#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1438
msgid "extobj dma-resv usage"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:602
msgid "See drm_gpuvm_resv_add_fence()."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:616
#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1407
msgid "validate all BOs marked as evicted"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:618
msgid "See drm_gpuvm_validate()."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:630
msgid ""
"structure representing a :c:type:`drm_gpuvm` and :c:type:`drm_gem_object` "
"combination"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:648
msgid ""
"The :c:type:`drm_gpuvm` the **obj** is mapped in. This is a reference "
"counted pointer."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:654
msgid ""
"The :c:type:`drm_gem_object` being mapped in **vm**. This is a reference "
"counted pointer."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:659
msgid "``evicted``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:660
msgid ""
"Indicates whether the :c:type:`drm_gem_object` is evicted; field protected "
"by the :c:type:`drm_gem_object`'s dma-resv lock."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:666
msgid "The reference count for this :c:type:`drm_gpuvm_bo`."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:671
msgid "Structure containing all :c:type:`list_heads`."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:674
msgid "``list.gpuva``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:675
msgid "The list of linked :c:type:`drm_gpuvas`."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:677
msgid ""
"It is safe to access entries from this list as long as the GEM's gpuva lock "
"is held. See also struct drm_gem_object."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:682
msgid "``list.entry``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:683
msgid "Structure containing all :c:type:`list_heads` serving as entry."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:687
msgid "``list.entry.gem``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:688
msgid "List entry to attach to the :c:type:`drm_gem_objects` gpuva list."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:699
msgid "``list.entry.evict``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:700
msgid "List entry to attach to the :c:type:`drm_gpuvms` evict list."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:631
msgid ""
"This structure is an abstraction representing a :c:type:`drm_gpuvm` and :c:"
"type:`drm_gem_object` combination. It serves as an indirection to accelerate "
"iterating all :c:type:`drm_gpuvas` within a :c:type:`drm_gpuvm` backed by "
"the same :c:type:`drm_gem_object`."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:636
msgid ""
"Furthermore it is used cache evicted GEM objects for a certain GPU-VM to "
"accelerate validation."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:639
msgid ""
"Typically, drivers want to create an instance of a struct drm_gpuvm_bo once "
"a GEM object is mapped first in a GPU-VM and release the instance once the "
"last mapping of the GEM object in this GPU-VM is unmapped."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:718
msgid "acquire a struct drm_gpuvm_bo reference"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:724
#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1534
#: drivers/gpu/drm/drm_gpuvm.c:1669 drivers/gpu/drm/drm_gpuvm.c:1692
#: drivers/gpu/drm/drm_gpuvm.c:1819 drivers/gpu/drm/drm_gpuvm.c:2801
msgid "``struct drm_gpuvm_bo *vm_bo``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:719
msgid "the :c:type:`drm_gpuvm_bo` to acquire the reference of"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:720
msgid ""
"This function acquires an additional reference to **vm_bo**. It is illegal "
"to call this without already holding a reference. No locks required."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:724
msgid "the :c:type:`struct vm_bo <vm_bo>` pointer"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:742
msgid ""
"add/remove all :c:type:`drm_gpuvm_bo`'s in the list to/from the :c:type:"
"`drm_gpuvms` evicted list"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:746
#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1690
msgid "``bool evict``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:745
msgid "indicates whether **obj** is evicted"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:746
msgid "See drm_gpuvm_bo_evict()."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:764
msgid "``drm_gpuvm_bo_for_each_va (va__, vm_bo__)``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:762
msgid "iterator to walk over a list of :c:type:`drm_gpuva`"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:765
#: include/drm/drm_gpuvm.h:780
msgid "``vm_bo__``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:764
#: include/drm/drm_gpuvm.h:779
msgid ""
"the :c:type:`drm_gpuvm_bo` the :c:type:`drm_gpuva` to walk are associated "
"with"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:765
msgid ""
"This iterator walks over all :c:type:`drm_gpuva` structures associated with "
"the :c:type:`drm_gpuvm_bo`."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:768
#: include/drm/drm_gpuvm.h:784
msgid "The caller must hold the GEM's gpuva lock."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:777
msgid "``drm_gpuvm_bo_for_each_va_safe (va__, next__, vm_bo__)``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:775
msgid "iterator to safely walk over a list of :c:type:`drm_gpuva`"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:778
msgid ":c:type:`next` :c:type:`drm_gpuva` to store the next step"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:780
msgid ""
"This iterator walks over all :c:type:`drm_gpuva` structures associated with "
"the :c:type:`drm_gpuvm_bo`. It is implemented with "
"list_for_each_entry_safe(), hence it is save against removal of elements."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:791
msgid "GPU VA operation type"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:797
msgid "``DRM_GPUVA_OP_MAP``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:798
msgid "the map op type"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:800
msgid "``DRM_GPUVA_OP_REMAP``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:801
msgid "the remap op type"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:803
msgid "``DRM_GPUVA_OP_UNMAP``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:804
msgid "the unmap op type"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:806
msgid "``DRM_GPUVA_OP_PREFETCH``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:807
msgid "the prefetch op type"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:809
msgid "``DRM_GPUVA_OP_DRIVER``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:810
msgid "the driver defined op type"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:792
msgid ""
"Operations to alter the GPU VA mappings tracked by the :c:type:`drm_gpuvm`."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:823
msgid "GPU VA map operation"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:831
msgid "structure containing address and range of a map operation"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:836
msgid "the base address of the new mapping"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:840
msgid "``va.range``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:841
#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2307
#: drivers/gpu/drm/drm_gpuvm.c:2615
msgid "the range of the new mapping"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:856
#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2308
#: drivers/gpu/drm/drm_gpuvm.c:2426 drivers/gpu/drm/drm_gpuvm.c:2616
msgid "the :c:type:`drm_gem_object` to map"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:824
msgid ""
"This structure represents a single map operation generated by the DRM GPU VA "
"manager."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:862
msgid "GPU VA unmap operation"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:870
msgid "the :c:type:`drm_gpuva` to unmap"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:874
msgid "``keep``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:875
msgid ""
"Indicates whether this :c:type:`drm_gpuva` is physically contiguous with the "
"original mapping request."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:878
msgid ""
"Optionally, if :c:type:`keep` is set, drivers may keep the actual page table "
"mappings for this :c:type:`drm_gpuva`, adding the missing page table entries "
"only and update the :c:type:`drm_gpuvm` accordingly."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:863
msgid ""
"This structure represents a single unmap operation generated by the DRM GPU "
"VA manager."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:887
msgid "GPU VA remap operation"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:911
msgid "``prev``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:912
msgid "the preceding part of a split mapping"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:917
msgid "the subsequent part of a split mapping"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:921
#: include/drm/drm_gpuvm.h:972
msgid "``unmap``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:922
msgid "the unmap operation for the original existing mapping"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:888
msgid ""
"This represents a single remap operation generated by the DRM GPU VA manager."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:890
msgid ""
"A remap operation is generated when an existing GPU VA mmapping is split up "
"by inserting a new GPU VA mapping or by partially unmapping existent "
"mapping(s), hence it consists of a maximum of two map and one unmap "
"operation."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:895
msgid ""
"The **unmap** operation takes care of removing the original existing "
"mapping. **prev** is used to remap the preceding part, **next** the "
"subsequent part."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:898
msgid ""
"If either a new mapping's start address is aligned with the start address of "
"the old mapping or the new mapping's end address is aligned with the end "
"address of the old mapping, either **prev** or **next** is NULL."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:902
msgid ""
"Note, the reason for a dedicated remap operation, rather than arbitrary "
"unmap and map operations, is to give drivers the chance of extracting driver "
"specific data for creating the new mappings from the unmap operations's :c:"
"type:`drm_gpuva` structure which typically is embedded in larger driver "
"specific structures."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:927
msgid "GPU VA prefetch operation"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:935
msgid "the :c:type:`drm_gpuva` to prefetch"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:928
msgid ""
"This structure represents a single prefetch operation generated by the DRM "
"GPU VA manager."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:940
msgid "GPU VA operation"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:949
msgid ""
"The :c:type:`list_head` used to distribute instances of this struct within :"
"c:type:`drm_gpuva_ops`."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:956
#: include/drm/drm_gpuvm.h:1000 include/drm/drm_gpuvm.h:1009
#: include/drm/drm_gpuvm.h:1021 include/drm/drm_gpuvm.h:1032
#: include/drm/drm_gpuvm.h:1056 include/drm/drm_gpuvm.h:1062
msgid "``op``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:957
msgid "the type of the operation"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1 ../../../gpu/drm-mm:568:
#: include/drm/gpu_scheduler.h:1
msgid "``{unnamed_union}``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:2 ../../../gpu/drm-mm:568:
#: include/drm/gpu_scheduler.h:2
msgid "anonymous"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:963
msgid "the map operation"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:967
msgid "``remap``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:968
msgid "the remap operation"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:973
msgid "the unmap operation"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:977
msgid "``prefetch``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:978
msgid "the prefetch operation"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:941
msgid "This structure represents a single generic operation."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:943
msgid "The particular type of the operation is defined by **op**."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:984
msgid "wraps a list of :c:type:`drm_gpuva_op`"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:989
msgid "the :c:type:`list_head`"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:996
msgid "``drm_gpuva_for_each_op (op, ops)``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:994
msgid "iterator to walk over :c:type:`drm_gpuva_ops`"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:995
#: include/drm/drm_gpuvm.h:1004 include/drm/drm_gpuvm.h:1016
#: include/drm/drm_gpuvm.h:1027
msgid ":c:type:`drm_gpuva_op` to assign in each iteration step"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:996
#: include/drm/drm_gpuvm.h:1006 include/drm/drm_gpuvm.h:1017
#: include/drm/drm_gpuvm.h:1028
msgid ":c:type:`drm_gpuva_ops` to walk"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:997
msgid "This iterator walks over all ops within a given list of operations."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1005
msgid "``drm_gpuva_for_each_op_safe (op, next, ops)``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1003
msgid "iterator to safely walk over :c:type:`drm_gpuva_ops`"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1005
msgid ":c:type:`next` :c:type:`drm_gpuva_op` to store the next step"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1007
msgid ""
"This iterator walks over all ops within a given list of operations. It is "
"implemented with list_for_each_safe(), so save against removal of elements."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1017
msgid "``drm_gpuva_for_each_op_from_reverse (op, ops)``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1015
msgid "iterate backwards from the given point"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1018
msgid ""
"This iterator walks over all ops within a given list of operations beginning "
"from the given operation in reverse order."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1028
msgid "``drm_gpuva_for_each_op_reverse (op, ops)``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1026
msgid "iterator to walk over :c:type:`drm_gpuva_ops` in reverse"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1029
msgid ""
"This iterator walks over all ops within a given list of operations in reverse"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1038
msgid "``drm_gpuva_first_op (ops)``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1036
msgid "returns the first :c:type:`drm_gpuva_op` from :c:type:`drm_gpuva_ops`"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1037
msgid "the :c:type:`drm_gpuva_ops` to get the fist :c:type:`drm_gpuva_op` from"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1045
msgid "``drm_gpuva_last_op (ops)``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1043
msgid "returns the last :c:type:`drm_gpuva_op` from :c:type:`drm_gpuva_ops`"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1044
msgid "the :c:type:`drm_gpuva_ops` to get the last :c:type:`drm_gpuva_op` from"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1052
msgid "``drm_gpuva_prev_op (op)``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1050
msgid "previous :c:type:`drm_gpuva_op` in the list"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1051
#: include/drm/drm_gpuvm.h:1057
msgid "the current :c:type:`drm_gpuva_op`"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1058
msgid "``drm_gpuva_next_op (op)``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1056
msgid "next :c:type:`drm_gpuva_op` in the list"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1087
msgid "callbacks for split/merge steps"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1095
msgid "``vm_free``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1096
msgid "called when the last reference of a struct drm_gpuvm is dropped"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1103
msgid "``op_alloc``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1104
msgid "called when the :c:type:`drm_gpuvm` allocates a struct drm_gpuva_op"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1107
msgid ""
"Some drivers may want to embed struct drm_gpuva_op into driver specific "
"structures. By implementing this callback drivers can allocate memory "
"accordingly."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1115
msgid "``op_free``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1116
msgid "called when the :c:type:`drm_gpuvm` frees a struct drm_gpuva_op"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1119
msgid ""
"Some drivers may want to embed struct drm_gpuva_op into driver specific "
"structures. By implementing this callback drivers can free the previously "
"allocated memory accordingly."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1127
msgid "``vm_bo_alloc``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1128
msgid "called when the :c:type:`drm_gpuvm` allocates a struct drm_gpuvm_bo"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1131
msgid ""
"Some drivers may want to embed struct drm_gpuvm_bo into driver specific "
"structures. By implementing this callback drivers can allocate memory "
"accordingly."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1139
msgid "``vm_bo_free``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1140
msgid "called when the :c:type:`drm_gpuvm` frees a struct drm_gpuvm_bo"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1143
msgid ""
"Some drivers may want to embed struct drm_gpuvm_bo into driver specific "
"structures. By implementing this callback drivers can free the previously "
"allocated memory accordingly."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1151
msgid "``vm_bo_validate``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1152
msgid "called from drm_gpuvm_validate()"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1154
msgid ""
"Drivers receive this callback for every evicted :c:type:`drm_gem_object` "
"being mapped in the corresponding :c:type:`drm_gpuvm`."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1157
msgid ""
"Typically, drivers would call their driver specific variant of "
"ttm_bo_validate() from within this callback."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1163
msgid "``sm_step_map``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1164
msgid ""
"called from :c:type:`drm_gpuvm_sm_map` to finally insert the mapping once "
"all previous steps were completed"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1167
#: include/drm/drm_gpuvm.h:1183 include/drm/drm_gpuvm.h:1199
msgid ""
"The :c:type:`priv` pointer matches the one the driver passed to :c:type:"
"`drm_gpuvm_sm_map` or :c:type:`drm_gpuvm_sm_unmap`, respectively."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1170
msgid "Can be NULL if :c:type:`drm_gpuvm_sm_map` is used."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1174
msgid "``sm_step_remap``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1175
msgid ""
"called from :c:type:`drm_gpuvm_sm_map` and :c:type:`drm_gpuvm_sm_unmap` to "
"split up an existent mapping"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1178
msgid ""
"This callback is called when existent mapping needs to be split up. This is "
"the case when either a newly requested mapping overlaps or is enclosed by an "
"existent mapping or a partial unmap of an existent mapping is requested."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1186
#: include/drm/drm_gpuvm.h:1202
msgid ""
"Can be NULL if neither :c:type:`drm_gpuvm_sm_map` nor :c:type:"
"`drm_gpuvm_sm_unmap` is used."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1191
msgid "``sm_step_unmap``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1192
msgid ""
"called from :c:type:`drm_gpuvm_sm_map` and :c:type:`drm_gpuvm_sm_unmap` to "
"unmap an existing mapping"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1195
msgid ""
"This callback is called when existing mapping needs to be unmapped. This is "
"the case when either a newly requested mapping encloses an existing mapping "
"or an unmap of an existing mapping is requested."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1088
msgid ""
"This structure defines the callbacks used by :c:type:`drm_gpuvm_sm_map` and :"
"c:type:`drm_gpuvm_sm_unmap` to provide the split/merge steps for map and "
"unmap operations to drivers."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1233
msgid "Helper to get the start and range of the unmap stage of a remap op."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1239
msgid "``const struct drm_gpuva_op_remap *op``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1235
msgid "Remap op."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1237
msgid "``u64 *start_addr``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1236
msgid "Output pointer for the start of the required unmap."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1238
msgid "``u64 *range``"
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1237
msgid "Output pointer for the length of the required unmap."
msgstr ""

#: ../../../gpu/drm-mm:503: include/drm/drm_gpuvm.h:1238
msgid ""
"The given start address and range will be set such that they represent the "
"range of the address space that was previously covered by the mapping being "
"re-mapped, but is now empty."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:922
msgid ""
"checks whether the given range is valid for the given :c:type:`drm_gpuvm`"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:924
msgid "the GPUVM to check the range for"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:926
#: drivers/gpu/drm/drm_gpuvm.c:1203 drivers/gpu/drm/drm_gpuvm.c:1330
#: drivers/gpu/drm/drm_gpuvm.c:1887 drivers/gpu/drm/drm_gpuvm.c:1905
#: drivers/gpu/drm/drm_gpuvm.c:1979 drivers/gpu/drm/drm_gpuvm.c:2744
msgid "``u64 addr``"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:925
msgid "the base address"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:927
#: drivers/gpu/drm/drm_gpuvm.c:988 drivers/gpu/drm/drm_gpuvm.c:1204
#: drivers/gpu/drm/drm_gpuvm.c:1331 drivers/gpu/drm/drm_gpuvm.c:1888
#: drivers/gpu/drm/drm_gpuvm.c:1906 drivers/gpu/drm/drm_gpuvm.c:1980
#: drivers/gpu/drm/drm_gpuvm.c:2745
msgid "``u64 range``"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:926
msgid "the range starting from the base address"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:927
msgid "Checks whether the range is within the GPUVM's managed boundaries."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:930
msgid "true for a valid range, false otherwise"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:954
msgid "allocate a dummy :c:type:`drm_gem_object`"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:955
msgid "the drivers :c:type:`drm_device`"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:956
msgid ""
"Allocates a dummy :c:type:`drm_gem_object` which can be passed to "
"drm_gpuvm_init() in order to serve as root GEM object providing the :c:type:"
"`drm_resv` shared across :c:type:`drm_gem_objects` local to a single GPUVM."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:961
msgid "the :c:type:`drm_gem_object` on success, NULL on failure"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:980
msgid "initialize a :c:type:`drm_gpuvm`"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:981
msgid "pointer to the :c:type:`drm_gpuvm` to initialize"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:982
msgid "the name of the GPU VA space"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:984
msgid "``enum drm_gpuvm_flags flags``"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:983
msgid "the :c:type:`drm_gpuvm_flags` for this GPUVM"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:984
msgid "the :c:type:`drm_device` this VM resides in"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:986
msgid "``struct drm_gem_object *r_obj``"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:985
msgid ""
"the resv :c:type:`drm_gem_object` providing the GPUVM's common :c:type:"
"`dma_resv`"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:987
msgid "``u64 start_offset``"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:986
msgid "the start offset of the GPU VA space"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:987
msgid "the size of the GPU VA space"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:989
msgid "``u64 reserve_offset``"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:988
msgid "the start of the kernel reserved GPU VA area"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:990
msgid "``u64 reserve_range``"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:989
msgid "the size of the kernel reserved GPU VA area"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:991
msgid "``const struct drm_gpuvm_ops *ops``"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:990
msgid ""
":c:type:`drm_gpuvm_ops` called on :c:type:`drm_gpuvm_sm_map` / :c:type:"
"`drm_gpuvm_sm_unmap`"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:991
msgid ""
"The :c:type:`drm_gpuvm` must be initialized with this function before use."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:993
msgid ""
"Note that **gpuvm** must be cleared to 0 before calling this function. The "
"given :c:type:`name` is expected to be managed by the surrounding driver "
"structures."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1074
msgid "drop a struct drm_gpuvm reference"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1075
msgid "the :c:type:`drm_gpuvm` to release the reference of"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1076
msgid "This releases a reference to **gpuvm**."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1078
msgid "This function may be called from atomic context."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1098
msgid "prepare the GPUVMs common dma-resv"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1101
#: drivers/gpu/drm/drm_gpuvm.c:1166 drivers/gpu/drm/drm_gpuvm.c:1202
#: drivers/gpu/drm/drm_gpuvm.c:1410 drivers/gpu/drm/drm_gpuvm.c:1436
#: drivers/gpu/drm/drm_gpuvm.c:2423 drivers/gpu/drm/drm_gpuvm.c:2501
#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:61
#: include/drm/drm_exec.h:133 ../../../gpu/drm-mm:547:
#: drivers/gpu/drm/drm_exec.c:77 drivers/gpu/drm/drm_exec.c:104
#: drivers/gpu/drm/drm_exec.c:122 drivers/gpu/drm/drm_exec.c:205
#: drivers/gpu/drm/drm_exec.c:260 drivers/gpu/drm/drm_exec.c:287
#: drivers/gpu/drm/drm_exec.c:317
msgid "``struct drm_exec *exec``"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1100
msgid "the :c:type:`drm_exec` context"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1102
#: drivers/gpu/drm/drm_gpuvm.c:1167 drivers/gpu/drm/drm_gpuvm.c:1205
#: drivers/gpu/drm/drm_gpuvm.c:2424 ../../../gpu/drm-mm:547:
#: drivers/gpu/drm/drm_exec.c:285 drivers/gpu/drm/drm_exec.c:316
msgid "``unsigned int num_fences``"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1101
#: drivers/gpu/drm/drm_gpuvm.c:1166 drivers/gpu/drm/drm_gpuvm.c:1204
msgid "the amount of :c:type:`dma_fences` to reserve"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1102
msgid ""
"Calls drm_exec_prepare_obj() for the GPUVMs dummy :c:type:`drm_gem_object`; "
"if **num_fences** is zero drm_exec_lock_obj() is called instead."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1105
#: drivers/gpu/drm/drm_gpuvm.c:1171
msgid ""
"Using this function directly, it is the drivers responsibility to call "
"drm_exec_init() and drm_exec_fini() accordingly."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1163
msgid "prepare all associated BOs"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1165
#: drivers/gpu/drm/drm_gpuvm.c:1201 drivers/gpu/drm/drm_gpuvm.c:1435
#: drivers/gpu/drm/drm_gpuvm.c:2422 drivers/gpu/drm/drm_gpuvm.c:2500
msgid "the :c:type:`drm_exec` locking context"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1167
msgid ""
"Calls drm_exec_prepare_obj() for all :c:type:`drm_gem_objects` the given :c:"
"type:`drm_gpuvm` contains mappings of; if **num_fences** is zero "
"drm_exec_lock_obj() is called instead."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1175
msgid ""
"This function is safe against concurrent insertion and removal of external "
"objects, however it is not safe against concurrent usage itself."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1178
msgid ""
"Drivers need to make sure to protect this case with either an outer VM lock "
"or by calling drm_gpuvm_prepare_vm() before this function within the "
"drm_exec_until_all_locked() loop, such that the GPUVM's dma-resv lock "
"ensures mutual exclusion."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1199
#: drivers/gpu/drm/drm_gpuvm.c:1327
msgid "prepare all BOs mapped within a given range"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1202
#: drivers/gpu/drm/drm_gpuvm.c:1329
msgid "the start address within the VA space"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1203
#: drivers/gpu/drm/drm_gpuvm.c:1330
msgid "the range to iterate within the VA space"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1205
msgid ""
"Calls drm_exec_prepare_obj() for all :c:type:`drm_gem_objects` mapped "
"between **addr** and **addr** + **range**; if **num_fences** is zero "
"drm_exec_lock_obj() is called instead."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1233
#: drivers/gpu/drm/drm_gpuvm.c:1296
msgid "lock all dma-resv of all associated BOs"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1235
msgid ""
"Acquires all dma-resv locks of all :c:type:`drm_gem_objects` the given :c:"
"type:`drm_gpuvm` contains mappings of."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1238
msgid ""
"Additionally, when calling this function with struct drm_gpuvm_exec::extra "
"being set the driver receives the given **fn** callback to lock additional "
"dma-resv in the context of the :c:type:`drm_gpuvm_exec` instance. Typically, "
"drivers would call drm_exec_prepare_obj() from within this callback."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1298
msgid "additional :c:type:`drm_gem_objects` to lock"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1300
msgid "``unsigned int num_objs``"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1299
msgid "the number of additional :c:type:`drm_gem_objects` to lock"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1300
msgid ""
"Acquires all dma-resv locks of all :c:type:`drm_gem_objects` the given :c:"
"type:`drm_gpuvm` contains mappings of, plus the ones given through **objs**."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1331
msgid ""
"Acquires all dma-resv locks of all :c:type:`drm_gem_objects` mapped between "
"**addr** and **addr** + **range**."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1408
msgid "the :c:type:`drm_gpuvm` to validate evicted BOs"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1409
msgid "the :c:type:`drm_exec` instance used for locking the GPUVM"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1410
msgid ""
"Calls the :c:type:`drm_gpuvm_ops`::vm_bo_validate callback for all evicted "
"buffer objects being mapped in the given :c:type:`drm_gpuvm`."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1432
msgid "add fence to private and all extobj dma-resv"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1434
msgid "the :c:type:`drm_gpuvm` to add a fence to"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1460
msgid "create a new instance of struct drm_gpuvm_bo"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1461
#: drivers/gpu/drm/drm_gpuvm.c:1571 drivers/gpu/drm/drm_gpuvm.c:1593
msgid "The :c:type:`drm_gpuvm` the **obj** is mapped in."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1462
#: drivers/gpu/drm/drm_gpuvm.c:1572 drivers/gpu/drm/drm_gpuvm.c:1594
msgid "The :c:type:`drm_gem_object` being mapped in the **gpuvm**."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1463
msgid ""
"If provided by the driver, this function uses the :c:type:`drm_gpuvm_ops` "
"vm_bo_alloc() callback to allocate."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1467
#: drivers/gpu/drm/drm_gpuvm.c:1578
msgid "a pointer to the :c:type:`drm_gpuvm_bo` on success, NULL on failure"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1528
msgid "drop a struct drm_gpuvm_bo reference"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1529
msgid "the :c:type:`drm_gpuvm_bo` to release the reference of"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1530
msgid "This releases a reference to **vm_bo**."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1532
msgid ""
"If the reference count drops to zero, the :c:type:`gpuvm_bo` is destroyed, "
"which includes removing it from the GEMs gpuva list. Hence, if a call to "
"this function can potentially let the reference count drop to zero the "
"caller must hold the dma-resv or driver specific GEM gpuva lock."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1537
msgid "This function may only be called from non-atomic context."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1540
msgid "true if vm_bo was destroyed, false otherwise."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1569
msgid ""
"find the :c:type:`drm_gpuvm_bo` for the given :c:type:`drm_gpuvm` and :c:"
"type:`drm_gem_object`"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1573
msgid ""
"Find the :c:type:`drm_gpuvm_bo` representing the combination of the given :c:"
"type:`drm_gpuvm` and :c:type:`drm_gem_object`. If found, increases the "
"reference count of the :c:type:`drm_gpuvm_bo` accordingly."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1591
#: drivers/gpu/drm/drm_gpuvm.c:1627
msgid ""
"obtains an instance of the :c:type:`drm_gpuvm_bo` for the given :c:type:"
"`drm_gpuvm` and :c:type:`drm_gem_object`"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1595
msgid ""
"Find the :c:type:`drm_gpuvm_bo` representing the combination of the given :c:"
"type:`drm_gpuvm` and :c:type:`drm_gem_object`. If found, increases the "
"reference count of the :c:type:`drm_gpuvm_bo` accordingly. If not found, "
"allocates a new :c:type:`drm_gpuvm_bo`."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1600
#: drivers/gpu/drm/drm_gpuvm.c:1636
msgid "A new :c:type:`drm_gpuvm_bo` is added to the GEMs gpuva list."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1603
msgid ""
"a pointer to the :c:type:`drm_gpuvm_bo` on success, an ERR_PTR on failure"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1633
msgid "``struct drm_gpuvm_bo *__vm_bo``"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1629
msgid "A pre-allocated struct drm_gpuvm_bo."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1630
msgid ""
"Find the :c:type:`drm_gpuvm_bo` representing the combination of the given :c:"
"type:`drm_gpuvm` and :c:type:`drm_gem_object`. If found, increases the "
"reference count of the found :c:type:`drm_gpuvm_bo` accordingly, while the "
"**__vm_bo** reference count is decreased. If not found **__vm_bo** is "
"returned without further increase of the reference count."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1639
msgid ""
"a pointer to the found :c:type:`drm_gpuvm_bo` or **__vm_bo** if no existing :"
"c:type:`drm_gpuvm_bo` was found"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1663
msgid ""
"adds the :c:type:`drm_gpuvm_bo` to its :c:type:`drm_gpuvm`'s extobj list"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1665
msgid ""
"The :c:type:`drm_gpuvm_bo` to add to its :c:type:`drm_gpuvm`'s the extobj "
"list."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1666
msgid ""
"Adds the given **vm_bo** to its :c:type:`drm_gpuvm`'s extobj list if not on "
"the list already and if the corresponding :c:type:`drm_gem_object` is an "
"external object, actually."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1686
msgid ""
"add / remove a :c:type:`drm_gpuvm_bo` to / from the :c:type:`drm_gpuvms` "
"evicted list"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1688
msgid "the :c:type:`drm_gpuvm_bo` to add or remove"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1689
msgid "indicates whether the object is evicted"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1690
msgid ""
"Adds a :c:type:`drm_gpuvm_bo` to or removes it from the :c:type:"
"`drm_gpuvm`'s evicted list."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1745
msgid "insert a :c:type:`drm_gpuva`"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1746
msgid "the :c:type:`drm_gpuvm` to insert the :c:type:`drm_gpuva` in"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1747
#: drivers/gpu/drm/drm_gpuvm.c:1994
msgid "the :c:type:`drm_gpuva` to insert"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1748
msgid ""
"Insert a :c:type:`drm_gpuva` with a given address and range into a :c:type:"
"`drm_gpuvm`."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1751
#: drivers/gpu/drm/drm_gpuvm.c:1794
msgid ""
"It is safe to use this function using the safe versions of iterating the GPU "
"VA space, such as drm_gpuvm_for_each_va_safe() and "
"drm_gpuvm_for_each_va_range_safe()."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1790
msgid "remove a :c:type:`drm_gpuva`"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1791
msgid "the :c:type:`drm_gpuva` to remove"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1792
msgid "This removes the given :c:type:`va` from the underlying tree."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1816
msgid "link a :c:type:`drm_gpuva`"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1817
msgid "the :c:type:`drm_gpuva` to link"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1818
msgid "the :c:type:`drm_gpuvm_bo` to add the :c:type:`drm_gpuva` to"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1819
msgid ""
"This adds the given :c:type:`va` to the GPU VA list of the :c:type:"
"`drm_gpuvm_bo` and the :c:type:`drm_gpuvm_bo` to the :c:type:"
"`drm_gem_object` it is associated with."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1822
msgid ""
"For every :c:type:`drm_gpuva` entry added to the :c:type:`drm_gpuvm_bo` an "
"additional reference of the latter is taken."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1825
#: drivers/gpu/drm/drm_gpuvm.c:1861
msgid ""
"This function expects the caller to protect the GEM's GPUVA list against "
"concurrent access using either the GEMs dma_resv lock or a driver specific "
"lock set through drm_gem_gpuva_set_lock()."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1849
msgid "unlink a :c:type:`drm_gpuva`"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1850
msgid "the :c:type:`drm_gpuva` to unlink"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1851
msgid ""
"This removes the given :c:type:`va` from the GPU VA list of the :c:type:"
"`drm_gem_object` it is associated with."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1854
msgid ""
"This removes the given :c:type:`va` from the GPU VA list of the :c:type:"
"`drm_gpuvm_bo` and the :c:type:`drm_gpuvm_bo` from the :c:type:"
"`drm_gem_object` it is associated with in case this call unlinks the last :c:"
"type:`drm_gpuva` from the :c:type:`drm_gpuvm_bo`."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1858
msgid ""
"For every :c:type:`drm_gpuva` entry removed from the :c:type:`drm_gpuvm_bo` "
"a reference of the latter is dropped."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1884
msgid "find the first :c:type:`drm_gpuva` in the given range"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1885
#: drivers/gpu/drm/drm_gpuvm.c:1903 drivers/gpu/drm/drm_gpuvm.c:1932
#: drivers/gpu/drm/drm_gpuvm.c:1954
msgid "the :c:type:`drm_gpuvm` to search in"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1886
#: drivers/gpu/drm/drm_gpuvm.c:1904
msgid "the :c:type:`drm_gpuvas` address"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1887
#: drivers/gpu/drm/drm_gpuvm.c:1905
msgid "the :c:type:`drm_gpuvas` range"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1889
msgid "the first :c:type:`drm_gpuva` within the given range"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1902
msgid "find a :c:type:`drm_gpuva`"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1907
msgid ""
"the :c:type:`drm_gpuva` at a given :c:type:`addr` and with a given :c:type:"
"`range`"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1931
msgid "find the :c:type:`drm_gpuva` before the given address"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1933
msgid "the given GPU VA's start address"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1934
msgid ""
"Find the adjacent :c:type:`drm_gpuva` before the GPU VA with given :c:type:"
"`start` address."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1936
#: drivers/gpu/drm/drm_gpuvm.c:1958
msgid ""
"Note that if there is any free space between the GPU VA mappings no mapping "
"is returned."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1940
#: drivers/gpu/drm/drm_gpuvm.c:1962
msgid "a pointer to the found :c:type:`drm_gpuva` or NULL if none was found"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1953
msgid "find the :c:type:`drm_gpuva` after the given address"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1955
msgid "the given GPU VA's end address"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1956
msgid ""
"Find the adjacent :c:type:`drm_gpuva` after the GPU VA with given :c:type:"
"`end` address."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1975
msgid "indicate whether a given interval of the VA space is empty"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1977
msgid "the :c:type:`drm_gpuvm` to check the range for"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1978
msgid "the start address of the range"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1979
msgid "the range of the interval"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1981
msgid "true if the interval is empty, false otherwise"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1991
msgid ""
"helper to insert a :c:type:`drm_gpuva` according to a :c:type:"
"`drm_gpuva_op_map`"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1996
msgid "``struct drm_gpuva_op_map *op``"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1995
msgid "the :c:type:`drm_gpuva_op_map` to initialize **va** with"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:1996
msgid ""
"Initializes the **va** from the **op** and inserts it into the given "
"**gpuvm**."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2010
msgid ""
"helper to remap a :c:type:`drm_gpuva` according to a :c:type:"
"`drm_gpuva_op_remap`"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2016
msgid "``struct drm_gpuva *prev``"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2012
msgid "the :c:type:`drm_gpuva` to remap when keeping the start of a mapping"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2014
msgid "``struct drm_gpuva *next``"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2013
msgid "the :c:type:`drm_gpuva` to remap when keeping the end of a mapping"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2015
msgid "``struct drm_gpuva_op_remap *op``"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2014
msgid ""
"the :c:type:`drm_gpuva_op_remap` to initialize **prev** and **next** with"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2015
msgid ""
"Removes the currently mapped :c:type:`drm_gpuva` and remaps it using "
"**prev** and/or **next**."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2042
msgid ""
"helper to remove a :c:type:`drm_gpuva` according to a :c:type:"
"`drm_gpuva_op_unmap`"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2048
msgid "``struct drm_gpuva_op_unmap *op``"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2044
msgid ""
"the :c:type:`drm_gpuva_op_unmap` specifying the :c:type:`drm_gpuva` to remove"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2045
msgid ""
"Removes the :c:type:`drm_gpuva` associated with the :c:type:"
"`drm_gpuva_op_unmap`."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2303
msgid "calls the :c:type:`drm_gpuva_op` split/merge steps"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2304
#: drivers/gpu/drm/drm_gpuvm.c:2354 drivers/gpu/drm/drm_gpuvm.c:2421
#: drivers/gpu/drm/drm_gpuvm.c:2499 drivers/gpu/drm/drm_gpuvm.c:2613
#: drivers/gpu/drm/drm_gpuvm.c:2681 drivers/gpu/drm/drm_gpuvm.c:2742
msgid "the :c:type:`drm_gpuvm` representing the GPU VA space"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2306
#: drivers/gpu/drm/drm_gpuvm.c:2356
msgid "``void *priv``"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2305
#: drivers/gpu/drm/drm_gpuvm.c:2355
msgid "pointer to a driver private data structure"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2307
#: drivers/gpu/drm/drm_gpuvm.c:2357 drivers/gpu/drm/drm_gpuvm.c:2425
#: drivers/gpu/drm/drm_gpuvm.c:2502 drivers/gpu/drm/drm_gpuvm.c:2615
#: drivers/gpu/drm/drm_gpuvm.c:2683
msgid "``u64 req_addr``"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2306
#: drivers/gpu/drm/drm_gpuvm.c:2614
msgid "the start address of the new mapping"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2308
#: drivers/gpu/drm/drm_gpuvm.c:2358 drivers/gpu/drm/drm_gpuvm.c:2426
#: drivers/gpu/drm/drm_gpuvm.c:2503 drivers/gpu/drm/drm_gpuvm.c:2616
#: drivers/gpu/drm/drm_gpuvm.c:2684
msgid "``u64 req_range``"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2309
#: drivers/gpu/drm/drm_gpuvm.c:2427 drivers/gpu/drm/drm_gpuvm.c:2617
msgid "``struct drm_gem_object *req_obj``"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2310
#: drivers/gpu/drm/drm_gpuvm.c:2428 drivers/gpu/drm/drm_gpuvm.c:2618
msgid "``u64 req_offset``"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2310
msgid ""
"This function iterates the given range of the GPU VA space. It utilizes the :"
"c:type:`drm_gpuvm_ops` to call back into the driver providing the split and "
"merge steps."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2314
msgid ""
"Drivers may use these callbacks to update the GPU VA space right away within "
"the callback. In case the driver decides to copy and store the operations "
"for later processing neither this function nor :c:type:`drm_gpuvm_sm_unmap` "
"is allowed to be called before the :c:type:`drm_gpuvm`'s view of the GPU VA "
"space was updated with the previous set of operations. To update the :c:type:"
"`drm_gpuvm`'s view of the GPU VA space drm_gpuva_insert(), "
"drm_gpuva_destroy_locked() and/or drm_gpuva_destroy_unlocked() should be "
"used."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2323
msgid ""
"A sequence of callbacks can contain map, unmap and remap operations, but the "
"sequence of callbacks might also be empty if no operation is required, e.g. "
"if the requested mapping already exists in the exact same way."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2327
#: drivers/gpu/drm/drm_gpuvm.c:2626
msgid ""
"There can be an arbitrary amount of unmap operations, a maximum of two remap "
"operations and a single map operation. The latter one represents the "
"original map operation requested by the caller."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2332
#: drivers/gpu/drm/drm_gpuvm.c:2377 drivers/gpu/drm/drm_gpuvm.c:2476
#: drivers/gpu/drm/drm_gpuvm.c:2509
msgid "0 on success or a negative error code"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2353
msgid "calls the :c:type:`drm_gpuva_ops` to split on unmap"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2356
#: drivers/gpu/drm/drm_gpuvm.c:2424 drivers/gpu/drm/drm_gpuvm.c:2501
#: drivers/gpu/drm/drm_gpuvm.c:2682
msgid "the start address of the range to unmap"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2357
#: drivers/gpu/drm/drm_gpuvm.c:2425 drivers/gpu/drm/drm_gpuvm.c:2502
#: drivers/gpu/drm/drm_gpuvm.c:2683
msgid "the range of the mappings to unmap"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2358
msgid ""
"This function iterates the given range of the GPU VA space. It utilizes the :"
"c:type:`drm_gpuvm_ops` to call back into the driver providing the operations "
"to unmap and, if required, split existing mappings."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2362
msgid ""
"Drivers may use these callbacks to update the GPU VA space right away within "
"the callback. In case the driver decides to copy and store the operations "
"for later processing neither this function nor :c:type:`drm_gpuvm_sm_map` is "
"allowed to be called before the :c:type:`drm_gpuvm`'s view of the GPU VA "
"space was updated with the previous set of operations. To update the :c:type:"
"`drm_gpuvm`'s view of the GPU VA space drm_gpuva_insert(), "
"drm_gpuva_destroy_locked() and/or drm_gpuva_destroy_unlocked() should be "
"used."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2370
msgid ""
"A sequence of callbacks can contain unmap and remap operations, depending on "
"whether there are actual overlapping mappings to split."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2373
#: drivers/gpu/drm/drm_gpuvm.c:2691
msgid ""
"There can be an arbitrary amount of unmap operations and a maximum of two "
"remap operations."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2420
msgid "locks the objects touched by a drm_gpuvm_sm_map()"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2423
msgid "for newly mapped objects, the # of fences to reserve"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2428
msgid ""
"This function locks (drm_exec_lock_obj()) objects that will be unmapped/ "
"remapped, and locks+prepares (drm_exec_prepare_object()) objects that will "
"be newly mapped."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2432
msgid "The expected usage is::"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2460
msgid ""
"This enables all locking to be performed before the driver begins modifying "
"the VM.  This is safe to do in the case of overlapping DRIVER_VM_BIND_OPs, "
"where an earlier op can alter the sequence of steps generated for a later "
"op, because the later altered step will involve the same GEM object(s) "
"already seen in the earlier locking step.  For example:"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2466
msgid ""
"An earlier driver DRIVER_OP_UNMAP op removes the need for a "
"DRM_GPUVA_OP_REMAP/UNMAP step.  This is safe because we've already locked "
"the GEM object in the earlier DRIVER_OP_UNMAP op."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2470
msgid ""
"An earlier DRIVER_OP_MAP op overlaps with a later DRIVER_OP_MAP/UNMAP op, "
"introducing a DRM_GPUVA_OP_REMAP/UNMAP that wouldn't have been required "
"without the earlier DRIVER_OP_MAP.  This is safe because we've already "
"locked the GEM object in the earlier DRIVER_OP_MAP step."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2498
msgid "locks the objects touched by drm_gpuvm_sm_unmap()"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2503
msgid ""
"This function locks (drm_exec_lock_obj()) objects that will be unmapped/ "
"remapped by drm_gpuvm_sm_unmap()."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2506
msgid "See drm_gpuvm_sm_map_exec_lock() for expected usage."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2612
msgid "creates the :c:type:`drm_gpuva_ops` to split and merge"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2618
msgid ""
"This function creates a list of operations to perform splitting and merging "
"of existing mapping(s) with the newly requested one."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2621
msgid ""
"The list can be iterated with :c:type:`drm_gpuva_for_each_op` and must be "
"processed in the given order. It can contain map, unmap and remap "
"operations, but it also can be empty if no operation is required, e.g. if "
"the requested mapping already exists in the exact same way."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2630
msgid ""
"Note that before calling this function again with another mapping request it "
"is necessary to update the :c:type:`drm_gpuvm`'s view of the GPU VA space. "
"The previously obtained operations must be either processed or abandoned. To "
"update the :c:type:`drm_gpuvm`'s view of the GPU VA space "
"drm_gpuva_insert(), drm_gpuva_destroy_locked() and/or "
"drm_gpuva_destroy_unlocked() should be used."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2637
#: drivers/gpu/drm/drm_gpuvm.c:2701 drivers/gpu/drm/drm_gpuvm.c:2752
#: drivers/gpu/drm/drm_gpuvm.c:2803
msgid ""
"After the caller finished processing the returned :c:type:`drm_gpuva_ops`, "
"they must be freed with :c:type:`drm_gpuva_ops_free`."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2641
#: drivers/gpu/drm/drm_gpuvm.c:2705 drivers/gpu/drm/drm_gpuvm.c:2756
#: drivers/gpu/drm/drm_gpuvm.c:2810
msgid ""
"a pointer to the :c:type:`drm_gpuva_ops` on success, an ERR_PTR on failure"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2679
msgid "creates the :c:type:`drm_gpuva_ops` to split on unmap"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2684
msgid ""
"This function creates a list of operations to perform unmapping and, if "
"required, splitting of the mappings overlapping the unmap range."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2687
msgid ""
"The list can be iterated with :c:type:`drm_gpuva_for_each_op` and must be "
"processed in the given order. It can contain unmap and remap operations, "
"depending on whether there are actual overlapping mappings to split."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2694
msgid ""
"Note that before calling this function again with another range to unmap it "
"is necessary to update the :c:type:`drm_gpuvm`'s view of the GPU VA space. "
"The previously obtained operations must be processed or abandoned. To update "
"the :c:type:`drm_gpuvm`'s view of the GPU VA space drm_gpuva_insert(), "
"drm_gpuva_destroy_locked() and/or drm_gpuva_destroy_unlocked() should be "
"used."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2741
msgid "creates the :c:type:`drm_gpuva_ops` to prefetch"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2743
msgid "the start address of the range to prefetch"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2744
msgid "the range of the mappings to prefetch"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2745
msgid "This function creates a list of operations to perform prefetching."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2747
msgid ""
"The list can be iterated with :c:type:`drm_gpuva_for_each_op` and must be "
"processed in the given order. It can contain prefetch operations."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2750
msgid "There can be an arbitrary amount of prefetch operations."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2795
msgid "creates the :c:type:`drm_gpuva_ops` to unmap a GEM"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2796
msgid "the :c:type:`drm_gpuvm_bo` abstraction"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2797
msgid ""
"This function creates a list of operations to perform unmapping for every "
"GPUVA attached to a GEM."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2800
msgid ""
"The list can be iterated with :c:type:`drm_gpuva_for_each_op` and consists "
"out of an arbitrary amount of unmap operations."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2806
msgid ""
"It is the callers responsibility to protect the GEMs GPUVA list against "
"concurrent access using the GEMs dma_resv lock."
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2849
msgid "free the given :c:type:`drm_gpuva_ops`"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2850
msgid "the :c:type:`drm_gpuvm` the ops were created for"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2852
msgid "``struct drm_gpuva_ops *ops``"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2851
msgid "the :c:type:`drm_gpuva_ops` to free"
msgstr ""

#: ../../../gpu/drm-mm:506: drivers/gpu/drm/drm_gpuvm.c:2852
msgid ""
"Frees the given :c:type:`drm_gpuva_ops` structure including all the ops "
"associated with it."
msgstr ""

#: ../../../gpu/drm-mm.rst:510
msgid "DRM Buddy Allocator"
msgstr ""

#: ../../../gpu/drm-mm.rst:513
msgid "DRM Buddy Function References"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:226
msgid "init memory manager"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:232
#: drivers/gpu/drm/drm_buddy.c:328 drivers/gpu/drm/drm_buddy.c:415
#: drivers/gpu/drm/drm_buddy.c:458 drivers/gpu/drm/drm_buddy.c:507
#: drivers/gpu/drm/drm_buddy.c:906 drivers/gpu/drm/drm_buddy.c:1013
#: drivers/gpu/drm/drm_buddy.c:1179 drivers/gpu/drm/drm_buddy.c:1197
msgid "``struct drm_buddy *mm``"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:228
msgid "DRM buddy manager to initialize"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:229
msgid "size in bytes to manage"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:231
msgid "``u64 chunk_size``"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:230
msgid "minimum page size in bytes for our allocations"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:231
msgid "Initializes the memory manager and its resources."
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:234
#: drivers/gpu/drm/drm_buddy.c:915 drivers/gpu/drm/drm_buddy.c:1023
msgid "0 on success, error code on failure."
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:322
msgid "tear down the memory manager"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:324
msgid "DRM buddy manager to free"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:325
msgid "Cleanup memory manager resources and the freelist"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:392
msgid "get buddy address"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:398
#: drivers/gpu/drm/drm_buddy.c:456 drivers/gpu/drm/drm_buddy.c:1177
msgid "``struct drm_buddy_block *block``"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:394
#: drivers/gpu/drm/drm_buddy.c:1176
msgid "DRM buddy block"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:395
msgid ""
"Returns the corresponding buddy block for **block**, or NULL if this is a "
"root block and can't be merged further. Requires some kind of locking to "
"protect against any concurrent allocate and free operations."
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:409
msgid "reset blocks clear state"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:411
#: drivers/gpu/drm/drm_buddy.c:454 drivers/gpu/drm/drm_buddy.c:503
#: drivers/gpu/drm/drm_buddy.c:902 drivers/gpu/drm/drm_buddy.c:1175
#: drivers/gpu/drm/drm_buddy.c:1193
msgid "DRM buddy manager"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:413
msgid "``bool is_clear``"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:412
msgid "blocks clear state"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:413
msgid ""
"Reset the clear state based on **is_clear** value for each block in the "
"freelist."
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:452
msgid "free a block"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:455
msgid "block to be freed"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:501
msgid "free blocks"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:505
msgid "``struct list_head *objects``"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:504
msgid "input list head to free blocks"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:506
msgid "``unsigned int flags``"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:505
msgid "optional flags like DRM_BUDDY_CLEARED"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:900
msgid "free unused pages"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:904
msgid "``u64 *start``"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:903
msgid "start address to begin the trimming."
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:905
msgid "``u64 new_size``"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:904
msgid "original size requested"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:906
#: drivers/gpu/drm/drm_buddy.c:1015
msgid "``struct list_head *blocks``"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:905
msgid ""
"Input and output list of allocated blocks. MUST contain single block as "
"input to be trimmed. On success will contain the newly allocated blocks "
"making up the **new_size**. Blocks always appear in ascending order"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:910
msgid ""
"For contiguous allocation, we round up the size to the nearest power of two "
"value, drivers consume *actual* size, so remaining portions are unused and "
"can be optionally freed with this function"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:1007
msgid "allocate power-of-two blocks"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:1009
msgid "DRM buddy manager to allocate from"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:1010
msgid "start of the allowed range for this block"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:1011
msgid "end of the allowed range for this block"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:1012
msgid "size of the allocation in bytes"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:1014
msgid "``u64 min_block_size``"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:1014
msgid "output list head to add allocated blocks"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:1015
msgid "DRM_BUDDY_*_ALLOCATION flags"
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:1016
msgid ""
"alloc_range_bias() called on range limitations, which traverses the tree and "
"returns the desired block."
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:1019
msgid ""
"alloc_from_freelist() called when *no* range restrictions are enforced, "
"which picks the block from the freelist."
msgstr ""

#: ../../../gpu/drm-mm:515: drivers/gpu/drm/drm_buddy.c:1173
msgid "print block information"
msgstr ""

#: ../../../gpu/drm-mm.rst:519
msgid "DRM Cache Handling and Fast WC memcpy()"
msgstr ""

#: ../../../gpu/drm-mm:521: drivers/gpu/drm/drm_cache.c:79
msgid "Flush dcache lines of a set of pages."
msgstr ""

#: ../../../gpu/drm-mm:521: drivers/gpu/drm/drm_cache.c:85
msgid "``struct page *pages[]``"
msgstr ""

#: ../../../gpu/drm-mm:521: drivers/gpu/drm/drm_cache.c:80
msgid "List of pages to be flushed."
msgstr ""

#: ../../../gpu/drm-mm:521: drivers/gpu/drm/drm_cache.c:82
msgid "``unsigned long num_pages``"
msgstr ""

#: ../../../gpu/drm-mm:521: drivers/gpu/drm/drm_cache.c:81
msgid "Number of pages in the array."
msgstr ""

#: ../../../gpu/drm-mm:521: drivers/gpu/drm/drm_cache.c:82
msgid ""
"Flush every data cache line entry that points to an address belonging to a "
"page in the array."
msgstr ""

#: ../../../gpu/drm-mm:521: drivers/gpu/drm/drm_cache.c:120
msgid "Flush dcache lines pointing to a scather-gather."
msgstr ""

#: ../../../gpu/drm-mm:521: drivers/gpu/drm/drm_cache.c:121
msgid "struct sg_table."
msgstr ""

#: ../../../gpu/drm-mm:521: drivers/gpu/drm/drm_cache.c:122
msgid "Flush every data cache line entry that points to an address in the sg."
msgstr ""

#: ../../../gpu/drm-mm:521: drivers/gpu/drm/drm_cache.c:149
msgid "Flush dcache lines of a region"
msgstr ""

#: ../../../gpu/drm-mm:521: drivers/gpu/drm/drm_cache.c:155
msgid "``void *addr``"
msgstr ""

#: ../../../gpu/drm-mm:521: drivers/gpu/drm/drm_cache.c:150
msgid "Initial kernel memory address."
msgstr ""

#: ../../../gpu/drm-mm:521: drivers/gpu/drm/drm_cache.c:152
msgid "``unsigned long length``"
msgstr ""

#: ../../../gpu/drm-mm:521: drivers/gpu/drm/drm_cache.c:151
msgid "Region size."
msgstr ""

#: ../../../gpu/drm-mm:521: drivers/gpu/drm/drm_cache.c:152
msgid ""
"Flush every data cache line entry that points to an address in the region "
"requested."
msgstr ""

#: ../../../gpu/drm-mm:521: drivers/gpu/drm/drm_cache.c:293
msgid "Perform the fastest available memcpy from a source that may be WC."
msgstr ""

#: ../../../gpu/drm-mm:521: drivers/gpu/drm/drm_cache.c:299
msgid "``struct iosys_map *dst``"
msgstr ""

#: ../../../gpu/drm-mm:521: drivers/gpu/drm/drm_cache.c:295
msgid "The destination pointer"
msgstr ""

#: ../../../gpu/drm-mm:521: drivers/gpu/drm/drm_cache.c:297
msgid "``const struct iosys_map *src``"
msgstr ""

#: ../../../gpu/drm-mm:521: drivers/gpu/drm/drm_cache.c:296
msgid "The source pointer"
msgstr ""

#: ../../../gpu/drm-mm:521: drivers/gpu/drm/drm_cache.c:297
msgid "The size of the area o transfer in bytes"
msgstr ""

#: ../../../gpu/drm-mm:521: drivers/gpu/drm/drm_cache.c:298
msgid ""
"Tries an arch optimized memcpy for prefetching reading out of a WC region, "
"and if no such beast is available, falls back to a normal memcpy."
msgstr ""

#: ../../../gpu/drm-mm.rst:527
msgid "DRM Sync Objects"
msgstr ""

#: ../../../gpu/drm-mm:529: drivers/gpu/drm/drm_syncobj.c:30
msgid ""
"DRM synchronisation objects (syncobj, see struct :c:type:`drm_syncobj`) "
"provide a container for a synchronization primitive which can be used by "
"userspace to explicitly synchronize GPU commands, can be shared between "
"userspace processes, and can be shared between different DRM drivers. Their "
"primary use-case is to implement Vulkan fences and semaphores. The syncobj "
"userspace API provides ioctls for several operations:"
msgstr ""

#: ../../../gpu/drm-mm:529: drivers/gpu/drm/drm_syncobj.c:37
msgid "Creation and destruction of syncobjs"
msgstr ""

#: ../../../gpu/drm-mm:529: drivers/gpu/drm/drm_syncobj.c:38
msgid "Import and export of syncobjs to/from a syncobj file descriptor"
msgstr ""

#: ../../../gpu/drm-mm:529: drivers/gpu/drm/drm_syncobj.c:39
msgid "Import and export a syncobj's underlying fence to/from a sync file"
msgstr ""

#: ../../../gpu/drm-mm:529: drivers/gpu/drm/drm_syncobj.c:40
msgid "Reset a syncobj (set its fence to NULL)"
msgstr ""

#: ../../../gpu/drm-mm:529: drivers/gpu/drm/drm_syncobj.c:41
msgid "Signal a syncobj (set a trivially signaled fence)"
msgstr ""

#: ../../../gpu/drm-mm:529: drivers/gpu/drm/drm_syncobj.c:42
msgid "Wait for a syncobj's fence to appear and be signaled"
msgstr ""

#: ../../../gpu/drm-mm:529: drivers/gpu/drm/drm_syncobj.c:44
msgid ""
"The syncobj userspace API also provides operations to manipulate a syncobj "
"in terms of a timeline of struct :c:type:`dma_fence_chain` rather than a "
"single struct :c:type:`dma_fence`, through the following operations:"
msgstr ""

#: ../../../gpu/drm-mm:529: drivers/gpu/drm/drm_syncobj.c:48
msgid "Signal a given point on the timeline"
msgstr ""

#: ../../../gpu/drm-mm:529: drivers/gpu/drm/drm_syncobj.c:49
msgid "Wait for a given point to appear and/or be signaled"
msgstr ""

#: ../../../gpu/drm-mm:529: drivers/gpu/drm/drm_syncobj.c:50
msgid "Import and export from/to a given point of a timeline"
msgstr ""

#: ../../../gpu/drm-mm:529: drivers/gpu/drm/drm_syncobj.c:52
msgid ""
"At it's core, a syncobj is simply a wrapper around a pointer to a struct :c:"
"type:`dma_fence` which may be NULL. When a syncobj is first created, its "
"pointer is either NULL or a pointer to an already signaled fence depending "
"on whether the :c:type:`DRM_SYNCOBJ_CREATE_SIGNALED` flag is passed to :c:"
"type:`DRM_IOCTL_SYNCOBJ_CREATE`."
msgstr ""

#: ../../../gpu/drm-mm:529: drivers/gpu/drm/drm_syncobj.c:59
msgid ""
"If the syncobj is considered as a binary (its state is either signaled or "
"unsignaled) primitive, when GPU work is enqueued in a DRM driver to signal "
"the syncobj, the syncobj's fence is replaced with a fence which will be "
"signaled by the completion of that work. If the syncobj is considered as a "
"timeline primitive, when GPU work is enqueued in a DRM driver to signal the "
"a given point of the syncobj, a new struct :c:type:`dma_fence_chain` "
"pointing to the DRM driver's fence and also pointing to the previous fence "
"that was in the syncobj. The new struct :c:type:`dma_fence_chain` fence "
"replace the syncobj's fence and will be signaled by completion of the DRM "
"driver's work and also any work associated with the fence previously in the "
"syncobj."
msgstr ""

#: ../../../gpu/drm-mm:529: drivers/gpu/drm/drm_syncobj.c:71
msgid ""
"When GPU work which waits on a syncobj is enqueued in a DRM driver, at the "
"time the work is enqueued, it waits on the syncobj's fence before submitting "
"the work to hardware. That fence is either :"
msgstr ""

#: ../../../gpu/drm-mm:529: drivers/gpu/drm/drm_syncobj.c:75
msgid ""
"The syncobj's current fence if the syncobj is considered as a binary "
"primitive."
msgstr ""

#: ../../../gpu/drm-mm:529: drivers/gpu/drm/drm_syncobj.c:77
msgid ""
"The struct :c:type:`dma_fence` associated with a given point if the syncobj "
"is considered as a timeline primitive."
msgstr ""

#: ../../../gpu/drm-mm:529: drivers/gpu/drm/drm_syncobj.c:80
msgid ""
"If the syncobj's fence is NULL or not present in the syncobj's timeline, the "
"enqueue operation is expected to fail."
msgstr ""

#: ../../../gpu/drm-mm:529: drivers/gpu/drm/drm_syncobj.c:83
msgid ""
"With binary syncobj, all manipulation of the syncobjs's fence happens in "
"terms of the current fence at the time the ioctl is called by userspace "
"regardless of whether that operation is an immediate host-side operation "
"(signal or reset) or or an operation which is enqueued in some driver "
"queue. :c:type:`DRM_IOCTL_SYNCOBJ_RESET` and :c:type:"
"`DRM_IOCTL_SYNCOBJ_SIGNAL` can be used to manipulate a syncobj from the host "
"by resetting its pointer to NULL or setting its pointer to a fence which is "
"already signaled."
msgstr ""

#: ../../../gpu/drm-mm:529: drivers/gpu/drm/drm_syncobj.c:91
msgid ""
"With a timeline syncobj, all manipulation of the synobj's fence happens in "
"terms of a u64 value referring to point in the timeline. See "
"dma_fence_chain_find_seqno() to see how a given point is found in the "
"timeline."
msgstr ""

#: ../../../gpu/drm-mm:529: drivers/gpu/drm/drm_syncobj.c:96
msgid ""
"Note that applications should be careful to always use timeline set of "
"ioctl() when dealing with syncobj considered as timeline. Using a binary set "
"of ioctl() with a syncobj considered as timeline could result incorrect "
"synchronization. The use of binary syncobj is supported through the timeline "
"set of ioctl() by using a point value of 0, this will reproduce the behavior "
"of the binary set of ioctl() (for example replace the syncobj's fence when "
"signaling)."
msgstr ""

#: ../../../gpu/drm-mm:529: drivers/gpu/drm/drm_syncobj.c:108
msgid ""
":c:type:`DRM_IOCTL_SYNCOBJ_WAIT` takes an array of syncobj handles and does "
"a host-side wait on all of the syncobj fences simultaneously. If :c:type:"
"`DRM_SYNCOBJ_WAIT_FLAGS_WAIT_ALL` is set, the wait ioctl will wait on all of "
"the syncobj fences to be signaled before it returns. Otherwise, it returns "
"once at least one syncobj fence has been signaled and the index of a "
"signaled fence is written back to the client."
msgstr ""

#: ../../../gpu/drm-mm:529: drivers/gpu/drm/drm_syncobj.c:115
msgid ""
"Unlike the enqueued GPU work dependencies which fail if they see a NULL "
"fence in a syncobj, if :c:type:`DRM_SYNCOBJ_WAIT_FLAGS_WAIT_FOR_SUBMIT` is "
"set, the host-side wait will first wait for the syncobj to receive a non-"
"NULL fence and then wait on that fence. If :c:type:"
"`DRM_SYNCOBJ_WAIT_FLAGS_WAIT_FOR_SUBMIT` is not set and any one of the "
"syncobjs in the array has a NULL fence, -EINVAL will be returned. Assuming "
"the syncobj starts off with a NULL fence, this allows a client to do a host "
"wait in one thread (or process) which waits on GPU work submitted in another "
"thread (or process) without having to manually synchronize between the two. "
"This requirement is inherited from the Vulkan fence API."
msgstr ""

#: ../../../gpu/drm-mm:529: drivers/gpu/drm/drm_syncobj.c:127
msgid ""
"If :c:type:`DRM_SYNCOBJ_WAIT_FLAGS_WAIT_DEADLINE` is set, the ioctl will "
"also set a fence deadline hint on the backing fences before waiting, to "
"provide the fence signaler with an appropriate sense of urgency.  The "
"deadline is specified as an absolute :c:type:`CLOCK_MONOTONIC` value in "
"units of ns."
msgstr ""

#: ../../../gpu/drm-mm:529: drivers/gpu/drm/drm_syncobj.c:132
msgid ""
"Similarly, :c:type:`DRM_IOCTL_SYNCOBJ_TIMELINE_WAIT` takes an array of "
"syncobj handles as well as an array of u64 points and does a host-side wait "
"on all of syncobj fences at the given points simultaneously."
msgstr ""

#: ../../../gpu/drm-mm:529: drivers/gpu/drm/drm_syncobj.c:136
msgid ""
":c:type:`DRM_IOCTL_SYNCOBJ_TIMELINE_WAIT` also adds the ability to wait for "
"a given fence to materialize on the timeline without waiting for the fence "
"to be signaled by using the :c:type:`DRM_SYNCOBJ_WAIT_FLAGS_WAIT_AVAILABLE` "
"flag. This requirement is inherited from the wait-before-signal behavior "
"required by the Vulkan timeline semaphore API."
msgstr ""

#: ../../../gpu/drm-mm:529: drivers/gpu/drm/drm_syncobj.c:142
msgid ""
"Alternatively, :c:type:`DRM_IOCTL_SYNCOBJ_EVENTFD` can be used to wait "
"without blocking: an eventfd will be signaled when the syncobj is. This is "
"useful to integrate the wait in an event loop."
msgstr ""

#: ../../../gpu/drm-mm:529: drivers/gpu/drm/drm_syncobj.c:150
msgid ""
":c:type:`DRM_IOCTL_SYNCOBJ_FD_TO_HANDLE` and :c:type:"
"`DRM_IOCTL_SYNCOBJ_HANDLE_TO_FD` provide two mechanisms for import/export of "
"syncobjs."
msgstr ""

#: ../../../gpu/drm-mm:529: drivers/gpu/drm/drm_syncobj.c:153
msgid ""
"The first lets the client import or export an entire syncobj to a file "
"descriptor. These fd's are opaque and have no other use case, except passing "
"the syncobj between processes. All exported file descriptors and any syncobj "
"handles created as a result of importing those file descriptors own a "
"reference to the same underlying struct :c:type:`drm_syncobj` and the "
"syncobj can be used persistently across all the processes with which it is "
"shared. The syncobj is freed only once the last reference is dropped. Unlike "
"dma-buf, importing a syncobj creates a new handle (with its own reference) "
"for every import instead of de-duplicating. The primary use-case of this "
"persistent import/export is for shared Vulkan fences and semaphores."
msgstr ""

#: ../../../gpu/drm-mm:529: drivers/gpu/drm/drm_syncobj.c:167
msgid ""
"The second import/export mechanism, which is indicated by :c:type:"
"`DRM_SYNCOBJ_FD_TO_HANDLE_FLAGS_IMPORT_SYNC_FILE` or :c:type:"
"`DRM_SYNCOBJ_HANDLE_TO_FD_FLAGS_EXPORT_SYNC_FILE` lets the client import/"
"export the syncobj's current fence from/to a :c:type:`sync_file`. When a "
"syncobj is exported to a sync file, that sync file wraps the sycnobj's fence "
"at the time of export and any later signal or reset operations on the "
"syncobj will not affect the exported sync file. When a sync file is imported "
"into a syncobj, the syncobj's fence is set to the fence wrapped by that sync "
"file. Because sync files are immutable, resetting or signaling the syncobj "
"will not affect any sync files whose fences have been imported into the "
"syncobj."
msgstr ""

#: ../../../gpu/drm-mm:529: drivers/gpu/drm/drm_syncobj.c:184
msgid ""
":c:type:`DRM_IOCTL_SYNCOBJ_TRANSFER` provides a mechanism to transfer a "
"struct :c:type:`dma_fence_chain` of a syncobj at a given u64 point to "
"another u64 point into another syncobj."
msgstr ""

#: ../../../gpu/drm-mm:529: drivers/gpu/drm/drm_syncobj.c:188
msgid ""
"Note that if you want to transfer a struct :c:type:`dma_fence_chain` from a "
"given point on a timeline syncobj from/into a binary syncobj, you can use "
"the point 0 to mean take/replace the fence in the syncobj."
msgstr ""

#: ../../../gpu/drm-mm:532: include/drm/drm_syncobj.h:35
#: include/drm/drm_syncobj.h:87 include/drm/drm_syncobj.h:97
msgid "sync object."
msgstr ""

#: ../../../gpu/drm-mm:532: include/drm/drm_syncobj.h:42
msgid "Reference count of this object."
msgstr ""

#: ../../../gpu/drm-mm:532: include/drm/drm_syncobj.h:45
msgid "``fence``"
msgstr ""

#: ../../../gpu/drm-mm:532: include/drm/drm_syncobj.h:46
msgid "NULL or a pointer to the fence bound to this object."
msgstr ""

#: ../../../gpu/drm-mm:532: include/drm/drm_syncobj.h:48
msgid ""
"This field should not be used directly. Use drm_syncobj_fence_get() and "
"drm_syncobj_replace_fence() instead."
msgstr ""

#: ../../../gpu/drm-mm:532: include/drm/drm_syncobj.h:53
msgid "``cb_list``"
msgstr ""

#: ../../../gpu/drm-mm:532: include/drm/drm_syncobj.h:54
msgid "List of callbacks to call when the :c:type:`fence` gets replaced."
msgstr ""

#: ../../../gpu/drm-mm:532: include/drm/drm_syncobj.h:57
msgid "``ev_fd_list``"
msgstr ""

#: ../../../gpu/drm-mm:532: include/drm/drm_syncobj.h:58
msgid "List of registered eventfd."
msgstr ""

#: ../../../gpu/drm-mm:532: include/drm/drm_syncobj.h:62
msgid ""
"Protects :c:type:`cb_list` and :c:type:`ev_fd_list`, and write-locks :c:type:"
"`fence`."
msgstr ""

#: ../../../gpu/drm-mm:532: include/drm/drm_syncobj.h:65
msgid "``file``"
msgstr ""

#: ../../../gpu/drm-mm:532: include/drm/drm_syncobj.h:66
msgid "A file backing for this syncobj."
msgstr ""

#: ../../../gpu/drm-mm:532: include/drm/drm_syncobj.h:36
msgid ""
"This structure defines a generic sync object which wraps a :c:type:"
"`dma_fence`."
msgstr ""

#: ../../../gpu/drm-mm:532: include/drm/drm_syncobj.h:73
msgid "acquire a syncobj reference"
msgstr ""

#: ../../../gpu/drm-mm:532: include/drm/drm_syncobj.h:79
#: include/drm/drm_syncobj.h:92
msgid "``struct drm_syncobj *obj``"
msgstr ""

#: ../../../gpu/drm-mm:532: include/drm/drm_syncobj.h:74
msgid "sync object"
msgstr ""

#: ../../../gpu/drm-mm:532: include/drm/drm_syncobj.h:75
msgid ""
"This acquires an additional reference to **obj**. It is illegal to call this "
"without already holding a reference. No locks required."
msgstr ""

#: ../../../gpu/drm-mm:532: include/drm/drm_syncobj.h:86
msgid "release a reference to a sync object."
msgstr ""

#: ../../../gpu/drm-mm:532: include/drm/drm_syncobj.h:96
msgid "get a reference to a fence in a sync object"
msgstr ""

#: ../../../gpu/drm-mm:532: include/drm/drm_syncobj.h:102
#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:331
#: drivers/gpu/drm/drm_syncobj.c:372 drivers/gpu/drm/drm_syncobj.c:589
#: drivers/gpu/drm/drm_syncobj.c:671
msgid "``struct drm_syncobj *syncobj``"
msgstr ""

#: ../../../gpu/drm-mm:532: include/drm/drm_syncobj.h:98
msgid ""
"This acquires additional reference to :c:type:`drm_syncobj.fence "
"<drm_syncobj>` contained in **obj**, if not NULL. It is illegal to call this "
"without already holding a reference. No locks required."
msgstr ""

#: ../../../gpu/drm-mm:532: include/drm/drm_syncobj.h:103
msgid "Either the fence of **obj** or NULL if there's none."
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:241
msgid "lookup and reference a sync object."
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:247
#: drivers/gpu/drm/drm_syncobj.c:428 drivers/gpu/drm/drm_syncobj.c:592
msgid "``struct drm_file *file_private``"
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:242
#: drivers/gpu/drm/drm_syncobj.c:423 drivers/gpu/drm/drm_syncobj.c:587
#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:941
msgid "drm file private pointer"
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:243
#: drivers/gpu/drm/drm_syncobj.c:424
msgid "sync object handle to lookup."
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:244
msgid ""
"Returns a reference to the syncobj pointed to by handle or NULL. The "
"reference must be released by calling drm_syncobj_put()."
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:325
msgid "add new timeline point to the syncobj"
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:326
msgid "sync object to add timeline point do"
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:328
msgid "``struct dma_fence_chain *chain``"
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:327
msgid "chain node to use to add the point"
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:328
msgid "fence to encapsulate in the chain node"
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:330
msgid "``uint64_t point``"
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:329
msgid "sequence number to use for the point"
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:330
msgid "Add the chain node as new timeline point to the syncobj."
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:366
msgid "replace fence in a sync object."
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:367
msgid "Sync object to replace fence in"
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:368
msgid "fence to install in sync file."
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:369
msgid "This replaces the fence on a sync object."
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:422
msgid "lookup and reference the fence in a sync object"
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:426
msgid "``u64 point``"
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:425
#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:943
msgid "timeline point"
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:427
msgid "``u64 flags``"
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:426
msgid "DRM_SYNCOBJ_WAIT_FLAGS_WAIT_FOR_SUBMIT or not"
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:428
msgid "``struct dma_fence **fence``"
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:427
msgid "out parameter for the fence"
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:428
msgid ""
"This is just a convenience function that combines drm_syncobj_find() and "
"drm_syncobj_fence_get()."
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:431
msgid ""
"Returns 0 on success or a negative error value on failure. On success "
"**fence** contains a reference to the fence, which must be released by "
"calling dma_fence_put()."
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:521
msgid "free a sync object."
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:522
msgid "kref to free."
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:523
msgid "Only to be called from kref_put in drm_syncobj_put."
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:543
msgid "create a new syncobj"
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:549
msgid "``struct drm_syncobj **out_syncobj``"
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:544
msgid "returned syncobj"
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:545
msgid "DRM_SYNCOBJ_* flags"
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:546
msgid "if non-NULL, the syncobj will represent this fence"
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:547
msgid ""
"This is the first function to create a sync object. After creating, drivers "
"probably want to make it available to userspace, either through "
"drm_syncobj_get_handle() or drm_syncobj_get_fd()."
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:551
#: drivers/gpu/drm/drm_syncobj.c:593 drivers/gpu/drm/drm_syncobj.c:670
msgid "Returns 0 on success or a negative error value on failure."
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:586
msgid "get a handle from a syncobj"
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:588
#: drivers/gpu/drm/drm_syncobj.c:666
msgid "Sync object to export"
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:590
msgid "``u32 *handle``"
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:589
msgid "out parameter with the new handle"
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:590
msgid ""
"Exports a sync object created with drm_syncobj_create() as a handle on "
"**file_private** to userspace."
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:665
msgid "get a file descriptor from a syncobj"
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:668
msgid "``int *p_fd``"
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:667
msgid "out parameter with the new file descriptor"
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:668
msgid ""
"Exports a sync object created with drm_syncobj_create() as a file descriptor."
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:1220
msgid "calculate jiffies timeout from absolute value"
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:1226
msgid "``int64_t timeout_nsec``"
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:1222
msgid "timeout nsec component in ns, 0 for poll"
msgstr ""

#: ../../../gpu/drm-mm:535: drivers/gpu/drm/drm_syncobj.c:1223
msgid "Calculate the timeout in jiffies from an absolute time in sec/nsec."
msgstr ""

#: ../../../gpu/drm-mm.rst:539
msgid "DRM Execution context"
msgstr ""

#: ../../../gpu/drm-mm:541: drivers/gpu/drm/drm_exec.c:10
msgid ""
"This component mainly abstracts the retry loop necessary for locking "
"multiple GEM objects while preparing hardware operations (e.g. command "
"submissions, page table updates etc..)."
msgstr ""

#: ../../../gpu/drm-mm:541: drivers/gpu/drm/drm_exec.c:14
msgid ""
"If a contention is detected while locking a GEM object the cleanup procedure "
"unlocks all previously locked GEM objects and locks the contended one first "
"before locking any further objects."
msgstr ""

#: ../../../gpu/drm-mm:541: drivers/gpu/drm/drm_exec.c:18
msgid ""
"After an object is locked fences slots can optionally be reserved on the "
"dma_resv object inside the GEM object."
msgstr ""

#: ../../../gpu/drm-mm:541: drivers/gpu/drm/drm_exec.c:21
msgid "A typical usage pattern should look like this::"
msgstr ""

#: ../../../gpu/drm-mm:541: drivers/gpu/drm/drm_exec.c:47
msgid "See struct dma_exec for more details."
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:15
msgid "Execution context"
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:20
msgid "Flags to control locking behavior"
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:24
msgid "``ticket``"
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:25
msgid "WW ticket used for acquiring locks"
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:29
msgid "``num_objects``"
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:30
msgid "number of objects locked"
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:34
msgid "``max_objects``"
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:35
msgid "maximum objects in array"
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:39
msgid "``objects``"
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:40
msgid "array of the locked objects"
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:44
msgid "``contended``"
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:45
msgid "contended GEM object we backed off for"
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:49
msgid "``prelocked``"
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:50
msgid "already locked GEM object due to contention"
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:55
msgid "Return the object for a give drm_exec index"
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:56
msgid "Pointer to the drm_exec context"
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:58
msgid "``unsigned long index``"
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:57
msgid "The index."
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:59
msgid ""
"Pointer to the locked object corresponding to **index** if index is within "
"the number of locked objects. NULL otherwise."
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:71
msgid "``drm_exec_for_each_locked_object (exec, index, obj)``"
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:69
msgid "iterate over all the locked objects"
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:70 include/drm/drm_exec.h:82
#: include/drm/drm_exec.h:96 include/drm/drm_exec.h:115
#: include/drm/drm_exec.h:128
msgid "drm_exec object"
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:72 include/drm/drm_exec.h:84
msgid "``index``"
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:71 include/drm/drm_exec.h:83
msgid "unsigned long index for the iteration"
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:72 include/drm/drm_exec.h:84
msgid "the current GEM object"
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:73
msgid "Iterate over all the locked GEM objects inside the drm_exec object."
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:82
msgid "``drm_exec_for_each_locked_object_reverse (exec, index, obj)``"
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:80
msgid "iterate over all the locked objects in reverse locking order"
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:85
msgid ""
"Iterate over all the locked GEM objects inside the drm_exec object in "
"reverse locking order. Note that **index** may go below zero and wrap, but "
"that will be caught by drm_exec_obj(), returning a NULL object."
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:97
msgid "``drm_exec_until_all_locked (exec)``"
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:95
msgid "loop until all GEM objects are locked"
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:97
msgid ""
"Core functionality of the drm_exec object. Loops until all GEM objects are "
"locked and no more contention exists. At the beginning of the loop it is "
"guaranteed that no GEM object is locked."
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:101
msgid ""
"Since labels can't be defined local to the loops body we use a jump pointer "
"to make sure that the retry is only used from within the loops body."
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:116
msgid "``drm_exec_retry_on_contention (exec)``"
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:114
msgid "restart the loop to grap all locks"
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:116
msgid ""
"Control flow helper to continue when a contention was detected and we need "
"to clean up and re-start the loop to prepare all GEM objects."
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:127
msgid "check for contention"
msgstr ""

#: ../../../gpu/drm-mm:544: include/drm/drm_exec.h:129
msgid ""
"Returns true if the drm_exec object has run into some contention while "
"locking a GEM object and needs to clean up."
msgstr ""

#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:71
msgid "initialize a drm_exec object"
msgstr ""

#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:72
msgid "the drm_exec object to initialize"
msgstr ""

#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:74
msgid "``u32 flags``"
msgstr ""

#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:73
msgid "controls locking behavior, see DRM_EXEC_* defines"
msgstr ""

#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:75
msgid "``unsigned nr``"
msgstr ""

#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:74
msgid "the initial # of objects"
msgstr ""

#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:75
msgid "Initialize the object and make sure that we can track locked objects."
msgstr ""

#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:77
msgid ""
"If nr is non-zero then it is used as the initial objects table size. In "
"either case, the table will grow (be re-allocated) on demand."
msgstr ""

#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:98
msgid "finalize a drm_exec object"
msgstr ""

#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:99
msgid "the drm_exec object to finalize"
msgstr ""

#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:100
msgid ""
"Unlock all locked objects, drop the references to objects and free all "
"memory used for tracking the state."
msgstr ""

#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:116
msgid "cleanup when contention is detected"
msgstr ""

#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:117
msgid "the drm_exec object to cleanup"
msgstr ""

#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:118
msgid ""
"Cleanup the current state and return true if we should stay inside the retry "
"loop, false if there wasn't any contention detected and we can keep the "
"objects locked."
msgstr ""

#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:199
msgid "lock a GEM object for use"
msgstr ""

#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:200
#: drivers/gpu/drm/drm_exec.c:255 drivers/gpu/drm/drm_exec.c:282
#: drivers/gpu/drm/drm_exec.c:312
msgid "the drm_exec object with the state"
msgstr ""

#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:201
msgid "the GEM object to lock"
msgstr ""

#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:202
msgid "Lock a GEM object for use and grab a reference to it."
msgstr ""

#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:205
msgid ""
"-EDEADLK if a contention is detected, -EALREADY when object is already "
"locked (can be suppressed by setting the DRM_EXEC_IGNORE_DUPLICATES flag), -"
"ENOMEM when memory allocation failed and zero for success."
msgstr ""

#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:254
msgid "unlock a GEM object in this exec context"
msgstr ""

#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:256
msgid "the GEM object to unlock"
msgstr ""

#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:257
msgid ""
"Unlock the GEM object and remove it from the collection of locked objects. "
"Should only be used to unlock the most recently locked objects. It's not "
"time efficient to unlock objects locked long ago."
msgstr ""

#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:281
msgid "prepare a GEM object for use"
msgstr ""

#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:283
msgid "the GEM object to prepare"
msgstr ""

#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:284
msgid "how many fences to reserve"
msgstr ""

#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:285
msgid "Prepare a GEM object for use by locking it and reserving fence slots."
msgstr ""

#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:288
msgid ""
"-EDEADLK if a contention is detected, -EALREADY when object is already "
"locked, -ENOMEM when memory allocation failed and zero for success."
msgstr ""

#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:311
msgid "helper to prepare an array of objects"
msgstr ""

#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:314
msgid "``struct drm_gem_object **objects``"
msgstr ""

#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:313
msgid "array of GEM object to prepare"
msgstr ""

#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:315
msgid "``unsigned int num_objects``"
msgstr ""

#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:314
msgid "number of GEM objects in the array"
msgstr ""

#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:315
msgid "number of fences to reserve on each GEM object"
msgstr ""

#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:316
msgid ""
"Prepares all GEM objects in an array, aborts on first error. Reserves "
"**num_fences** on each GEM object after locking it."
msgstr ""

#: ../../../gpu/drm-mm:547: drivers/gpu/drm/drm_exec.c:320
msgid ""
"-EDEADLOCK on contention, -EALREADY when object is already locked, -ENOMEM "
"when memory allocation failed and zero for success."
msgstr ""

#: ../../../gpu/drm-mm.rst:551
msgid "GPU Scheduler"
msgstr ""

#: ../../../gpu/drm-mm:556: drivers/gpu/drm/scheduler/sched_main.c:25
msgid ""
"The GPU scheduler provides entities which allow userspace to push jobs into "
"software queues which are then scheduled on a hardware run queue. The "
"software queues have a priority among them. The scheduler selects the "
"entities from the run queue using a FIFO. The scheduler provides dependency "
"handling features among jobs. The driver is supposed to provide callback "
"functions for backend operations to the scheduler like submitting a job to "
"hardware run queue, returning the dependencies of a job etc."
msgstr ""

#: ../../../gpu/drm-mm:556: drivers/gpu/drm/scheduler/sched_main.c:33
msgid "The organisation of the scheduler is the following:"
msgstr ""

#: ../../../gpu/drm-mm:556: drivers/gpu/drm/scheduler/sched_main.c:35
msgid "Each hw run queue has one scheduler"
msgstr ""

#: ../../../gpu/drm-mm:556: drivers/gpu/drm/scheduler/sched_main.c:36
msgid ""
"Each scheduler has multiple run queues with different priorities (e.g., "
"HIGH_HW,HIGH_SW, KERNEL, NORMAL)"
msgstr ""

#: ../../../gpu/drm-mm:556: drivers/gpu/drm/scheduler/sched_main.c:38
msgid "Each scheduler run queue has a queue of entities to schedule"
msgstr ""

#: ../../../gpu/drm-mm:556: drivers/gpu/drm/scheduler/sched_main.c:39
msgid ""
"Entities themselves maintain a queue of jobs that will be scheduled on the "
"hardware."
msgstr ""

#: ../../../gpu/drm-mm:556: drivers/gpu/drm/scheduler/sched_main.c:42
msgid ""
"The jobs in an entity are always scheduled in the order in which they were "
"pushed."
msgstr ""

#: ../../../gpu/drm-mm:556: drivers/gpu/drm/scheduler/sched_main.c:44
msgid ""
"Note that once a job was taken from the entities queue and pushed to the "
"hardware, i.e. the pending queue, the entity must not be referenced anymore "
"through the jobs entity pointer."
msgstr ""

#: ../../../gpu/drm-mm.rst:560
msgid "Flow Control"
msgstr ""

#: ../../../gpu/drm-mm:562: drivers/gpu/drm/scheduler/sched_main.c:52
msgid ""
"The DRM GPU scheduler provides a flow control mechanism to regulate the rate "
"in which the jobs fetched from scheduler entities are executed."
msgstr ""

#: ../../../gpu/drm-mm:562: drivers/gpu/drm/scheduler/sched_main.c:55
msgid ""
"In this context the :c:type:`drm_gpu_scheduler` keeps track of a driver "
"specified credit limit representing the capacity of this scheduler and a "
"credit count; every :c:type:`drm_sched_job` carries a driver specified "
"number of credits."
msgstr ""

#: ../../../gpu/drm-mm:562: drivers/gpu/drm/scheduler/sched_main.c:59
msgid ""
"Once a job is executed (but not yet finished), the job's credits contribute "
"to the scheduler's credit count until the job is finished. If by executing "
"one more job the scheduler's credit count would exceed the scheduler's "
"credit limit, the job won't be executed. Instead, the scheduler will wait "
"until the credit count has decreased enough to not overflow its credit "
"limit. This implies waiting for previously executed jobs."
msgstr ""

#: ../../../gpu/drm-mm.rst:566
msgid "Scheduler Function References"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:38
msgid "``DRM_SCHED_FENCE_DONT_PIPELINE``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:36
msgid "Prevent dependency pipelining"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:37
msgid ""
"Setting this flag on a scheduler fence prevents pipelining of jobs depending "
"on this fence. In other words we always insert a full CPU round trip before "
"dependent jobs are pushed to the hw queue."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:47
msgid "``DRM_SCHED_FENCE_FLAG_HAS_DEADLINE_BIT``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:45
msgid "A fence deadline hint has been set"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:46
msgid ""
"Because we could have a deadline hint can be set before the backing hw fence "
"is created, we need to keep track of whether a deadline has already been set."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:75
msgid "A wrapper around a job queue (typically attached to the DRM file_priv)."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:85
msgid ""
"Used to append this struct to the list of entities in the runqueue **rq** "
"under :c:type:`drm_sched_rq.entities <drm_sched_rq>`."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:88
msgid "Protected by :c:type:`drm_sched_rq.lock <drm_sched_rq>` of **rq**."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:95
msgid ""
"Lock protecting the run-queue (**rq**) to which this entity belongs, "
"**priority** and the list of schedulers (**sched_list**, **num_sched_list**)."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:102
msgid "``rq``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:103
msgid "Runqueue on which this entity is currently scheduled."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:105
msgid ""
"FIXME: Locking is very unclear for this. Writers are protected by **lock**, "
"but readers are generally lockless and seem to just race with not even a "
"READ_ONCE."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:113
msgid "``sched_list``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:114
msgid ""
"A list of schedulers (struct drm_gpu_scheduler).  Jobs from this entity can "
"be scheduled on any scheduler on this list."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:117
msgid ""
"This can be modified by calling drm_sched_entity_modify_sched(). Locking is "
"entirely up to the driver, see the above function for more details."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:121
msgid ""
"This will be set to NULL if :c:type:`num_sched_list` equals 1 and **rq** has "
"been set already."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:124
msgid ""
"FIXME: This means priority changes through drm_sched_entity_set_priority() "
"will be lost henceforth in this case."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:131
msgid "``num_sched_list``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:132
msgid "Number of drm_gpu_schedulers in the **sched_list**."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:139
msgid ""
"Priority of the entity. This can be modified by calling "
"drm_sched_entity_set_priority(). Protected by **lock**."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:146
msgid "``job_queue``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:147
msgid "the list of jobs of this entity."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:151
msgid "``fence_seq``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:152
msgid ""
"A linearly increasing seqno incremented with each new :c:type:"
"`drm_sched_fence` which is part of the entity."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:155
msgid ""
"FIXME: Callers of drm_sched_job_arm() need to ensure correct locking, this "
"doesn't need to be atomic."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:162
msgid "``fence_context``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:163
msgid ""
"A unique context for all the fences which belong to this entity.  The :c:"
"type:`drm_sched_fence.scheduled <drm_sched_fence>` uses the fence_context "
"but :c:type:`drm_sched_fence.finished <drm_sched_fence>` uses fence_context "
"+ 1."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:171
msgid "``dependency``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:172
msgid "The dependency fence of the job which is on the top of the job queue."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:178
#: include/drm/gpu_scheduler.h:334
msgid "``cb``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:179
msgid "Callback for the dependency fence above."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:185
msgid "``guilty``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:186
msgid "Points to entities' guilty."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:192
msgid "``last_scheduled``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:193
msgid ""
"Points to the finished fence of the last scheduled job. Only written by "
"drm_sched_entity_pop_job(). Can be accessed locklessly from "
"drm_sched_job_arm() if the queue is empty."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:201
msgid "``last_user``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:202
msgid "last group leader pushing a job into the entity."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:206
msgid "``stopped``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:207
msgid ""
"Marks the enity as removed from rq and destined for termination. This is set "
"by calling drm_sched_entity_flush() and by drm_sched_fini()."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:215
msgid "``entity_idle``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:216
msgid ""
"Signals when entity is not in use, used to sequence entity cleanup in "
"drm_sched_entity_fini()."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:223
msgid "``oldest_job_waiting``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:224
msgid "Marks earliest job waiting in SW queue"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:230
msgid "``rb_tree_node``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:231
msgid "The node used to insert this entity into time based priority queue"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:76
msgid ""
"Entities will emit jobs in order to their corresponding hardware ring, and "
"the scheduler will alternate between entities based on scheduling policy."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:239
msgid "queue of entities to be scheduled."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:241
#: include/drm/gpu_scheduler.h:296 include/drm/gpu_scheduler.h:349
msgid "``sched``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:242
msgid "the scheduler to which this rq belongs to."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:243
msgid "protects **entities**, **rb_tree_root** and **current_entity**."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:243
msgid "``current_entity``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:244
msgid "the entity which is to be scheduled."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:244
msgid "``entities``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:245
msgid "list of the entities to be scheduled."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:245
msgid "``rb_tree_root``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:246
msgid "root of time based priority queue of entities for FIFO scheduling"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:246
msgid ""
"Run queue is a set of entities scheduling command submissions for one "
"specific ring. It implements the scheduling policy that selects the next "
"entity to emit commands from."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:262
msgid "fences corresponding to the scheduling of a job."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:266
msgid "``scheduled``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:267
msgid ""
"this fence is what will be signaled by the scheduler when the job is "
"scheduled."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:272
msgid "``finished``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:273
msgid ""
"this fence is what will be signaled by the scheduler when the job is "
"completed."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:276
msgid ""
"When setting up an out fence for the job, you should use this, since it's "
"available immediately upon drm_sched_job_init(), and the fence returned by "
"the driver from run_job() won't be created until the dependencies have "
"resolved."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:284
msgid "``deadline``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:285
msgid ""
"deadline set on :c:type:`drm_sched_fence.finished <drm_sched_fence>` which "
"potentially needs to be propagated to :c:type:`drm_sched_fence.parent "
"<drm_sched_fence>`"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:290
msgid "``parent``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:291
msgid ""
"the fence returned by :c:type:`drm_sched_backend_ops.run_job "
"<drm_sched_backend_ops>` when scheduling the job on hardware. We signal the :"
"c:type:`drm_sched_fence.finished <drm_sched_fence>` fence once parent is "
"signalled."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:297
msgid "the scheduler instance to which the job having this struct belongs to."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:302
msgid "the lock used by the scheduled and the finished fences."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:305
msgid "``owner``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:306
#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:797
msgid "job owner for debugging"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:310
msgid "``drm_client_id``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:311
msgid "The client_id of the drm_file which owns the job."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:320
msgid "A job to be run by an entity."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:342
msgid "``submit_ts``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:343
msgid "When the job was pushed into the entity queue."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:350
msgid ""
"The scheduler this job is or will be scheduled on. Gets set by "
"drm_sched_job_arm(). Valid until drm_sched_backend_ops.free_job() has "
"finished."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:325
msgid "``s_fence``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:326
msgid "contains the fences for the scheduling of job."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:333
msgid "``entity``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:334
msgid "the entity to which this job belongs."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:332
msgid "``s_priority``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:333
msgid "the priority of the job."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:327
msgid "``credits``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:328
msgid "the number of credits this job contributes to the scheduler"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:362
msgid "``last_dependency``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:363
msgid "tracks **dependencies** as they signal"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:329
msgid "``karma``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:330
msgid ""
"increment on every hang caused by this job. If this exceeds the hang limit "
"of the scheduler then the job is marked guilty and will not be scheduled "
"further."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:322
msgid "``queue_node``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:323
msgid "used to append this struct to the queue of jobs in an entity."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:324
msgid "a job participates in a \"pending\" and \"done\" lists."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:326
msgid "``finish_cb``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:327
msgid "the callback for the finished fence."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:328
msgid "``work``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:329
msgid "Helper to reschedule job kill to different context."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:335
msgid "the callback for the parent fence in s_fence."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:381
msgid "``dependencies``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:382
msgid ""
"Contains the dependencies as struct dma_fence for this job, see "
"drm_sched_job_add_dependency() and drm_sched_job_add_implicit_dependencies()."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:335
msgid ""
"A job is created by the driver using drm_sched_job_init(), and should call "
"drm_sched_entity_push_job() once it wants the scheduler to schedule the job."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:391
msgid "the scheduler's status"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:397
msgid "``DRM_GPU_SCHED_STAT_NONE``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:398
msgid "Reserved. Do not use."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:400
msgid "``DRM_GPU_SCHED_STAT_RESET``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:401
msgid "The GPU hung and successfully reset."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:403
msgid "``DRM_GPU_SCHED_STAT_ENODEV``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:404
msgid "Error: Device is not available anymore."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:406
msgid "``DRM_GPU_SCHED_STAT_NO_HANG``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:407
msgid ""
"Contrary to scheduler's assumption, the GPU did not hang and is still "
"running."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:407
msgid "Define the backend operations called by the scheduler"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:414
msgid "``prepare_job``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:415
msgid ""
"Called when the scheduler is considering scheduling this job next, to get "
"another struct dma_fence for this job to block on.  Once it returns NULL, "
"run_job() may be called."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:419
msgid ""
"Can be NULL if no additional preparation to the dependencies are necessary. "
"Skipped when jobs are killed instead of run."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:427
msgid "``run_job``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:428
msgid ""
"Called to execute the job once all of the dependencies have been resolved."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:431
msgid "**sched_job**: the job to run"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:433
msgid ""
"The deprecated drm_sched_resubmit_jobs() (called by :c:type:`struct "
"drm_sched_backend_ops <drm_sched_backend_ops>`.timedout_job) can invoke this "
"again with the same parameters. Using this is discouraged because it "
"violates dma_fence rules, notably dma_fence_init() has to be called on "
"already initialized fences for a second time. Moreover, this is dangerous "
"because attempts to allocate memory might deadlock with memory management "
"code waiting for the reset to complete."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:441
msgid "TODO: Document what drivers should do / use instead."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:443
msgid ""
"This method is called in a workqueue context - either from the submit_wq the "
"driver passed through drm_sched_init(), or, if the driver passed NULL, a "
"separate, ordered workqueue the scheduler allocated."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:448
msgid ""
"Note that the scheduler expects to 'inherit' its own reference to this fence "
"from the callback. It does not invoke an extra dma_fence_get() on it. "
"Consequently, this callback must take a reference for the scheduler, and "
"additional ones for the driver's respective needs."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:454
msgid ""
"Return: * On success: dma_fence the driver must signal once the hardware has "
"completed the job (\"hardware fence\"). * On failure: NULL or an ERR_PTR."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:461
msgid "``timedout_job``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:462
msgid ""
"Called when a job has taken too long to execute, to trigger GPU recovery."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:465
msgid "**sched_job**: The job that has timed out"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:467
msgid ""
"Drivers typically issue a reset to recover from GPU hangs. This procedure "
"looks very different depending on whether a firmware or a hardware scheduler "
"is being used."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:471
msgid ""
"For a FIRMWARE SCHEDULER, each ring has one scheduler, and each scheduler "
"has one entity. Hence, the steps taken typically look as follows:"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:475
msgid ""
"Stop the scheduler using drm_sched_stop(). This will pause the scheduler "
"workqueues and cancel the timeout work, guaranteeing that nothing is queued "
"while the ring is being removed."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:478
msgid ""
"Remove the ring. The firmware will make sure that the corresponding parts of "
"the hardware are resetted, and that other rings are not impacted."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:481
msgid "Kill the entity and the associated scheduler."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:484
msgid ""
"For a HARDWARE SCHEDULER, a scheduler instance schedules jobs from one or "
"more entities to one ring. This implies that all entities associated with "
"the affected scheduler cannot be torn down, because this would effectively "
"also affect innocent userspace processes which did not submit faulty jobs "
"(for example)."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:490
msgid ""
"Consequently, the procedure to recover with a hardware scheduler should look "
"like this:"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:493
msgid "Stop all schedulers impacted by the reset using drm_sched_stop()."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:494
msgid "Kill the entity the faulty job stems from."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:495
msgid "Issue a GPU reset on all faulty rings (driver-specific)."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:496
msgid ""
"Re-submit jobs on all schedulers impacted by re-submitting them to the "
"entities which are still alive."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:498
msgid ""
"Restart all schedulers that were stopped in step #1 using drm_sched_start()."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:501
msgid ""
"Note that some GPUs have distinct hardware queues but need to reset the GPU "
"globally, which requires extra synchronization between the timeout handlers "
"of different schedulers. One way to achieve this synchronization is to "
"create an ordered workqueue (using alloc_ordered_workqueue()) at the driver "
"level, and pass this queue as drm_sched_init()'s **timeout_wq** parameter. "
"This will guarantee that timeout handlers are executed sequentially."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:509
msgid ""
"Return: The scheduler's status, defined by :c:type:`enum drm_gpu_sched_stat "
"<drm_gpu_sched_stat>`"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:514
msgid "``free_job``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:515
msgid ""
"Called once the job's finished fence has been signaled and it's time to "
"clean it up."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:520
msgid "``cancel_job``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:521
msgid ""
"Used by the scheduler to guarantee remaining jobs' fences get signaled in "
"drm_sched_fini()."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:524
msgid ""
"Used by the scheduler to cancel all jobs that have not been executed with :c:"
"type:`struct drm_sched_backend_ops <drm_sched_backend_ops>`.run_job by the "
"time drm_sched_fini() gets invoked."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:528
msgid ""
"Drivers need to signal the passed job's hardware fence with an appropriate "
"error code (e.g., -ECANCELED) in this callback. They must not free the job."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:532
msgid ""
"The scheduler will only call this callback once it stopped calling all other "
"callbacks forever, with the exception of :c:type:`struct "
"drm_sched_backend_ops <drm_sched_backend_ops>`.free_job."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:408
msgid "These functions should be implemented in the driver side."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:539
msgid "scheduler instance-specific data"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:542
msgid "backend operations provided by the driver."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:542
#: include/drm/gpu_scheduler.h:608
msgid "``credit_limit``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:543
msgid "the credit limit of this scheduler"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:543
msgid "``credit_count``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:544
msgid "the current credit count of this scheduler"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:544
#: include/drm/gpu_scheduler.h:611
msgid "``timeout``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:545
msgid "the time after which a job is removed from the scheduler."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:546
msgid "name of the ring for which this scheduler is being used."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:546
#: include/drm/gpu_scheduler.h:606
msgid "``num_rqs``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:547
msgid ""
"Number of run-queues. This is at most DRM_SCHED_PRIORITY_COUNT, as there's "
"usually one run-queue per priority, but could be less."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:548
msgid "``sched_rq``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:549
msgid "An allocated array of run-queues of size **num_rqs**;"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:549
msgid "``job_scheduled``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:550
msgid ""
"once **drm_sched_entity_do_release** is called the scheduler waits on this "
"wait queue until all the scheduled jobs are finished."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:552
msgid "``job_id_count``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:553
msgid "used to assign unique id to the each job."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:553
#: include/drm/gpu_scheduler.h:604
msgid "``submit_wq``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:554
msgid "workqueue used to queue **work_run_job** and **work_free_job**"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:554
#: include/drm/gpu_scheduler.h:612
msgid "``timeout_wq``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:555
msgid "workqueue used to queue **work_tdr**"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:555
msgid "``work_run_job``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:556
msgid "work which calls run_job op of each scheduler."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:556
msgid "``work_free_job``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:557
msgid "work which calls free_job op of each scheduler."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:557
msgid "``work_tdr``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:558
msgid ""
"schedules a delayed call to **drm_sched_job_timedout** after the timeout "
"interval is over."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:559
msgid "``pending_list``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:560
msgid "the list of jobs which are currently in the job queue."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:560
msgid "``job_list_lock``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:561
msgid "lock to protect the pending_list."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:561
#: include/drm/gpu_scheduler.h:609
msgid "``hang_limit``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:562
msgid ""
"once the hangs by a job crosses this limit then it is marked guilty and it "
"will no longer be considered for scheduling."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:563
#: include/drm/gpu_scheduler.h:613
msgid "``score``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:564
msgid "score to help loadbalancer pick a idle sched"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:564
msgid "``_score``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:565
msgid "score used when the driver doesn't provide one"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:565
msgid "``ready``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:566
msgid "marks if the underlying HW is ready to work"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:566
msgid "``free_guilty``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:567
msgid "A hit to time out handler to free the guilty job."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:567
msgid "``pause_submit``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:568
msgid "pause queuing of **work_run_job** on **submit_wq**"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:568
msgid "``own_submit_wq``"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:569
msgid "scheduler owns allocation of **submit_wq**"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:570
msgid "system :c:type:`struct device <device>`"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:570
msgid "One scheduler is implemented for each hardware ring."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:601
msgid "parameters for initializing a DRM GPU scheduler"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:604
msgid "backend operations provided by the driver"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:605
msgid ""
"workqueue to use for submission. If NULL, an ordered wq is allocated and "
"used."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:613
msgid "workqueue to use for timeout work. If NULL, the system_wq is used."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:607
msgid ""
"Number of run-queues. This may be at most DRM_SCHED_PRIORITY_COUNT, as "
"there's usually one run-queue per priority, but may be less."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:609
msgid "the number of credits this scheduler can hold from all jobs"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:610
msgid ""
"number of times to allow a job to hang before dropping it. This mechanism is "
"DEPRECATED. Set it to 0."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:612
msgid "timeout value in jiffies for submitted jobs."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:614
msgid "score atomic shared with other schedulers. May be NULL."
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:615
msgid "name (typically the driver's name). Used for debugging"
msgstr ""

#: ../../../gpu/drm-mm:568: include/drm/gpu_scheduler.h:616
msgid "associated device. Used for debugging"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:440
msgid "immediately start job timeout handler"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:446
#: drivers/gpu/drm/scheduler/sched_main.c:462
#: drivers/gpu/drm/scheduler/sched_main.c:476
#: drivers/gpu/drm/scheduler/sched_main.c:506
#: drivers/gpu/drm/scheduler/sched_main.c:608
#: drivers/gpu/drm/scheduler/sched_main.c:701
#: drivers/gpu/drm/scheduler/sched_main.c:745
#: drivers/gpu/drm/scheduler/sched_main.c:1318
#: drivers/gpu/drm/scheduler/sched_main.c:1416
#: drivers/gpu/drm/scheduler/sched_main.c:1515
#: drivers/gpu/drm/scheduler/sched_main.c:1528
#: drivers/gpu/drm/scheduler/sched_main.c:1543
msgid "``struct drm_gpu_scheduler *sched``"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:442
msgid "scheduler for which the timeout handling should be started."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:443
msgid "Start timeout handling immediately for the named scheduler."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:456
msgid "immediately start timeout handler"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:458
msgid "scheduler where the timeout handling should be started."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:459
msgid ""
"Start timeout handling immediately when the driver detects a hardware fault."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:470
msgid "Suspend scheduler job timeout"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:472
msgid "scheduler instance for which to suspend the timeout"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:473
msgid ""
"Suspend the delayed work timeout for the scheduler. This is done by "
"modifying the delayed work timeout to an arbitrary large value, "
"MAX_SCHEDULE_TIMEOUT in this case."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:477
msgid "Returns the timeout remaining"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:500
msgid "Resume scheduler job timeout"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:502
msgid "scheduler instance for which to resume the timeout"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:504
msgid "``unsigned long remaining``"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:503
msgid "remaining timeout"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:504
msgid "Resume the delayed work timeout for the scheduler."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:602
msgid "stop the scheduler"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:604
#: drivers/gpu/drm/scheduler/sched_main.c:697
#: drivers/gpu/drm/scheduler/sched_main.c:741
#: drivers/gpu/drm/scheduler/sched_main.c:1314
#: drivers/gpu/drm/scheduler/sched_main.c:1412
#: drivers/gpu/drm/scheduler/sched_main.c:1511
#: drivers/gpu/drm/scheduler/sched_main.c:1523
#: drivers/gpu/drm/scheduler/sched_main.c:1538
msgid "scheduler instance"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:606
#: drivers/gpu/drm/scheduler/sched_main.c:1472
msgid "``struct drm_sched_job *bad``"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:605
msgid "job which caused the time out"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:606
msgid "Stop the scheduler and also removes and frees all completed jobs."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:608
msgid ""
"bad job will not be freed as it might be used later and so it's callers "
"responsibility to release it manually if it's not part of the pending list "
"any more."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:612
msgid ""
"This function is typically used for reset recovery (see the docu of "
"drm_sched_backend_ops.timedout_job() for details). Do not call it for "
"scheduler teardown, i.e., before calling drm_sched_fini()."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:616
#: drivers/gpu/drm/scheduler/sched_main.c:704
msgid ""
"As it's only used for reset recovery, drivers must not call this function in "
"their :c:type:`struct drm_sched_backend_ops <drm_sched_backend_ops>`."
"timedout_job callback when they skip a reset using :c:type:`enum "
"drm_gpu_sched_stat <drm_gpu_sched_stat>`.DRM_GPU_SCHED_STAT_NO_HANG."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:695
msgid "recover jobs after a reset"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:699
msgid "``int errno``"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:698
msgid "error to set on the pending fences"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:699
msgid ""
"This function is typically used for reset recovery (see the docu of "
"drm_sched_backend_ops.timedout_job() for details). Do not call it for "
"scheduler startup. The scheduler itself is fully operational after "
"drm_sched_init() succeeded."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:739
msgid "Deprecated, don't use in new code!"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:742
msgid ""
"Re-submitting jobs was a concept AMD came up as cheap way to implement "
"recovery after a job timeout."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:745
msgid ""
"This turned out to be not working very well. First of all there are many "
"problem with the dma_fence implementation and requirements. Either the "
"implementation is risking deadlocks with core memory management or violating "
"documented implementation details of the dma_fence object."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:750
msgid ""
"Drivers can still save and restore their state for recovery operations, but "
"we shouldn't make this a general scheduler feature around the dma_fence "
"interface."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:792
msgid "init a scheduler job"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:798
#: drivers/gpu/drm/scheduler/sched_main.c:866
#: drivers/gpu/drm/scheduler/sched_main.c:899
#: drivers/gpu/drm/scheduler/sched_main.c:945
#: drivers/gpu/drm/scheduler/sched_main.c:973
#: drivers/gpu/drm/scheduler/sched_main.c:1008
#: drivers/gpu/drm/scheduler/sched_main.c:1032
#: drivers/gpu/drm/scheduler/sched_main.c:1055
msgid "``struct drm_sched_job *job``"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:793
msgid "scheduler job to init"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:795
#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:42
#: drivers/gpu/drm/scheduler/sched_entity.c:124
#: drivers/gpu/drm/scheduler/sched_entity.c:161
#: drivers/gpu/drm/scheduler/sched_entity.c:273
#: drivers/gpu/drm/scheduler/sched_entity.c:320
#: drivers/gpu/drm/scheduler/sched_entity.c:351
#: drivers/gpu/drm/scheduler/sched_entity.c:380
msgid "``struct drm_sched_entity *entity``"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:794
msgid "scheduler entity to use"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:796
msgid "``u32 credits``"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:795
msgid ""
"the number of credits this job contributes to the schedulers credit limit"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:798
msgid "``void *owner``"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:799
msgid "``uint64_t drm_client_id``"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:798
msgid ""
":c:type:`struct drm_file <drm_file>`.client_id of the owner (used by trace "
"events)"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:800
#: drivers/gpu/drm/scheduler/sched_main.c:867
msgid ""
"Refer to drm_sched_entity_push_job() documentation for locking "
"considerations."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:803
msgid ""
"Drivers must make sure drm_sched_job_cleanup() if this function returns "
"successfully, even when **job** is aborted before drm_sched_job_arm() is "
"called."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:806
msgid ""
"Note that this function does not assign a valid value to each struct member "
"of struct drm_sched_job. Take a look at that struct's documentation to see "
"who sets which struct member with what lifetime."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:810
msgid ""
"WARNING: amdgpu abuses :c:type:`drm_sched.ready <drm_sched>` to signal when "
"the hardware has died, which can mean that there's no valid runqueue for a "
"**entity**. This function returns -ENOENT in this case (which probably "
"should be -EIO as a more meanigful return value)."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:815
msgid "Returns 0 for success, negative error code otherwise."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:860
msgid "arm a scheduler job for execution"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:861
msgid "scheduler job to arm"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:862
msgid ""
"This arms a scheduler job for execution. Specifically it initializes the :c:"
"type:`drm_sched_job.s_fence <drm_sched_job>` of **job**, so that it can be "
"attached to struct dma_resv or other places that need to track the "
"completion of this job. It also initializes sequence numbers, which are "
"fundamental for fence ordering."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:870
msgid ""
"Once this function was called, you *must* submit **job** with "
"drm_sched_entity_push_job()."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:873
msgid "This can only be called if drm_sched_job_init() succeeded."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:893
msgid "adds the fence as a job dependency"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:894
#: drivers/gpu/drm/scheduler/sched_main.c:940
#: drivers/gpu/drm/scheduler/sched_main.c:968
#: drivers/gpu/drm/scheduler/sched_main.c:1004
msgid "scheduler job to add the dependencies to"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:895
msgid "the dma_fence to add to the list of dependencies."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:896
msgid "Note that **fence** is consumed in both the success and error cases."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:899
#: drivers/gpu/drm/scheduler/sched_main.c:947
#: drivers/gpu/drm/scheduler/sched_main.c:975
#: drivers/gpu/drm/scheduler/sched_main.c:1013
msgid "0 on success, or an error on failing to expand the array."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:939
msgid "adds a syncobj's fence as a job dependency"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:942
msgid "syncobj handle to lookup"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:944
msgid "``u32 point``"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:944
msgid "This adds the fence matching the given syncobj to **job**."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:967
msgid "add all fences from the resv to the job"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:970
msgid "``struct dma_resv *resv``"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:969
msgid "the dma_resv object to get the fences from"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:971
msgid "``enum dma_resv_usage usage``"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:970
msgid "the dma_resv_usage to use to filter the fences"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:971
msgid ""
"This adds all fences matching the given usage from **resv** to **job**. Must "
"be called with the **resv** lock held."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1002
msgid "adds implicit dependencies as job dependencies"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1005
msgid "the gem object to add new dependencies from."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1007
msgid "``bool write``"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1006
msgid ""
"whether the job might write the object (so we need to depend on shared "
"fences in the reservation object)."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1008
msgid ""
"This should be called after drm_gem_lock_reservations() on your array of GEM "
"objects used in the job but before updating the reservations with your own "
"fences."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1026
msgid "check whether fence is the job's dependency"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1027
msgid "scheduler job to check"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1028
msgid "fence to look for"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1030
msgid ""
"True if **fence** is found within the job's dependencies, or otherwise false."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1049
msgid "clean up scheduler job resources"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1050
msgid "scheduler job to clean up"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1051
msgid "Cleans up the resources allocated with drm_sched_job_init()."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1053
msgid ""
"Drivers should call this from their error unwind code if **job** is aborted "
"before drm_sched_job_arm() is called."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1056
msgid ""
"drm_sched_job_arm() is a point of no return since it initializes the fences "
"and their sequence number etc. Once that function has been called, you "
"*must* submit it with drm_sched_entity_push_job() and cannot simply abort it "
"by calling drm_sched_job_cleanup()."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1061
msgid ""
"This function should be called in the :c:type:`drm_sched_backend_ops."
"free_job <drm_sched_backend_ops>` callback."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1179
msgid "Get a drm sched from a sched_list with the least load"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1185
#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:42
#: drivers/gpu/drm/scheduler/sched_entity.c:121
msgid "``struct drm_gpu_scheduler **sched_list``"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1180
msgid "list of drm_gpu_schedulers"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1182
#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:44
#: drivers/gpu/drm/scheduler/sched_entity.c:123
msgid "``unsigned int num_sched_list``"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1181
msgid "number of drm_gpu_schedulers in the sched_list"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1182
msgid ""
"Returns pointer of the sched with the least load or NULL if none of the "
"drm_gpu_schedulers are ready"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1312
msgid "Init a gpu scheduler instance"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1316
msgid "``const struct drm_sched_init_args *args``"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1315
msgid "scheduler initialization arguments"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1316
msgid "Return 0 on success, otherwise error code."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1410
msgid "Destroy a gpu scheduler"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1413
msgid "Tears down and cleans up the scheduler."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1415
msgid ""
"This stops submission of new jobs to the hardware through :c:type:`struct "
"drm_sched_backend_ops <drm_sched_backend_ops>`.run_job. If :c:type:`struct "
"drm_sched_backend_ops <drm_sched_backend_ops>`.cancel_job is implemented, "
"all jobs will be canceled through it and afterwards cleaned up through :c:"
"type:`struct drm_sched_backend_ops <drm_sched_backend_ops>`.free_job. If "
"cancel_job is not implemented, memory could leak."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1466
msgid "Update sched_entity guilty flag"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1468
msgid "The job guilty of time out"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1469
msgid ""
"Increment on every hang caused by the 'bad' job. If this exceeds the hang "
"limit of the scheduler then the respective sched entity is marked guilty and "
"jobs from it will not be scheduled further"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1509
msgid "Is the scheduler ready for submission"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1512
msgid "Returns true if submission is ready"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1522
msgid "stop scheduler submission"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1524
msgid ""
"Stops the scheduler from pulling new jobs from entities. It also stops "
"freeing jobs automatically through drm_sched_backend_ops.free_job()."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1537
msgid "start scheduler submission"
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1539
msgid "Restarts the scheduler after drm_sched_wqueue_stop() has stopped it."
msgstr ""

#: ../../../gpu/drm-mm:571: drivers/gpu/drm/scheduler/sched_main.c:1541
msgid ""
"This function is not necessary for 'conventional' startup. The scheduler is "
"fully operational after drm_sched_init() succeeded."
msgstr ""

#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:36
msgid "Init a context entity used by scheduler when submit to HW ring."
msgstr ""

#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:39
#: drivers/gpu/drm/scheduler/sched_entity.c:119
msgid "scheduler entity to init"
msgstr ""

#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:41
#: drivers/gpu/drm/scheduler/sched_entity.c:378
msgid "``enum drm_sched_priority priority``"
msgstr ""

#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:40
msgid "priority of the entity"
msgstr ""

#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:41
msgid "the list of drm scheds on which jobs from this entity can be submitted"
msgstr ""

#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:43
#: drivers/gpu/drm/scheduler/sched_entity.c:122
msgid "number of drm sched in sched_list"
msgstr ""

#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:45
msgid "``atomic_t *guilty``"
msgstr ""

#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:44
msgid ""
"atomic_t set to 1 when a job on this queue is found to be guilty causing a "
"timeout"
msgstr ""

#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:46
msgid ""
"Note that the :c:type:`sched_list` must have at least one element to "
"schedule the entity."
msgstr ""

#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:48
msgid ""
"For changing **priority** later on at runtime see "
"drm_sched_entity_set_priority(). For changing the set of schedulers "
"**sched_list** at runtime see drm_sched_entity_modify_sched()."
msgstr ""

#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:52
msgid ""
"An entity is cleaned up by calling drm_sched_entity_fini(). See also "
"drm_sched_entity_destroy()."
msgstr ""

#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:118
msgid "Modify sched of an entity"
msgstr ""

#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:120
msgid ""
"the list of new drm scheds which will replace existing entity->sched_list"
msgstr ""

#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:123
msgid ""
"Note that this must be called under the same common lock for **entity** as "
"drm_sched_job_arm() and drm_sched_entity_push_job(), or the driver needs to "
"guarantee through some other means that this is never called while new jobs "
"can be pushed to **entity**."
msgstr ""

#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:155
msgid "return error of last scheduled job"
msgstr ""

#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:156
msgid "scheduler entity to check"
msgstr ""

#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:157
msgid ""
"Opportunistically return the error of the last scheduled job. Result can "
"change any time when new jobs are pushed to the hw."
msgstr ""

#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:267
msgid "Flush a context entity"
msgstr ""

#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:269
#: drivers/gpu/drm/scheduler/sched_entity.c:316
#: drivers/gpu/drm/scheduler/sched_entity.c:346
#: drivers/gpu/drm/scheduler/sched_entity.c:376
msgid "scheduler entity"
msgstr ""

#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:271
msgid "``long timeout``"
msgstr ""

#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:270
msgid "time to wait in for Q to become empty in jiffies."
msgstr ""

#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:271
msgid ""
"Splitting drm_sched_entity_fini() into two functions, The first one does the "
"waiting, removes the entity from the runqueue and returns an error when the "
"process was killed."
msgstr ""

#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:275
msgid "Returns the remaining time in jiffies left from the input timeout"
msgstr ""

#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:314
#: drivers/gpu/drm/scheduler/sched_entity.c:345
msgid "Destroy a context entity"
msgstr ""

#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:317
msgid ""
"Cleanups up **entity** which has been initialized by drm_sched_entity_init()."
msgstr ""

#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:319
msgid ""
"If there are potentially job still in flight or getting newly queued "
"drm_sched_entity_flush() must be called first. This function then goes over "
"the entity and signals all jobs with an error code if the process was killed."
msgstr ""

#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:347
msgid ""
"Calls drm_sched_entity_flush() and drm_sched_entity_fini() as a convenience "
"wrapper."
msgstr ""

#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:374
msgid "Sets priority of the entity"
msgstr ""

#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:377
msgid "scheduler priority"
msgstr ""

#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:378
msgid "Update the priority of runqueues used for the entity."
msgstr ""

#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:562
msgid "Submit a job to the entity's job queue"
msgstr ""

#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:568
msgid "``struct drm_sched_job *sched_job``"
msgstr ""

#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:563
msgid "job to submit"
msgstr ""

#: ../../../gpu/drm-mm:574: drivers/gpu/drm/scheduler/sched_entity.c:565
msgid ""
"To guarantee that the order of insertion to queue matches the job's fence "
"sequence number this function should be called with drm_sched_job_arm() "
"under common lock for the struct drm_sched_entity that was set up for "
"**sched_job** in drm_sched_job_init()."
msgstr ""
