# SOME DESCRIPTIVE TITLE.
# Copyright (C) The kernel development community
# This file is distributed under the same license as the The Linux Kernel package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: The Linux Kernel master\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-12-01 08:33+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: \n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../core-api/workqueue.rst:3
msgid "Workqueue"
msgstr ""

#: ../../../core-api/workqueue.rst:0
msgid "Date"
msgstr ""

#: ../../../core-api/workqueue.rst:5
msgid "September, 2010"
msgstr ""

#: ../../../core-api/workqueue.rst:0
msgid "Author"
msgstr ""

#: ../../../core-api/workqueue.rst:6
msgid "Tejun Heo <tj@kernel.org>"
msgstr ""

#: ../../../core-api/workqueue.rst:7
msgid "Florian Mickler <florian@mickler.org>"
msgstr ""

#: ../../../core-api/workqueue.rst:11
msgid "Introduction"
msgstr ""

#: ../../../core-api/workqueue.rst:13
msgid ""
"There are many cases where an asynchronous process execution context is "
"needed and the workqueue (wq) API is the most commonly used mechanism for "
"such cases."
msgstr ""

#: ../../../core-api/workqueue.rst:17
msgid ""
"When such an asynchronous execution context is needed, a work item "
"describing which function to execute is put on a queue.  An independent "
"thread serves as the asynchronous execution context.  The queue is called "
"workqueue and the thread is called worker."
msgstr ""

#: ../../../core-api/workqueue.rst:22
msgid ""
"While there are work items on the workqueue the worker executes the "
"functions associated with the work items one after the other.  When there is "
"no work item left on the workqueue the worker becomes idle. When a new work "
"item gets queued, the worker begins executing again."
msgstr ""

#: ../../../core-api/workqueue.rst:29
msgid "Why Concurrency Managed Workqueue?"
msgstr ""

#: ../../../core-api/workqueue.rst:31
msgid ""
"In the original wq implementation, a multi threaded (MT) wq had one worker "
"thread per CPU and a single threaded (ST) wq had one worker thread system-"
"wide.  A single MT wq needed to keep around the same number of workers as "
"the number of CPUs.  The kernel grew a lot of MT wq users over the years and "
"with the number of CPU cores continuously rising, some systems saturated the "
"default 32k PID space just booting up."
msgstr ""

#: ../../../core-api/workqueue.rst:39
msgid ""
"Although MT wq wasted a lot of resource, the level of concurrency provided "
"was unsatisfactory.  The limitation was common to both ST and MT wq albeit "
"less severe on MT.  Each wq maintained its own separate worker pool.  An MT "
"wq could provide only one execution context per CPU while an ST wq one for "
"the whole system.  Work items had to compete for those very limited "
"execution contexts leading to various problems including proneness to "
"deadlocks around the single execution context."
msgstr ""

#: ../../../core-api/workqueue.rst:47
msgid ""
"The tension between the provided level of concurrency and resource usage "
"also forced its users to make unnecessary tradeoffs like libata choosing to "
"use ST wq for polling PIOs and accepting an unnecessary limitation that no "
"two polling PIOs can progress at the same time.  As MT wq don't provide much "
"better concurrency, users which require higher level of concurrency, like "
"async or fscache, had to implement their own thread pool."
msgstr ""

#: ../../../core-api/workqueue.rst:55
msgid ""
"Concurrency Managed Workqueue (cmwq) is a reimplementation of wq with focus "
"on the following goals."
msgstr ""

#: ../../../core-api/workqueue.rst:58
msgid "Maintain compatibility with the original workqueue API."
msgstr ""

#: ../../../core-api/workqueue.rst:60
msgid ""
"Use per-CPU unified worker pools shared by all wq to provide flexible level "
"of concurrency on demand without wasting a lot of resource."
msgstr ""

#: ../../../core-api/workqueue.rst:64
msgid ""
"Automatically regulate worker pool and level of concurrency so that the API "
"users don't need to worry about such details."
msgstr ""

#: ../../../core-api/workqueue.rst:69
msgid "The Design"
msgstr ""

#: ../../../core-api/workqueue.rst:71
msgid ""
"In order to ease the asynchronous execution of functions a new abstraction, "
"the work item, is introduced."
msgstr ""

#: ../../../core-api/workqueue.rst:74
msgid ""
"A work item is a simple struct that holds a pointer to the function that is "
"to be executed asynchronously.  Whenever a driver or subsystem wants a "
"function to be executed asynchronously it has to set up a work item pointing "
"to that function and queue that work item on a workqueue."
msgstr ""

#: ../../../core-api/workqueue.rst:80
msgid ""
"A work item can be executed in either a thread or the BH (softirq) context."
msgstr ""

#: ../../../core-api/workqueue.rst:82
msgid ""
"For threaded workqueues, special purpose threads, called [k]workers, execute "
"the functions off of the queue, one after the other. If no work is queued, "
"the worker threads become idle. These worker threads are managed in worker-"
"pools."
msgstr ""

#: ../../../core-api/workqueue.rst:87
msgid ""
"The cmwq design differentiates between the user-facing workqueues that "
"subsystems and drivers queue work items on and the backend mechanism which "
"manages worker-pools and processes the queued work items."
msgstr ""

#: ../../../core-api/workqueue.rst:91
msgid ""
"There are two worker-pools, one for normal work items and the other for high "
"priority ones, for each possible CPU and some extra worker-pools to serve "
"work items queued on unbound workqueues - the number of these backing pools "
"is dynamic."
msgstr ""

#: ../../../core-api/workqueue.rst:96
msgid ""
"BH workqueues use the same framework. However, as there can only be one "
"concurrent execution context, there's no need to worry about concurrency. "
"Each per-CPU BH worker pool contains only one pseudo worker which represents "
"the BH execution context. A BH workqueue can be considered a convenience "
"interface to softirq."
msgstr ""

#: ../../../core-api/workqueue.rst:102
msgid ""
"Subsystems and drivers can create and queue work items through special "
"workqueue API functions as they see fit. They can influence some aspects of "
"the way the work items are executed by setting flags on the workqueue they "
"are putting the work item on. These flags include things like CPU locality, "
"concurrency limits, priority and more.  To get a detailed overview refer to "
"the API description of ``alloc_workqueue()`` below."
msgstr ""

#: ../../../core-api/workqueue.rst:110
msgid ""
"When a work item is queued to a workqueue, the target worker-pool is "
"determined according to the queue parameters and workqueue attributes and "
"appended on the shared worklist of the worker-pool.  For example, unless "
"specifically overridden, a work item of a bound workqueue will be queued on "
"the worklist of either normal or highpri worker-pool that is associated to "
"the CPU the issuer is running on."
msgstr ""

#: ../../../core-api/workqueue.rst:117
msgid ""
"For any thread pool implementation, managing the concurrency level (how many "
"execution contexts are active) is an important issue.  cmwq tries to keep "
"the concurrency at a minimal but sufficient level. Minimal to save resources "
"and sufficient in that the system is used at its full capacity."
msgstr ""

#: ../../../core-api/workqueue.rst:123
msgid ""
"Each worker-pool bound to an actual CPU implements concurrency management by "
"hooking into the scheduler.  The worker-pool is notified whenever an active "
"worker wakes up or sleeps and keeps track of the number of the currently "
"runnable workers.  Generally, work items are not expected to hog a CPU and "
"consume many cycles.  That means maintaining just enough concurrency to "
"prevent work processing from stalling should be optimal.  As long as there "
"are one or more runnable workers on the CPU, the worker-pool doesn't start "
"execution of a new work, but, when the last running worker goes to sleep, it "
"immediately schedules a new worker so that the CPU doesn't sit idle while "
"there are pending work items.  This allows using a minimal number of workers "
"without losing execution bandwidth."
msgstr ""

#: ../../../core-api/workqueue.rst:136
msgid ""
"Keeping idle workers around doesn't cost other than the memory space for "
"kthreads, so cmwq holds onto idle ones for a while before killing them."
msgstr ""

#: ../../../core-api/workqueue.rst:140
msgid ""
"For unbound workqueues, the number of backing pools is dynamic. Unbound "
"workqueue can be assigned custom attributes using "
"``apply_workqueue_attrs()`` and workqueue will automatically create backing "
"worker pools matching the attributes.  The responsibility of regulating "
"concurrency level is on the users.  There is also a flag to mark a bound wq "
"to ignore the concurrency management.  Please refer to the API section for "
"details."
msgstr ""

#: ../../../core-api/workqueue.rst:148
msgid ""
"Forward progress guarantee relies on that workers can be created when more "
"execution contexts are necessary, which in turn is guaranteed through the "
"use of rescue workers.  All work items which might be used on code paths "
"that handle memory reclaim are required to be queued on wq's that have a "
"rescue-worker reserved for execution under memory pressure.  Else it is "
"possible that the worker-pool deadlocks waiting for execution contexts to "
"free up."
msgstr ""

#: ../../../core-api/workqueue.rst:158
msgid "Application Programming Interface (API)"
msgstr ""

#: ../../../core-api/workqueue.rst:160
msgid ""
"``alloc_workqueue()`` allocates a wq.  The original ``create_*workqueue()`` "
"functions are deprecated and scheduled for removal.  ``alloc_workqueue()`` "
"takes three arguments - ``@name``, ``@flags`` and ``@max_active``.  "
"``@name`` is the name of the wq and also used as the name of the rescuer "
"thread if there is one."
msgstr ""

#: ../../../core-api/workqueue.rst:166
msgid ""
"A wq no longer manages execution resources but serves as a domain for "
"forward progress guarantee, flush and work item attributes. ``@flags`` and "
"``@max_active`` control how work items are assigned execution resources, "
"scheduled and executed."
msgstr ""

#: ../../../core-api/workqueue.rst:173 ../../../core-api/workqueue:787:
#: include/linux/workqueue.h:541 include/linux/workqueue.h:560
msgid "``flags``"
msgstr ""

#: ../../../core-api/workqueue.rst:175
msgid "``WQ_BH``"
msgstr ""

#: ../../../core-api/workqueue.rst:176
msgid ""
"BH workqueues can be considered a convenience interface to softirq. BH "
"workqueues are always per-CPU and all BH work items are executed in the "
"queueing CPU's softirq context in the queueing order."
msgstr ""

#: ../../../core-api/workqueue.rst:180
msgid ""
"All BH workqueues must have 0 ``max_active`` and ``WQ_HIGHPRI`` is the only "
"allowed additional flag."
msgstr ""

#: ../../../core-api/workqueue.rst:183
msgid ""
"BH work items cannot sleep. All other features such as delayed queueing, "
"flushing and canceling are supported."
msgstr ""

#: ../../../core-api/workqueue.rst:186
msgid "``WQ_PERCPU``"
msgstr ""

#: ../../../core-api/workqueue.rst:187
msgid ""
"Work items queued to a per-cpu wq are bound to a specific CPU. This flag is "
"the right choice when cpu locality is important."
msgstr ""

#: ../../../core-api/workqueue.rst:190
msgid "This flag is the complement of ``WQ_UNBOUND``."
msgstr ""

#: ../../../core-api/workqueue.rst:192
msgid "``WQ_UNBOUND``"
msgstr ""

#: ../../../core-api/workqueue.rst:193
msgid ""
"Work items queued to an unbound wq are served by the special worker-pools "
"which host workers which are not bound to any specific CPU.  This makes the "
"wq behave as a simple execution context provider without concurrency "
"management.  The unbound worker-pools try to start execution of work items "
"as soon as possible.  Unbound wq sacrifices locality but is useful for the "
"following cases."
msgstr ""

#: ../../../core-api/workqueue.rst:201
msgid ""
"Wide fluctuation in the concurrency level requirement is expected and using "
"bound wq may end up creating large number of mostly unused workers across "
"different CPUs as the issuer hops through different CPUs."
msgstr ""

#: ../../../core-api/workqueue.rst:206
msgid ""
"Long running CPU intensive workloads which can be better managed by the "
"system scheduler."
msgstr ""

#: ../../../core-api/workqueue.rst:209
msgid "``WQ_FREEZABLE``"
msgstr ""

#: ../../../core-api/workqueue.rst:210
msgid ""
"A freezable wq participates in the freeze phase of the system suspend "
"operations.  Work items on the wq are drained and no new work item starts "
"execution until thawed."
msgstr ""

#: ../../../core-api/workqueue.rst:214
msgid "``WQ_MEM_RECLAIM``"
msgstr ""

#: ../../../core-api/workqueue.rst:215
msgid ""
"All wq which might be used in the memory reclaim paths **MUST** have this "
"flag set.  The wq is guaranteed to have at least one execution context "
"regardless of memory pressure."
msgstr ""

#: ../../../core-api/workqueue.rst:219
msgid "``WQ_HIGHPRI``"
msgstr ""

#: ../../../core-api/workqueue.rst:220
msgid ""
"Work items of a highpri wq are queued to the highpri worker-pool of the "
"target cpu.  Highpri worker-pools are served by worker threads with elevated "
"nice level."
msgstr ""

#: ../../../core-api/workqueue.rst:224
msgid ""
"Note that normal and highpri worker-pools don't interact with each other.  "
"Each maintains its separate pool of workers and implements concurrency "
"management among its workers."
msgstr ""

#: ../../../core-api/workqueue.rst:228
msgid "``WQ_CPU_INTENSIVE``"
msgstr ""

#: ../../../core-api/workqueue.rst:229
msgid ""
"Work items of a CPU intensive wq do not contribute to the concurrency "
"level.  In other words, runnable CPU intensive work items will not prevent "
"other work items in the same worker-pool from starting execution.  This is "
"useful for bound work items which are expected to hog CPU cycles so that "
"their execution is regulated by the system scheduler."
msgstr ""

#: ../../../core-api/workqueue.rst:236
msgid ""
"Although CPU intensive work items don't contribute to the concurrency level, "
"start of their executions is still regulated by the concurrency management "
"and runnable non-CPU-intensive work items can delay execution of CPU "
"intensive work items."
msgstr ""

#: ../../../core-api/workqueue.rst:242
msgid "This flag is meaningless for unbound wq."
msgstr ""

#: ../../../core-api/workqueue.rst:246
msgid "``max_active``"
msgstr ""

#: ../../../core-api/workqueue.rst:248
msgid ""
"``@max_active`` determines the maximum number of execution contexts per CPU "
"which can be assigned to the work items of a wq. For example, with "
"``@max_active`` of 16, at most 16 work items of the wq can be executing at "
"the same time per CPU. This is always a per-CPU attribute, even for unbound "
"workqueues."
msgstr ""

#: ../../../core-api/workqueue.rst:254
msgid ""
"The maximum limit for ``@max_active`` is 2048 and the default value used "
"when 0 is specified is 1024. These values are chosen sufficiently high such "
"that they are not the limiting factor while providing protection in runaway "
"cases."
msgstr ""

#: ../../../core-api/workqueue.rst:259
msgid ""
"The number of active work items of a wq is usually regulated by the users of "
"the wq, more specifically, by how many work items the users may queue at the "
"same time.  Unless there is a specific need for throttling the number of "
"active work items, specifying '0' is recommended."
msgstr ""

#: ../../../core-api/workqueue.rst:265
msgid ""
"Some users depend on strict execution ordering where only one work item is "
"in flight at any given time and the work items are processed in queueing "
"order. While the combination of ``@max_active`` of 1 and ``WQ_UNBOUND`` used "
"to achieve this behavior, this is no longer the case. Use "
"alloc_ordered_workqueue() instead."
msgstr ""

#: ../../../core-api/workqueue.rst:273
msgid "Example Execution Scenarios"
msgstr ""

#: ../../../core-api/workqueue.rst:275
msgid ""
"The following example execution scenarios try to illustrate how cmwq behave "
"under different configurations."
msgstr ""

#: ../../../core-api/workqueue.rst:278
msgid ""
"Work items w0, w1, w2 are queued to a bound wq q0 on the same CPU. w0 burns "
"CPU for 5ms then sleeps for 10ms then burns CPU for 5ms again before "
"finishing.  w1 and w2 burn CPU for 5ms then sleep for 10ms."
msgstr ""

#: ../../../core-api/workqueue.rst:283
msgid ""
"Ignoring all other tasks, works and processing overhead, and assuming simple "
"FIFO scheduling, the following is one highly simplified version of possible "
"sequences of events with the original wq. ::"
msgstr ""

#: ../../../core-api/workqueue.rst:299
msgid "And with cmwq with ``@max_active`` >= 3, ::"
msgstr ""

#: ../../../core-api/workqueue.rst:313
msgid "If ``@max_active`` == 2, ::"
msgstr ""

#: ../../../core-api/workqueue.rst:327
msgid ""
"Now, let's assume w1 and w2 are queued to a different wq q1 which has "
"``WQ_CPU_INTENSIVE`` set, ::"
msgstr ""

#: ../../../core-api/workqueue.rst:343
msgid "Guidelines"
msgstr ""

#: ../../../core-api/workqueue.rst:345
msgid ""
"Do not forget to use ``WQ_MEM_RECLAIM`` if a wq may process work items which "
"are used during memory reclaim.  Each wq with ``WQ_MEM_RECLAIM`` set has an "
"execution context reserved for it.  If there is dependency among multiple "
"work items used during memory reclaim, they should be queued to separate wq "
"each with ``WQ_MEM_RECLAIM``."
msgstr ""

#: ../../../core-api/workqueue.rst:352
msgid "Unless strict ordering is required, there is no need to use ST wq."
msgstr ""

#: ../../../core-api/workqueue.rst:354
msgid ""
"Unless there is a specific need, using 0 for @max_active is recommended.  In "
"most use cases, concurrency level usually stays well under the default limit."
msgstr ""

#: ../../../core-api/workqueue.rst:358
msgid ""
"A wq serves as a domain for forward progress guarantee (``WQ_MEM_RECLAIM``, "
"flush and work item attributes.  Work items which are not involved in memory "
"reclaim and don't need to be flushed as a part of a group of work items, and "
"don't require any special attribute, can use one of the system wq.  There is "
"no difference in execution characteristics between using a dedicated wq and "
"a system wq."
msgstr ""

#: ../../../core-api/workqueue.rst:366
msgid ""
"Note: If something may generate more than @max_active outstanding work items "
"(do stress test your producers), it may saturate a system wq and potentially "
"lead to deadlock. It should utilize its own dedicated workqueue rather than "
"the system wq."
msgstr ""

#: ../../../core-api/workqueue.rst:371
msgid ""
"Unless work items are expected to consume a huge amount of CPU cycles, using "
"a bound wq is usually beneficial due to the increased level of locality in "
"wq operations and work item execution."
msgstr ""

#: ../../../core-api/workqueue.rst:377
msgid "Affinity Scopes"
msgstr ""

#: ../../../core-api/workqueue.rst:379
msgid ""
"An unbound workqueue groups CPUs according to its affinity scope to improve "
"cache locality. For example, if a workqueue is using the default affinity "
"scope of \"cache\", it will group CPUs according to last level cache "
"boundaries. A work item queued on the workqueue will be assigned to a worker "
"on one of the CPUs which share the last level cache with the issuing CPU. "
"Once started, the worker may or may not be allowed to move outside the scope "
"depending on the ``affinity_strict`` setting of the scope."
msgstr ""

#: ../../../core-api/workqueue.rst:387
msgid "Workqueue currently supports the following affinity scopes."
msgstr ""

#: ../../../core-api/workqueue.rst:389
msgid "``default``"
msgstr ""

#: ../../../core-api/workqueue.rst:390
msgid ""
"Use the scope in module parameter ``workqueue.default_affinity_scope`` which "
"is always set to one of the scopes below."
msgstr ""

#: ../../../core-api/workqueue.rst:393
msgid "``cpu``"
msgstr ""

#: ../../../core-api/workqueue.rst:394
msgid ""
"CPUs are not grouped. A work item issued on one CPU is processed by a worker "
"on the same CPU. This makes unbound workqueues behave as per-cpu workqueues "
"without concurrency management."
msgstr ""

#: ../../../core-api/workqueue.rst:398
msgid "``smt``"
msgstr ""

#: ../../../core-api/workqueue.rst:399
msgid ""
"CPUs are grouped according to SMT boundaries. This usually means that the "
"logical threads of each physical CPU core are grouped together."
msgstr ""

#: ../../../core-api/workqueue.rst:402
msgid "``cache``"
msgstr ""

#: ../../../core-api/workqueue.rst:403
msgid ""
"CPUs are grouped according to cache boundaries. Which specific cache "
"boundary is used is determined by the arch code. L3 is used in a lot of "
"cases. This is the default affinity scope."
msgstr ""

#: ../../../core-api/workqueue.rst:407
msgid "``numa``"
msgstr ""

#: ../../../core-api/workqueue.rst:408
msgid "CPUs are grouped according to NUMA boundaries."
msgstr ""

#: ../../../core-api/workqueue.rst:410
msgid "``system``"
msgstr ""

#: ../../../core-api/workqueue.rst:411
msgid ""
"All CPUs are put in the same group. Workqueue makes no effort to process a "
"work item on a CPU close to the issuing CPU."
msgstr ""

#: ../../../core-api/workqueue.rst:414
msgid ""
"The default affinity scope can be changed with the module parameter "
"``workqueue.default_affinity_scope`` and a specific workqueue's affinity "
"scope can be changed using ``apply_workqueue_attrs()``."
msgstr ""

#: ../../../core-api/workqueue.rst:418
msgid ""
"If ``WQ_SYSFS`` is set, the workqueue will have the following affinity scope "
"related interface files under its ``/sys/devices/virtual/workqueue/WQ_NAME/"
"`` directory."
msgstr ""

#: ../../../core-api/workqueue.rst:422
msgid "``affinity_scope``"
msgstr ""

#: ../../../core-api/workqueue.rst:423
msgid "Read to see the current affinity scope. Write to change."
msgstr ""

#: ../../../core-api/workqueue.rst:425
msgid ""
"When default is the current scope, reading this file will also show the "
"current effective scope in parentheses, for example, ``default (cache)``."
msgstr ""

#: ../../../core-api/workqueue.rst:428
msgid "``affinity_strict``"
msgstr ""

#: ../../../core-api/workqueue.rst:429
msgid ""
"0 by default indicating that affinity scopes are not strict. When a work "
"item starts execution, workqueue makes a best-effort attempt to ensure that "
"the worker is inside its affinity scope, which is called repatriation. Once "
"started, the scheduler is free to move the worker anywhere in the system as "
"it sees fit. This enables benefiting from scope locality while still being "
"able to utilize other CPUs if necessary and available."
msgstr ""

#: ../../../core-api/workqueue.rst:437
msgid ""
"If set to 1, all workers of the scope are guaranteed always to be in the "
"scope. This may be useful when crossing affinity scopes has other "
"implications, for example, in terms of power consumption or workload "
"isolation. Strict NUMA scope can also be used to match the workqueue "
"behavior of older kernels."
msgstr ""

#: ../../../core-api/workqueue.rst:445
msgid "Affinity Scopes and Performance"
msgstr ""

#: ../../../core-api/workqueue.rst:447
msgid ""
"It'd be ideal if an unbound workqueue's behavior is optimal for vast "
"majority of use cases without further tuning. Unfortunately, in the current "
"kernel, there exists a pronounced trade-off between locality and utilization "
"necessitating explicit configurations when workqueues are heavily used."
msgstr ""

#: ../../../core-api/workqueue.rst:452
msgid ""
"Higher locality leads to higher efficiency where more work is performed for "
"the same number of consumed CPU cycles. However, higher locality may also "
"cause lower overall system utilization if the work items are not spread "
"enough across the affinity scopes by the issuers. The following performance "
"testing with dm-crypt clearly illustrates this trade-off."
msgstr ""

#: ../../../core-api/workqueue.rst:458
msgid ""
"The tests are run on a CPU with 12-cores/24-threads split across four L3 "
"caches (AMD Ryzen 9 3900x). CPU clock boost is turned off for consistency. "
"``/dev/dm-0`` is a dm-crypt device created on NVME SSD (Samsung 990 PRO) and "
"opened with ``cryptsetup`` with default settings."
msgstr ""

#: ../../../core-api/workqueue.rst:465
msgid "Scenario 1: Enough issuers and work spread across the machine"
msgstr ""

#: ../../../core-api/workqueue.rst:467 ../../../core-api/workqueue.rst:509
#: ../../../core-api/workqueue.rst:552
msgid "The command used: ::"
msgstr ""

#: ../../../core-api/workqueue.rst:473
msgid ""
"There are 24 issuers, each issuing 64 IOs concurrently. ``--verify=sha512`` "
"makes ``fio`` generate and read back the content each time which makes "
"execution locality matter between the issuer and ``kcryptd``. The following "
"are the read bandwidths and CPU utilizations depending on different affinity "
"scope settings on ``kcryptd`` measured over five runs. Bandwidths are in "
"MiBps, and CPU util in percents."
msgstr ""

#: ../../../core-api/workqueue.rst:484 ../../../core-api/workqueue.rst:523
#: ../../../core-api/workqueue.rst:566
msgid "Affinity"
msgstr ""

#: ../../../core-api/workqueue.rst:485 ../../../core-api/workqueue.rst:524
#: ../../../core-api/workqueue.rst:567
msgid "Bandwidth (MiBps)"
msgstr ""

#: ../../../core-api/workqueue.rst:486 ../../../core-api/workqueue.rst:525
#: ../../../core-api/workqueue.rst:568
msgid "CPU util (%)"
msgstr ""

#: ../../../core-api/workqueue.rst:488 ../../../core-api/workqueue.rst:527
#: ../../../core-api/workqueue.rst:570
msgid "system"
msgstr ""

#: ../../../core-api/workqueue.rst:489
msgid "1159.40 ±1.34"
msgstr ""

#: ../../../core-api/workqueue.rst:490
msgid "99.31 ±0.02"
msgstr ""

#: ../../../core-api/workqueue.rst:492 ../../../core-api/workqueue.rst:531
#: ../../../core-api/workqueue.rst:574
msgid "cache"
msgstr ""

#: ../../../core-api/workqueue.rst:493
msgid "1166.40 ±0.89"
msgstr ""

#: ../../../core-api/workqueue.rst:494
msgid "99.34 ±0.01"
msgstr ""

#: ../../../core-api/workqueue.rst:496 ../../../core-api/workqueue.rst:535
#: ../../../core-api/workqueue.rst:578
msgid "cache (strict)"
msgstr ""

#: ../../../core-api/workqueue.rst:497
msgid "1166.00 ±0.71"
msgstr ""

#: ../../../core-api/workqueue.rst:498
msgid "99.35 ±0.01"
msgstr ""

#: ../../../core-api/workqueue.rst:500
msgid ""
"With enough issuers spread across the system, there is no downside to "
"\"cache\", strict or otherwise. All three configurations saturate the whole "
"machine but the cache-affine ones outperform by 0.6% thanks to improved "
"locality."
msgstr ""

#: ../../../core-api/workqueue.rst:507
msgid "Scenario 2: Fewer issuers, enough work for saturation"
msgstr ""

#: ../../../core-api/workqueue.rst:515
msgid ""
"The only difference from the previous scenario is ``--numjobs=8``. There are "
"a third of the issuers but is still enough total work to saturate the system."
msgstr ""

#: ../../../core-api/workqueue.rst:528
msgid "1155.40 ±0.89"
msgstr ""

#: ../../../core-api/workqueue.rst:529
msgid "97.41 ±0.05"
msgstr ""

#: ../../../core-api/workqueue.rst:532
msgid "1154.40 ±1.14"
msgstr ""

#: ../../../core-api/workqueue.rst:533
msgid "96.15 ±0.09"
msgstr ""

#: ../../../core-api/workqueue.rst:536
msgid "1112.00 ±4.64"
msgstr ""

#: ../../../core-api/workqueue.rst:537
msgid "93.26 ±0.35"
msgstr ""

#: ../../../core-api/workqueue.rst:539
msgid ""
"This is more than enough work to saturate the system. Both \"system\" and "
"\"cache\" are nearly saturating the machine but not fully. \"cache\" is "
"using less CPU but the better efficiency puts it at the same bandwidth as "
"\"system\"."
msgstr ""

#: ../../../core-api/workqueue.rst:544
msgid ""
"Eight issuers moving around over four L3 cache scope still allow \"cache "
"(strict)\" to mostly saturate the machine but the loss of work conservation "
"is now starting to hurt with 3.7% bandwidth loss."
msgstr ""

#: ../../../core-api/workqueue.rst:550
msgid "Scenario 3: Even fewer issuers, not enough work to saturate"
msgstr ""

#: ../../../core-api/workqueue.rst:558
msgid ""
"Again, the only difference is ``--numjobs=4``. With the number of issuers "
"reduced to four, there now isn't enough work to saturate the whole system "
"and the bandwidth becomes dependent on completion latencies."
msgstr ""

#: ../../../core-api/workqueue.rst:571
msgid "993.60 ±1.82"
msgstr ""

#: ../../../core-api/workqueue.rst:572
msgid "75.49 ±0.06"
msgstr ""

#: ../../../core-api/workqueue.rst:575
msgid "973.40 ±1.52"
msgstr ""

#: ../../../core-api/workqueue.rst:576
msgid "74.90 ±0.07"
msgstr ""

#: ../../../core-api/workqueue.rst:579
msgid "828.20 ±4.49"
msgstr ""

#: ../../../core-api/workqueue.rst:580
msgid "66.84 ±0.29"
msgstr ""

#: ../../../core-api/workqueue.rst:582
msgid ""
"Now, the tradeoff between locality and utilization is clearer. \"cache\" "
"shows 2% bandwidth loss compared to \"system\" and \"cache (struct)\" "
"whopping 20%."
msgstr ""

#: ../../../core-api/workqueue.rst:587
msgid "Conclusion and Recommendations"
msgstr ""

#: ../../../core-api/workqueue.rst:589
msgid ""
"In the above experiments, the efficiency advantage of the \"cache\" affinity "
"scope over \"system\" is, while consistent and noticeable, small. However, "
"the impact is dependent on the distances between the scopes and may be more "
"pronounced in processors with more complex topologies."
msgstr ""

#: ../../../core-api/workqueue.rst:594
msgid ""
"While the loss of work-conservation in certain scenarios hurts, it is a lot "
"better than \"cache (strict)\" and maximizing workqueue utilization is "
"unlikely to be the common case anyway. As such, \"cache\" is the default "
"affinity scope for unbound pools."
msgstr ""

#: ../../../core-api/workqueue.rst:599
msgid ""
"As there is no one option which is great for most cases, workqueue usages "
"that may consume a significant amount of CPU are recommended to configure "
"the workqueues using ``apply_workqueue_attrs()`` and/or enable ``WQ_SYSFS``."
msgstr ""

#: ../../../core-api/workqueue.rst:604
msgid ""
"An unbound workqueue with strict \"cpu\" affinity scope behaves the same as "
"``WQ_CPU_INTENSIVE`` per-cpu workqueue. There is no real advanage to the "
"latter and an unbound workqueue provides a lot more flexibility."
msgstr ""

#: ../../../core-api/workqueue.rst:608
msgid ""
"Affinity scopes are introduced in Linux v6.5. To emulate the previous "
"behavior, use strict \"numa\" affinity scope."
msgstr ""

#: ../../../core-api/workqueue.rst:611
msgid ""
"The loss of work-conservation in non-strict affinity scopes is likely "
"originating from the scheduler. There is no theoretical reason why the "
"kernel wouldn't be able to do the right thing and maintain work-conservation "
"in most cases. As such, it is possible that future scheduler improvements "
"may make most of these tunables unnecessary."
msgstr ""

#: ../../../core-api/workqueue.rst:619
msgid "Examining Configuration"
msgstr ""

#: ../../../core-api/workqueue.rst:621
msgid ""
"Use tools/workqueue/wq_dump.py to examine unbound CPU affinity "
"configuration, worker pools and how workqueues map to the pools: ::"
msgstr ""

#: ../../../core-api/workqueue.rst:692 ../../../core-api/workqueue.rst:723
msgid "See the command's help message for more info."
msgstr ""

#: ../../../core-api/workqueue.rst:696
msgid "Monitoring"
msgstr ""

#: ../../../core-api/workqueue.rst:698
msgid "Use tools/workqueue/wq_monitor.py to monitor workqueue operations: ::"
msgstr ""

#: ../../../core-api/workqueue.rst:727
msgid "Debugging"
msgstr ""

#: ../../../core-api/workqueue.rst:729
msgid ""
"Because the work functions are executed by generic worker threads there are "
"a few tricks needed to shed some light on misbehaving workqueue users."
msgstr ""

#: ../../../core-api/workqueue.rst:733
msgid "Worker threads show up in the process list as: ::"
msgstr ""

#: ../../../core-api/workqueue.rst:740
msgid ""
"If kworkers are going crazy (using too much cpu), there are two types of "
"possible problems:"
msgstr ""

#: ../../../core-api/workqueue.rst:743
msgid "Something being scheduled in rapid succession"
msgstr ""

#: ../../../core-api/workqueue.rst:744
msgid "A single work item that consumes lots of cpu cycles"
msgstr ""

#: ../../../core-api/workqueue.rst:746
msgid "The first one can be tracked using tracing: ::"
msgstr ""

#: ../../../core-api/workqueue.rst:753
msgid ""
"If something is busy looping on work queueing, it would be dominating the "
"output and the offender can be determined with the work item function."
msgstr ""

#: ../../../core-api/workqueue.rst:757
msgid ""
"For the second type of problems it should be possible to just check the "
"stack trace of the offending worker thread. ::"
msgstr ""

#: ../../../core-api/workqueue.rst:762
msgid ""
"The work item's function should be trivially visible in the stack trace."
msgstr ""

#: ../../../core-api/workqueue.rst:767
msgid "Non-reentrance Conditions"
msgstr ""

#: ../../../core-api/workqueue.rst:769
msgid ""
"Workqueue guarantees that a work item cannot be re-entrant if the following "
"conditions hold after a work item gets queued:"
msgstr ""

#: ../../../core-api/workqueue.rst:772
msgid "The work function hasn't been changed."
msgstr ""

#: ../../../core-api/workqueue.rst:773
msgid "No one queues the work item to another workqueue."
msgstr ""

#: ../../../core-api/workqueue.rst:774
msgid "The work item hasn't been reinitiated."
msgstr ""

#: ../../../core-api/workqueue.rst:776
msgid ""
"In other words, if the above conditions hold, the work item is guaranteed to "
"be executed by at most one worker system-wide at any given time."
msgstr ""

#: ../../../core-api/workqueue.rst:779
msgid ""
"Note that requeuing the work item (to the same queue) in the self function "
"doesn't break these conditions, so it's safe to do. Otherwise, caution is "
"required when breaking the conditions inside a work function."
msgstr ""

#: ../../../core-api/workqueue.rst:785
msgid "Kernel Inline Documentations Reference"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:143
msgid "A struct for workqueue attributes."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:147
msgid "**Definition**::"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:158
msgid "**Members**"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:149
msgid "``nice``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:150
msgid "nice level"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:154
msgid "``cpumask``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:155
msgid "allowed CPUs"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:157
msgid ""
"Work items in this workqueue are affine to these CPUs and not allowed to "
"execute on other CPUs. A pool serving a workqueue must have the same "
"**cpumask**."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:163
msgid "``__pod_cpumask``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:164
msgid "internal attribute used to create per-pod pools"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:166
msgid "Internal use only."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:168
msgid ""
"Per-pod unbound worker pools are used to improve locality. Always a subset "
"of ->cpumask. A workqueue can be associated with multiple worker pools with "
"disjoint **__pod_cpumask**'s. Whether the enforcement of a pool's "
"**__pod_cpumask** is strict depends on **affn_strict**."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:175
msgid "``affn_strict``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:176
msgid "affinity scope is strict"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:178
msgid ""
"If clear, workqueue will make a best-effort attempt at starting the worker "
"inside **__pod_cpumask** but the scheduler is free to migrate it outside."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:182
msgid "If set, workers are only allowed to run inside **__pod_cpumask**."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:195
msgid "``affn_scope``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:196
msgid "unbound CPU affinity scope"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:198
msgid ""
"CPU pods are used to improve execution locality of unbound work items. There "
"are multiple pod types, one for each wq_affn_scope, and every CPU in the "
"system belongs to one pod in every pod type. CPUs that belong to the same "
"pod share the worker pool. For example, selecting ``WQ_AFFN_NUMA`` makes the "
"workqueue use a separate worker pool for each NUMA node."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:207
msgid "``ordered``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:208
msgid "work items must be executed one by one in queueing order"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:211
#: include/linux/workqueue.h:482 include/linux/workqueue.h:524
#: include/linux/workqueue.h:544 include/linux/workqueue.h:562
#: include/linux/workqueue.h:648 include/linux/workqueue.h:678
#: include/linux/workqueue.h:693 include/linux/workqueue.h:707
#: include/linux/workqueue.h:718 include/linux/workqueue.h:738
#: include/linux/workqueue.h:804 include/linux/workqueue.h:818
#: ../../../core-api/workqueue:789: kernel/workqueue.c:3 kernel/workqueue.c:565
#: kernel/workqueue.c:582 kernel/workqueue.c:597 kernel/workqueue.c:709
#: kernel/workqueue.c:748 kernel/workqueue.c:874 kernel/workqueue.c:976
#: kernel/workqueue.c:998 kernel/workqueue.c:1032 kernel/workqueue.c:1066
#: kernel/workqueue.c:1087 kernel/workqueue.c:1135 kernel/workqueue.c:1173
#: kernel/workqueue.c:1236 kernel/workqueue.c:1389 kernel/workqueue.c:1422
#: kernel/workqueue.c:1468 kernel/workqueue.c:1518 kernel/workqueue.c:1550
#: kernel/workqueue.c:1575 kernel/workqueue.c:1626 kernel/workqueue.c:1640
#: kernel/workqueue.c:1659 kernel/workqueue.c:1710 kernel/workqueue.c:1785
#: kernel/workqueue.c:1809 kernel/workqueue.c:1850 kernel/workqueue.c:1930
#: kernel/workqueue.c:1981 kernel/workqueue.c:2028 kernel/workqueue.c:2145
#: kernel/workqueue.c:2174 kernel/workqueue.c:2374 kernel/workqueue.c:2405
#: kernel/workqueue.c:2436 kernel/workqueue.c:2539 kernel/workqueue.c:2577
#: kernel/workqueue.c:2671 kernel/workqueue.c:2725 kernel/workqueue.c:2769
#: kernel/workqueue.c:2870 kernel/workqueue.c:2904 kernel/workqueue.c:2942
#: kernel/workqueue.c:3038 kernel/workqueue.c:3112 kernel/workqueue.c:3153
#: kernel/workqueue.c:3327 kernel/workqueue.c:3364 kernel/workqueue.c:3450
#: kernel/workqueue.c:3723 kernel/workqueue.c:3772 kernel/workqueue.c:3848
#: kernel/workqueue.c:3963 kernel/workqueue.c:4120 kernel/workqueue.c:4284
#: kernel/workqueue.c:4302 kernel/workqueue.c:4414 kernel/workqueue.c:4438
#: kernel/workqueue.c:4460 kernel/workqueue.c:4475 kernel/workqueue.c:4493
#: kernel/workqueue.c:4512 kernel/workqueue.c:4539 kernel/workqueue.c:4552
#: kernel/workqueue.c:4565 kernel/workqueue.c:4577 kernel/workqueue.c:4616
#: kernel/workqueue.c:4640 kernel/workqueue.c:4786 kernel/workqueue.c:4952
#: kernel/workqueue.c:5031 kernel/workqueue.c:5224 kernel/workqueue.c:5412
#: kernel/workqueue.c:5439 kernel/workqueue.c:5630 kernel/workqueue.c:5871
#: kernel/workqueue.c:5970 kernel/workqueue.c:6004 kernel/workqueue.c:6062
#: kernel/workqueue.c:6099 kernel/workqueue.c:6134 kernel/workqueue.c:6157
#: kernel/workqueue.c:6591 kernel/workqueue.c:6648 kernel/workqueue.c:6784
#: kernel/workqueue.c:6951 kernel/workqueue.c:7281 kernel/workqueue.c:7386
#: kernel/workqueue.c:7453
msgid "**Description**"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:144
msgid "This can be used to change attributes of an unbound workqueue."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:354
msgid "``work_pending (work)``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:352
msgid "Find out whether a work item is currently pending"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:356
#: include/linux/workqueue.h:363 include/linux/workqueue.h:480
#: include/linux/workqueue.h:521 include/linux/workqueue.h:540
#: include/linux/workqueue.h:561 include/linux/workqueue.h:648
#: include/linux/workqueue.h:677 include/linux/workqueue.h:692
#: include/linux/workqueue.h:707 include/linux/workqueue.h:719
#: include/linux/workqueue.h:738 include/linux/workqueue.h:803
#: include/linux/workqueue.h:818 ../../../core-api/workqueue:789:
#: kernel/workqueue.c:565 kernel/workqueue.c:582 kernel/workqueue.c:597
#: kernel/workqueue.c:710 kernel/workqueue.c:749 kernel/workqueue.c:875
#: kernel/workqueue.c:976 kernel/workqueue.c:998 kernel/workqueue.c:1033
#: kernel/workqueue.c:1067 kernel/workqueue.c:1087 kernel/workqueue.c:1134
#: kernel/workqueue.c:1172 kernel/workqueue.c:1237 kernel/workqueue.c:1390
#: kernel/workqueue.c:1423 kernel/workqueue.c:1469 kernel/workqueue.c:1519
#: kernel/workqueue.c:1550 kernel/workqueue.c:1575 kernel/workqueue.c:1627
#: kernel/workqueue.c:1641 kernel/workqueue.c:1660 kernel/workqueue.c:1710
#: kernel/workqueue.c:1785 kernel/workqueue.c:1810 kernel/workqueue.c:1850
#: kernel/workqueue.c:1931 kernel/workqueue.c:1981 kernel/workqueue.c:2027
#: kernel/workqueue.c:2144 kernel/workqueue.c:2172 kernel/workqueue.c:2373
#: kernel/workqueue.c:2406 kernel/workqueue.c:2435 kernel/workqueue.c:2537
#: kernel/workqueue.c:2575 kernel/workqueue.c:2619 kernel/workqueue.c:2671
#: kernel/workqueue.c:2726 kernel/workqueue.c:2770 kernel/workqueue.c:2870
#: kernel/workqueue.c:2905 kernel/workqueue.c:2943 kernel/workqueue.c:3039
#: kernel/workqueue.c:3113 kernel/workqueue.c:3153 kernel/workqueue.c:3328
#: kernel/workqueue.c:3365 kernel/workqueue.c:3451 kernel/workqueue.c:3722
#: kernel/workqueue.c:3770 kernel/workqueue.c:3847 kernel/workqueue.c:3964
#: kernel/workqueue.c:4121 kernel/workqueue.c:4285 kernel/workqueue.c:4303
#: kernel/workqueue.c:4325 kernel/workqueue.c:4415 kernel/workqueue.c:4439
#: kernel/workqueue.c:4461 kernel/workqueue.c:4476 kernel/workqueue.c:4494
#: kernel/workqueue.c:4513 kernel/workqueue.c:4540 kernel/workqueue.c:4553
#: kernel/workqueue.c:4566 kernel/workqueue.c:4578 kernel/workqueue.c:4615
#: kernel/workqueue.c:4641 kernel/workqueue.c:4656 kernel/workqueue.c:4787
#: kernel/workqueue.c:4953 kernel/workqueue.c:5032 kernel/workqueue.c:5224
#: kernel/workqueue.c:5412 kernel/workqueue.c:5439 kernel/workqueue.c:5631
#: kernel/workqueue.c:5872 kernel/workqueue.c:5970 kernel/workqueue.c:6004
#: kernel/workqueue.c:6031 kernel/workqueue.c:6047 kernel/workqueue.c:6062
#: kernel/workqueue.c:6100 kernel/workqueue.c:6134 kernel/workqueue.c:6157
#: kernel/workqueue.c:6350 kernel/workqueue.c:6394 kernel/workqueue.c:6443
#: kernel/workqueue.c:6467 kernel/workqueue.c:6592 kernel/workqueue.c:6648
#: kernel/workqueue.c:6782 kernel/workqueue.c:6810 kernel/workqueue.c:6838
#: kernel/workqueue.c:6884 kernel/workqueue.c:6952 kernel/workqueue.c:7282
#: kernel/workqueue.c:7387 kernel/workqueue.c:7454 kernel/workqueue.c:7741
#: kernel/workqueue.c:7898 kernel/workqueue.c:8022
msgid "**Parameters**"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:358
msgid "``work``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:353
#: include/linux/workqueue.h:361
msgid "The work item in question"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:361
msgid "``delayed_work_pending (w)``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:359
msgid "Find out whether a delayable work item is currently pending"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:365
msgid "``w``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:476
msgid "allocate a workqueue"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:482
#: include/linux/workqueue.h:523 ../../../core-api/workqueue:789:
#: kernel/workqueue.c:6136
msgid "``const char *fmt``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:477
#: include/linux/workqueue.h:518 include/linux/workqueue.h:539
#: include/linux/workqueue.h:558
msgid "printf format for the name of the workqueue"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:479
#: include/linux/workqueue.h:520 ../../../core-api/workqueue:789:
#: kernel/workqueue.c:975 kernel/workqueue.c:997
msgid "``unsigned int flags``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:478
#: include/linux/workqueue.h:519
msgid "WQ_* flags"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:480
#: include/linux/workqueue.h:521 ../../../core-api/workqueue:789:
#: kernel/workqueue.c:5969
msgid "``int max_active``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:479
#: include/linux/workqueue.h:520
msgid "max in-flight work items, 0 for default"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:481
#: include/linux/workqueue.h:523 ../../../core-api/workqueue:789:
#: kernel/workqueue.c:6133
msgid "``...``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:480
#: include/linux/workqueue.h:522 include/linux/workqueue.h:542
#: include/linux/workqueue.h:560
msgid "args for **fmt**"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:481
msgid ""
"For a per-cpu workqueue, **max_active** limits the number of in-flight work "
"items for each CPU. e.g. **max_active** of 1 indicates that each CPU can be "
"executing at most one work item for the workqueue."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:485
msgid ""
"For unbound workqueues, **max_active** limits the number of in-flight work "
"items for the whole system. e.g. **max_active** of 16 indicates that there "
"can be at most 16 work items executing for the workqueue in the whole system."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:489
msgid ""
"As sharing the same active counter for an unbound workqueue across multiple "
"NUMA nodes can be expensive, **max_active** is distributed to each NUMA node "
"according to the proportion of the number of online CPUs and enforced "
"independently."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:494
msgid ""
"Depending on online CPU distribution, a node may end up with per-node "
"max_active which is significantly lower than **max_active**, which can lead "
"to deadlocks if the per-node concurrency limit is lower than the maximum "
"number of interdependent work items for the workqueue."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:499
msgid ""
"To guarantee forward progress regardless of online CPU distribution, the "
"concurrency limit on every node is guaranteed to be equal to or greater than "
"min_active which is set to min(**max_active**, ``WQ_DFL_MIN_ACTIVE``). This "
"means that the sum of per-node max_active's may be larger than "
"**max_active**."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:504
msgid ""
"For detailed information on ``WQ_``\\* flags, please refer to Documentation/"
"core-api/workqueue.rst."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:507
#: include/linux/workqueue.h:527 include/linux/workqueue.h:547
#: include/linux/workqueue.h:565 ../../../core-api/workqueue:789:
#: kernel/workqueue.c:882 kernel/workqueue.c:1110 kernel/workqueue.c:1523
#: kernel/workqueue.c:2379 kernel/workqueue.c:2447 kernel/workqueue.c:2544
#: kernel/workqueue.c:2585 kernel/workqueue.c:2619 kernel/workqueue.c:2773
#: kernel/workqueue.c:3123 kernel/workqueue.c:3369 kernel/workqueue.c:3464
#: kernel/workqueue.c:3868 kernel/workqueue.c:4286 kernel/workqueue.c:4305
#: kernel/workqueue.c:4324 kernel/workqueue.c:4439 kernel/workqueue.c:4461
#: kernel/workqueue.c:4580 kernel/workqueue.c:4618 kernel/workqueue.c:4656
#: kernel/workqueue.c:4787 kernel/workqueue.c:5037 kernel/workqueue.c:5419
#: kernel/workqueue.c:6031 kernel/workqueue.c:6047 kernel/workqueue.c:6072
#: kernel/workqueue.c:6102 kernel/workqueue.c:6786 kernel/workqueue.c:6841
#: kernel/workqueue.c:7284 kernel/workqueue.c:7394
msgid "**Return**"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:508
#: include/linux/workqueue.h:528 include/linux/workqueue.h:548
#: include/linux/workqueue.h:566
msgid "Pointer to the allocated workqueue on success, ``NULL`` on failure."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:517
msgid "allocate a workqueue with user-defined lockdep_map"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:522
msgid "``struct lockdep_map *lockdep_map``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:521
#: include/linux/workqueue.h:541
msgid "user-defined lockdep_map"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:523
msgid ""
"Same as alloc_workqueue but with the a user-define lockdep_map. Useful for "
"workqueues created with the same purpose and to avoid leaking a lockdep_map "
"on each workqueue creation."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:538
msgid ""
"``alloc_ordered_workqueue_lockdep_map (fmt, flags, lockdep_map, args...)``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:536
msgid "allocate an ordered workqueue with user-defined lockdep_map"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:542
#: include/linux/workqueue.h:563
msgid "``fmt``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:540
#: include/linux/workqueue.h:559
msgid "WQ_* flags (only WQ_FREEZABLE and WQ_MEM_RECLAIM are meaningful)"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:542
msgid "``lockdep_map``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:543
#: include/linux/workqueue.h:561
msgid "``args...``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:543
msgid ""
"Same as alloc_ordered_workqueue but with the a user-define lockdep_map. "
"Useful for workqueues created with the same purpose and to avoid leaking a "
"lockdep_map on each workqueue creation."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:559
msgid "``alloc_ordered_workqueue (fmt, flags, args...)``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:557
msgid "allocate an ordered workqueue"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:561
msgid ""
"Allocate an ordered workqueue.  An ordered workqueue executes at most one "
"work item at any given time in the queued order.  They are implemented as "
"unbound workqueues with **max_active** of one."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:644
msgid "queue work on a workqueue"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:650
#: include/linux/workqueue.h:679 include/linux/workqueue.h:694
#: include/linux/workqueue.h:740 ../../../core-api/workqueue:789:
#: kernel/workqueue.c:751 kernel/workqueue.c:1552 kernel/workqueue.c:1577
#: kernel/workqueue.c:1812 kernel/workqueue.c:2372 kernel/workqueue.c:2434
#: kernel/workqueue.c:2536 kernel/workqueue.c:2574 kernel/workqueue.c:2621
#: kernel/workqueue.c:3849 kernel/workqueue.c:3966 kernel/workqueue.c:4123
#: kernel/workqueue.c:5414 kernel/workqueue.c:5441 kernel/workqueue.c:5633
#: kernel/workqueue.c:5874 kernel/workqueue.c:5972 kernel/workqueue.c:6006
#: kernel/workqueue.c:6061 kernel/workqueue.c:6352 kernel/workqueue.c:7389
#: kernel/workqueue.c:7456
msgid "``struct workqueue_struct *wq``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:645
#: include/linux/workqueue.h:674 include/linux/workqueue.h:689
#: ../../../core-api/workqueue:789: kernel/workqueue.c:2371
#: kernel/workqueue.c:2433 kernel/workqueue.c:2535 kernel/workqueue.c:2573
#: kernel/workqueue.c:2616
msgid "workqueue to use"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:647
#: include/linux/workqueue.h:706 include/linux/workqueue.h:721
#: include/linux/workqueue.h:737 ../../../core-api/workqueue:789:
#: kernel/workqueue.c:877 kernel/workqueue.c:1086 kernel/workqueue.c:1136
#: kernel/workqueue.c:1174 kernel/workqueue.c:2029 kernel/workqueue.c:2146
#: kernel/workqueue.c:2171 kernel/workqueue.c:2373 kernel/workqueue.c:2435
#: kernel/workqueue.c:2945 kernel/workqueue.c:3152 kernel/workqueue.c:4287
#: kernel/workqueue.c:4417 kernel/workqueue.c:4478 kernel/workqueue.c:4496
#: kernel/workqueue.c:4515 kernel/workqueue.c:6102
msgid "``struct work_struct *work``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:646
#: include/linux/workqueue.h:690 ../../../core-api/workqueue:789:
#: kernel/workqueue.c:2372 kernel/workqueue.c:2434 kernel/workqueue.c:2536
#: kernel/workqueue.c:2574 kernel/workqueue.c:2617
msgid "work to queue"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:647
msgid ""
"Returns ``false`` if **work** was already on a queue, ``true`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:649
msgid ""
"We queue the work to the CPU on which it was submitted, but if the CPU dies "
"it can be processed by another CPU."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:652
msgid ""
"Memory-ordering properties:  If it returns ``true``, guarantees that all "
"stores preceding the call to queue_work() in the program order will be "
"visible from the CPU which will execute **work** by the time such work "
"executes, e.g.,"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:656
msgid "{ x is initially 0 }"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:658
msgid "CPU0                               CPU1"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:660
msgid ""
"WRITE_ONCE(x, 1);                  [ **work** is being executed ] r0 = "
"queue_work(wq, work);           r1 = READ_ONCE(x);"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:663
msgid "Forbids: r0 == true && r1 == 0"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:673
msgid "queue work on a workqueue after delay"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:676
#: include/linux/workqueue.h:691 include/linux/workqueue.h:802
#: include/linux/workqueue.h:820 ../../../core-api/workqueue:789:
#: kernel/workqueue.c:2537 kernel/workqueue.c:2575 kernel/workqueue.c:4305
#: kernel/workqueue.c:4441 kernel/workqueue.c:4463 kernel/workqueue.c:4542
#: kernel/workqueue.c:4555 kernel/workqueue.c:4568
msgid "``struct delayed_work *dwork``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:675
msgid "delayable work to queue"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:677
#: include/linux/workqueue.h:692 include/linux/workqueue.h:803
#: include/linux/workqueue.h:817 ../../../core-api/workqueue:789:
#: kernel/workqueue.c:2538 kernel/workqueue.c:2576
msgid "``unsigned long delay``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:676
#: include/linux/workqueue.h:691 ../../../core-api/workqueue:789:
#: kernel/workqueue.c:2537 kernel/workqueue.c:2575
msgid "number of jiffies to wait before queueing"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:677
msgid "Equivalent to queue_delayed_work_on() but tries to use the local CPU."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:688
msgid "modify delay of or queue a delayed work"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:692
msgid "mod_delayed_work_on() on local CPU."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:703
msgid "put work task on a specific cpu"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:709
#: include/linux/workqueue.h:805 ../../../core-api/workqueue:789:
#: kernel/workqueue.c:2375 kernel/workqueue.c:2539 kernel/workqueue.c:2577
#: kernel/workqueue.c:5223 kernel/workqueue.c:5438 kernel/workqueue.c:6064
#: kernel/workqueue.c:6647 kernel/workqueue.c:6784
msgid "``int cpu``"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:704
msgid "cpu to put the work task on"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:705
#: include/linux/workqueue.h:716 include/linux/workqueue.h:801
#: include/linux/workqueue.h:815
msgid "job to be done"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:706
msgid "This puts a job on a specific cpu"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:715
msgid "put work task in global workqueue"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:717
msgid ""
"Returns ``false`` if **work** was already on the kernel-global workqueue and "
"``true`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:720
msgid ""
"This puts a job in the kernel-global workqueue if it was not already queued "
"and leaves it in the same position on the kernel-global workqueue otherwise."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:724
msgid ""
"Shares the same memory-ordering properties of queue_work(), cf. the DocBook "
"header of queue_work()."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:734
msgid "Enable and queue a work item on a specific workqueue"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:735
msgid "The target workqueue"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:736
msgid "The work item to be enabled and queued"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:737
msgid ""
"This function combines the operations of enable_work() and queue_work(), "
"providing a convenient way to enable and queue a work item in a single call. "
"It invokes enable_work() on **work** and then queues it if the disable depth "
"reached 0. Returns ``true`` if the disable depth reached 0 and **work** is "
"queued, and ``false`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:743
msgid ""
"Note that **work** is always queued when disable depth reaches zero. If the "
"desired behavior is queueing only if certain events took place while "
"**work** is disabled, the user should implement the necessary state tracking "
"and perform explicit conditional queueing after enable_work()."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:799
msgid "queue work in global workqueue on CPU after delay"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:800
msgid "cpu to use"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:802
msgid "number of jiffies to wait"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:803
msgid ""
"After waiting for a given time this puts a job in the kernel-global "
"workqueue on the specified CPU."
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:814
msgid "put work task in global workqueue after delay"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:816
msgid "number of jiffies to wait or 0 for immediate execution"
msgstr ""

#: ../../../core-api/workqueue:787: include/linux/workqueue.h:817
msgid ""
"After waiting for a given time this puts a job in the kernel-global "
"workqueue."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:563
msgid "``for_each_pool (pool, pi)``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:561
msgid "iterate through all worker_pools in the system"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:567
#: kernel/workqueue.c:581
msgid "``pool``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:562
#: kernel/workqueue.c:579 kernel/workqueue.c:594
msgid "iteration cursor"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:564
msgid "``pi``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:563
msgid "integer used for iteration"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:564
msgid ""
"This must be called either with wq_pool_mutex held or RCU read locked.  If "
"the pool needs to be used beyond the locking in effect, the caller is "
"responsible for guaranteeing that the pool stays online."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:568
#: kernel/workqueue.c:583 kernel/workqueue.c:600
msgid ""
"The if/else clause exists only for the lockdep assertion and can be ignored."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:580
msgid "``for_each_pool_worker (worker, pool)``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:578
msgid "iterate through all workers of a worker_pool"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:584
msgid "``worker``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:580
msgid "worker_pool to iterate workers of"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:581
msgid "This must be called with wq_pool_attach_mutex."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:595
msgid "``for_each_pwq (pwq, wq)``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:593
msgid "iterate through all pool_workqueues of the specified workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:599
msgid "``pwq``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:596
msgid "``wq``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:595
#: kernel/workqueue.c:5409 kernel/workqueue.c:5436
msgid "the target workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:596
msgid ""
"This must be called either with wq->mutex held or RCU read locked. If the "
"pwq needs to be used beyond the locking in effect, the caller is responsible "
"for guaranteeing that the pwq stays online."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:706
msgid "allocate ID and assign it to **pool**"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:712
#: kernel/workqueue.c:1089 kernel/workqueue.c:1239 kernel/workqueue.c:2670
#: kernel/workqueue.c:2772 kernel/workqueue.c:3041 kernel/workqueue.c:4789
#: kernel/workqueue.c:4955 kernel/workqueue.c:6396 kernel/workqueue.c:6594
#: kernel/workqueue.c:6650
msgid "``struct worker_pool *pool``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:707
msgid "the pool pointer of interest"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:708
msgid ""
"Returns 0 if ID in [0, WORK_OFFQ_POOL_NONE) is allocated and assigned "
"successfully, -errno on failure."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:745
msgid "effective cpumask of an unbound workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:746
#: kernel/workqueue.c:1547
msgid "workqueue of interest"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:747
msgid ""
"**wq->unbound_attrs->cpumask** contains the cpumask requested by the user "
"which is masked with wq_unbound_cpumask to determine the effective cpumask. "
"The default pwq is always mapped to the pool with the current effective "
"cpumask."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:871
msgid "return the worker_pool a given work was associated with"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:872
msgid "the work item of interest"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:873
msgid ""
"Pools are created and destroyed under wq_pool_mutex, and allows read access "
"under RCU read lock.  As such, this function should be called under "
"wq_pool_mutex or inside of a rcu_read_lock() region."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:877
msgid ""
"All fields of the returned pool are accessible as long as the above "
"mentioned locking is in effect.  If the returned pool needs to be used "
"beyond the critical section, the caller is responsible for ensuring the "
"returned pool is and stays online."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:883
msgid "The worker_pool **work** was last associated with.  ``NULL`` if none."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:972
msgid "set worker flags and adjust nr_running accordingly"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:978
#: kernel/workqueue.c:1000 kernel/workqueue.c:1035 kernel/workqueue.c:1069
#: kernel/workqueue.c:1171 kernel/workqueue.c:2673 kernel/workqueue.c:2728
#: kernel/workqueue.c:2872 kernel/workqueue.c:3115 kernel/workqueue.c:3155
#: kernel/workqueue.c:3330 kernel/workqueue.c:3771
msgid "``struct worker *worker``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:973
#: kernel/workqueue.c:995 kernel/workqueue.c:3110 kernel/workqueue.c:3150
#: kernel/workqueue.c:3325 kernel/workqueue.c:3362 kernel/workqueue.c:3448
msgid "self"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:974
msgid "flags to set"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:975
msgid "Set **flags** in **worker->flags** and adjust nr_running accordingly."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:994
msgid "clear worker flags and adjust nr_running accordingly"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:996
msgid "flags to clear"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:997
msgid "Clear **flags** in **worker->flags** and adjust nr_running accordingly."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1029
msgid "enter idle state"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1030
msgid "worker which is entering idle state"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1031
msgid ""
"**worker** is entering idle state.  Update stats and idle timer if necessary."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1034
#: kernel/workqueue.c:1067
msgid "LOCKING: raw_spin_lock_irq(pool->lock)."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1063
msgid "leave idle state"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1064
msgid "worker which is leaving idle state"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1065
msgid "**worker** is leaving idle state.  Update stats."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1083
msgid "find worker which is executing a work"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1084
#: kernel/workqueue.c:6589
msgid "pool of interest"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1085
msgid "work to find worker for"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1086
msgid ""
"Find a worker which is executing **work** on **pool** by searching **pool-"
">busy_hash** which is keyed by the address of **work**.  For a worker to "
"match, its current execution should match the address of **work** and its "
"work function.  This is to avoid unwanted dependency between unrelated work "
"executions through a work item being recycled while still being executed."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1093
msgid ""
"This is a bit tricky.  A work item may be freed once its execution starts "
"and nothing prevents the freed area from being recycled for another work "
"item.  If the same work item address ends up being reused before the "
"original execution finishes, workqueue will identify the recycled work item "
"as currently executing and make it wait until the current execution "
"finishes, introducing an unwanted dependency."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1100
msgid ""
"This function checks the work item address and work function to avoid false "
"positives.  Note that this isn't complete as one may construct a work "
"function which can introduce dependency onto itself through a recycled work "
"item.  Well, if somebody wants to shoot oneself in the foot that badly, "
"there's only so much we can do, and if such deadlock actually occurs, it "
"should be easy to locate the culprit work function."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1107
#: kernel/workqueue.c:1139 kernel/workqueue.c:1531 kernel/workqueue.c:1988
#: kernel/workqueue.c:2176 kernel/workqueue.c:2770 kernel/workqueue.c:2872
#: kernel/workqueue.c:3119 kernel/workqueue.c:3158 kernel/workqueue.c:3330
#: kernel/workqueue.c:3785 kernel/workqueue.c:3865 kernel/workqueue.c:5972
#: kernel/workqueue.c:6811 kernel/workqueue.c:6838 kernel/workqueue.c:6884
msgid "**Context**"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1108
#: kernel/workqueue.c:1140 kernel/workqueue.c:1989 kernel/workqueue.c:2177
#: kernel/workqueue.c:2873 kernel/workqueue.c:3786
msgid "raw_spin_lock_irq(pool->lock)."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1111
msgid ""
"Pointer to worker which is executing **work** if found, ``NULL`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1130
msgid "move linked works to a list"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1131
msgid "start of series of works to be scheduled"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1133
#: kernel/workqueue.c:2172
msgid "``struct list_head *head``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1132
msgid "target list to append **work** to"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1134
#: kernel/workqueue.c:1172
msgid "``struct work_struct **nextp``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1133
#: kernel/workqueue.c:1171
msgid "out parameter for nested worklist walking"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1134
msgid ""
"Schedule linked works starting from **work** to **head**. Work series to be "
"scheduled starts at **work** and includes any consecutive work with "
"WORK_STRUCT_LINKED set in its predecessor. See assign_work() for details on "
"**nextp**."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1168
msgid "assign a work item and its linked work items to a worker"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1169
msgid "work to assign"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1170
msgid "worker to assign to"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1172
msgid ""
"Assign **work** and its linked work items to **worker**. If **work** is "
"already being executed by another worker in the same pool, it'll be punted "
"there."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1175
msgid ""
"If **nextp** is not NULL, it's updated to point to the next work of the last "
"scheduled work. This allows assign_work() to be nested inside "
"list_for_each_entry_safe()."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1179
msgid ""
"Returns ``true`` if **work** was successfully assigned to **worker**. "
"``false`` if **work** was punted to another worker already executing it."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1233
msgid "wake up an idle worker if necessary"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1234
msgid "pool to kick"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1235
msgid ""
"**pool** may have pending work items. Wake up worker if necessary. Returns "
"whether a worker was woken up."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1386
msgid "a worker is running again"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1392
#: kernel/workqueue.c:1425 kernel/workqueue.c:1471 kernel/workqueue.c:1521
#: kernel/workqueue.c:6156
msgid "``struct task_struct *task``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1387
msgid "task waking up"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1388
msgid "This function is called when a worker returns from schedule()"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1419
msgid "a worker is going to sleep"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1420
msgid "task going to sleep"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1421
msgid ""
"This function is called from schedule() when a busy worker is going to sleep."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1465
msgid "a scheduler tick occurred while a kworker is running"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1466
msgid "task currently running"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1467
msgid ""
"Called from sched_tick(). We're in the IRQ context and the current worker's "
"fields which follow the 'K' locking rule can be accessed safely."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1515
msgid "retrieve worker's last work function"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1516
msgid "Task to retrieve last work function of."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1517
msgid ""
"Determine the last function a worker executed. This is called from the "
"scheduler to get a worker's last known identity."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1521
msgid ""
"This function is called during schedule() when a kworker is going to sleep. "
"It's used by psi to identify aggregation workers during dequeuing, to allow "
"periodic aggregation to shut-off when that worker is the last task in the "
"system or cgroup to go to sleep."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1526
msgid ""
"As this function doesn't involve any workqueue-related locking, it only "
"returns stable values when called from inside the scheduler's queuing and "
"dequeuing paths, when **task**, which must be a kworker, is guaranteed to "
"not be processing any works."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1521
msgid "raw_spin_lock_irq(rq->lock)"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1534
msgid ""
"The last work function ``current`` executed as a worker, NULL if it hasn't "
"executed any work yet."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1546
msgid "Determine wq_node_nr_active to use"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1549
#: kernel/workqueue.c:2408 kernel/workqueue.c:2437
msgid "``int node``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1548
msgid "NUMA node, can be ``NUMA_NO_NODE``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1549
msgid "Determine wq_node_nr_active to use for **wq** on **node**. Returns:"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1551
msgid ""
"``NULL`` for per-cpu workqueues as they don't need to use shared nr_active."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1553
msgid "node_nr_active[nr_node_ids] if **node** is ``NUMA_NO_NODE``."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1555
msgid "Otherwise, node_nr_active[**node**]."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1571
msgid "Update per-node max_actives to use"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1572
msgid "workqueue to update"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1574
msgid "``int off_cpu``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1573
msgid "CPU that's going down, -1 if a CPU is not going down"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1574
msgid ""
"Update **wq->node_nr_active**[]->max. **wq** must be unbound. max_active is "
"distributed among nodes according to the proportions of numbers of online "
"cpus. The result is always between **wq->min_active** and max_active."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1623
msgid "get an extra reference on the specified pool_workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1629
#: kernel/workqueue.c:1643 kernel/workqueue.c:1662 kernel/workqueue.c:1712
#: kernel/workqueue.c:1787 kernel/workqueue.c:1933 kernel/workqueue.c:1983
#: kernel/workqueue.c:2174 kernel/workqueue.c:3772
msgid "``struct pool_workqueue *pwq``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1624
msgid "pool_workqueue to get"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1625
msgid ""
"Obtain an extra reference on **pwq**.  The caller should guarantee that "
"**pwq** has positive refcnt and be holding the matching pool->lock."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1637
msgid "put a pool_workqueue reference"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1638
msgid "pool_workqueue to put"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1639
msgid ""
"Drop a reference of **pwq**.  If its refcnt reaches zero, schedule its "
"destruction.  The caller should be holding the matching pool->lock."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1656
msgid "put_pwq() with surrounding pool lock/unlock"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1657
msgid "pool_workqueue to put (can be ``NULL``)"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1658
msgid "put_pwq() with locking.  This function also allows ``NULL`` **pwq**."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1706
msgid "Try to increment nr_active for a pwq"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1707
#: kernel/workqueue.c:1782 kernel/workqueue.c:1928
msgid "pool_workqueue of interest"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1709
#: kernel/workqueue.c:1784
msgid "``bool fill``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1708
#: kernel/workqueue.c:1783
msgid "max_active may have increased, try to increase concurrency level"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1709
msgid ""
"Try to increment nr_active for **pwq**. Returns ``true`` if an nr_active "
"count is successfully obtained. ``false`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1781
msgid "Activate the first inactive work item on a pwq"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1784
msgid ""
"Activate the first inactive work item of **pwq** if available and allowed by "
"max_active limit."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1787
msgid ""
"Returns ``true`` if an inactive work item has been activated. ``false`` if "
"no inactive work item is found or max_active limit is reached."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1806
msgid "unplug the oldest pool_workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1807
msgid "workqueue_struct where its oldest pwq is to be unplugged"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1808
msgid ""
"This function should only be called for ordered workqueues where only the "
"oldest pwq is unplugged, the others are plugged to suspend execution to "
"ensure proper work item ordering::"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1821
msgid ""
"When the oldest pwq is drained and removed, this function should be called "
"to unplug the next oldest one to start its work item execution. Note that "
"pwq's are linked into wq->pwqs with the oldest first, so the first one in "
"the list is the oldest."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1846
msgid "Activate a pending pwq on a wq_node_nr_active"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1852
msgid "``struct wq_node_nr_active *nna``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1847
msgid "wq_node_nr_active to activate a pending pwq for"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1849
msgid "``struct worker_pool *caller_pool``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1848
msgid "worker_pool the caller is locking"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1849
msgid ""
"Activate a pwq in **nna->pending_pwqs**. Called with **caller_pool** locked. "
"**caller_pool** may be unlocked and relocked to lock other worker_pools."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1927
msgid "Retire an active count"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1929
msgid ""
"Decrement **pwq**'s nr_active and try to activate the first inactive work "
"item. For unbound workqueues, this function may temporarily drop **pwq->pool-"
">lock**."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1977
msgid "decrement pwq's nr_in_flight"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1978
msgid "pwq of interest"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1980
msgid "``unsigned long work_data``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1979
msgid "work_data of work which left the queue"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1980
msgid ""
"A work either has completed or is removed from pending queue, decrement "
"nr_in_flight of its pwq and handle workqueue flushing."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1983
msgid "**NOTE**"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1984
msgid ""
"For unbound workqueues, this function may temporarily drop **pwq->pool-"
">lock** and thus should be called after all other state updates for the in-"
"flight work item is complete."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2023
#: kernel/workqueue.c:2140
msgid "steal work item from worklist and disable irq"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2024
#: kernel/workqueue.c:2141
msgid "work item to steal"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2026
#: kernel/workqueue.c:2143
msgid "``u32 cflags``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2025
#: kernel/workqueue.c:2142
msgid "``WORK_CANCEL_`` flags"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2027
#: kernel/workqueue.c:2144
msgid "``unsigned long *irq_flags``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2026
msgid "place to store irq state"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2027
msgid ""
"Try to grab PENDING bit of **work**.  This function can handle **work** in "
"any stable state - idle, on timer or on worklist."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2032
msgid "1"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2032
msgid "if **work** was pending and we successfully stole PENDING"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2033
#: kernel/workqueue.c:3370 kernel/workqueue.c:3465
msgid "0"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2033
msgid "if **work** was idle and we claimed PENDING"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2034
msgid "-EAGAIN"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2034
msgid "if PENDING couldn't be grabbed at the moment, safe to busy-retry"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2037
#: kernel/workqueue.c:4443
msgid "**Note**"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2039
msgid ""
"On >= 0 return, the caller owns **work**'s PENDING bit.  To avoid getting "
"interrupted while holding PENDING and **work** off queue, irq must be "
"disabled on entry.  This, combined with delayed_work->timer being irqsafe, "
"ensures that we return -EAGAIN for finite short period of time."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2044
msgid ""
"On successful return, >= 0, irq is disabled and the caller is responsible "
"for releasing it using local_irq_restore(***irq_flags**)."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2047
#: kernel/workqueue.c:4447
msgid "This function is safe to call from any context including IRQ handler."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2143
msgid "place to store IRQ state"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2144
msgid ""
"Grab PENDING bit of **work**. **work** can be in any stable state - idle, on "
"timer or on worklist."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2147
msgid ""
"Can be called from any context. IRQ is disabled on return with IRQ state "
"stored in ***irq_flags**. The caller is responsible for re-enabling it using "
"local_irq_restore()."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2151
msgid "Returns ``true`` if **work** was pending. ``false`` if idle."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2168
msgid "insert a work into a pool"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2169
msgid "pwq **work** belongs to"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2170
msgid "work to insert"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2171
msgid "insertion point"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2173
msgid "``unsigned int extra_flags``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2172
msgid "extra WORK_STRUCT_* flags to set"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2173
msgid ""
"Insert **work** which belongs to **pwq** after **head**.  **extra_flags** is "
"or'd to work_struct flags."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2369
msgid "queue work on specific cpu"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2370
#: kernel/workqueue.c:2534 kernel/workqueue.c:2572
msgid "CPU number to execute work on"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2373
msgid ""
"We queue the work to a specific CPU, the caller must ensure it can't go "
"away.  Callers that fail to ensure that the specified CPU cannot go away "
"will execute on a randomly chosen CPU. But note well that callers specifying "
"a CPU that never has been online will get a splat."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2380
#: kernel/workqueue.c:2448
msgid "``false`` if **work** was already on a queue, ``true`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2402
msgid "Select a CPU based on NUMA node"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2403
msgid "NUMA node ID that we want to select a CPU from"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2404
msgid ""
"This function will attempt to find a \"random\" cpu available on a given "
"node. If there are no CPUs available on the given node it will return "
"WORK_CPU_UNBOUND indicating that we should just schedule to any available "
"CPU if we need to schedule this work."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2431
msgid "queue work on a \"random\" cpu for a given NUMA node"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2432
msgid "NUMA node that we are targeting the work for"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2435
msgid ""
"We queue the work to a \"random\" CPU within a given NUMA node. The basic "
"idea here is to provide a way to somehow associate work with a given NUMA "
"node."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2439
msgid ""
"This function will only make a best effort attempt at getting this onto the "
"right NUMA node. If no node is requested or the requested node is offline "
"then we just fall back to standard queue_work behavior."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2443
msgid ""
"Currently the \"random\" CPU ends up being the first available CPU in the "
"intersection of cpu_online_mask and the cpumask of the node, unless we are "
"running on the node. In that case we just use the current CPU."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2533
msgid "queue work on specific CPU after delay"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2538
msgid ""
"We queue the delayed_work to a specific CPU, for non-zero delays the caller "
"must ensure it is online and can't go away. Callers that fail to ensure "
"this, may get **dwork->timer** queued to an offlined CPU and this will "
"prevent queueing of **dwork->work** unless the offlined CPU becomes online "
"again."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2545
msgid ""
"``false`` if **work** was already on a queue, ``true`` otherwise.  If "
"**delay** is zero and **dwork** is idle, it will be scheduled for immediate "
"execution."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2571
msgid "modify delay of or queue a delayed work on specific CPU"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2576
msgid ""
"If **dwork** is idle, equivalent to queue_delayed_work_on(); otherwise, "
"modify **dwork**'s timer so that it expires after **delay**.  If **delay** "
"is zero, **work** is guaranteed to be scheduled immediately regardless of "
"its current state."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2582
msgid ""
"This function is safe to call from any context including IRQ handler. See "
"try_to_grab_pending() for details."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2582
msgid ""
"``false`` if **dwork** was idle and queued, ``true`` if **dwork** was "
"pending and its timer was modified."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2615
msgid "queue work after a RCU grace period"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2618
#: kernel/workqueue.c:4327
msgid "``struct rcu_work *rwork``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2619
msgid ""
"``false`` if **rwork** was already pending, ``true`` otherwise.  Note that a "
"full RCU grace period is guaranteed only after a ``true`` return. While "
"**rwork** is guaranteed to be executed after a ``false`` return, the "
"execution may happen before a full RCU grace period has passed."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2667
msgid "attach a worker to a pool"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2668
msgid "worker to be attached"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2669
msgid "the target pool"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2670
msgid ""
"Attach **worker** to **pool**.  Once attached, the ``WORKER_UNBOUND`` flag "
"and cpu-binding of **worker** are kept coordinated with the pool across cpu-"
"[un]hotplugs."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2722
msgid "detach a worker from its pool"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2723
msgid "worker which is attached to its pool"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2724
msgid ""
"Undo the attaching which had been done in worker_attach_to_pool().  The "
"caller worker shouldn't access to the pool after detached except it has "
"other reference to the pool."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2766
msgid "create a new workqueue worker"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2767
msgid "pool the new worker will belong to"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2768
msgid "Create and start a new worker which is attached to **pool**."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2771
msgid "Might sleep.  Does GFP_KERNEL allocations."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2774
msgid "Pointer to the newly created worker."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2866
msgid "Tag a worker for destruction"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2867
msgid "worker to be destroyed"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2869
msgid "``struct list_head *list``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2868
msgid "transfer worker away from its pool->idle_list and into list"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2869
msgid ""
"Tag **worker** for destruction and adjust **pool** stats accordingly.  The "
"worker should be idle."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2901
msgid "check if some idle workers can now be deleted."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2907
msgid "``struct timer_list *t``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2902
msgid "The pool's idle_timer that just expired"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2903
msgid ""
"The timer is armed in worker_enter_idle(). Note that it isn't disarmed in "
"worker_leave_idle(), as a worker flicking between idle and active while its "
"pool is at the too_many_workers() tipping point would cause too much timer "
"housekeeping overhead. Since IDLE_WORKER_TIMEOUT is long enough, we just let "
"it expire and re-evaluate things from there."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2939
msgid "cull workers that have been idle for too long."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2940
msgid "the pool's work for handling these idle workers"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2941
msgid ""
"This goes through a pool's idle workers and gets rid of those that have been "
"idle for at least IDLE_WORKER_TIMEOUT seconds."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:2944
msgid ""
"We don't want to disturb isolated CPUs because of a pcpu kworker being "
"culled, so this also resets worker affinity. This requires a sleepable "
"context, hence the split between timer callback and work item."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3035
msgid "create a new worker if necessary"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3036
msgid "pool to create a new worker for"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3037
msgid ""
"Create a new worker for **pool** if necessary.  **pool** is guaranteed to "
"have at least one idle worker on return from this function.  If creating a "
"new worker takes longer than MAYDAY_INTERVAL, mayday is sent to all rescuers "
"with works scheduled on **pool** to resolve possible allocation deadlock."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3043
msgid ""
"On return, need_to_create_worker() is guaranteed to be ``false`` and "
"may_start_working() ``true``."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3046
msgid ""
"LOCKING: raw_spin_lock_irq(pool->lock) which may be released and regrabbed "
"multiple times.  Does GFP_KERNEL allocations.  Called only from manager."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3109
msgid "manage worker pool"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3111
msgid ""
"Assume the manager role and manage the worker pool **worker** belongs to.  "
"At any given time, there can be only zero or one manager per pool.  The "
"exclusion is handled automatically by this function."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3115
msgid ""
"The caller can safely start processing works on false return.  On true "
"return, it's guaranteed that need_to_create_worker() is false and "
"may_start_working() is true."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3120
msgid ""
"raw_spin_lock_irq(pool->lock) which may be released and regrabbed multiple "
"times.  Does GFP_KERNEL allocations."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3124
msgid ""
"``false`` if the pool doesn't need management and the caller can safely "
"start processing works, ``true`` if management function was performed and "
"the conditions that the caller verified before calling the function may no "
"longer be true."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3149
msgid "process single work"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3151
msgid "work to process"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3152
msgid ""
"Process **work**.  This function contains all the logics necessary to "
"process a single work including synchronization against and interaction with "
"other workers on the same cpu, queueing and flushing.  As long as context "
"requirement is met, any worker can call this function to process a work."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3159
msgid "raw_spin_lock_irq(pool->lock) which is released and regrabbed."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3324
msgid "process scheduled works"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3326
msgid ""
"Process all scheduled works.  Please note that the scheduled list may change "
"while processing a work, so this function repeatedly fetches a work from the "
"top and executes it."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3331
msgid ""
"raw_spin_lock_irq(pool->lock) which may be released and regrabbed multiple "
"times."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3361
msgid "the worker thread function"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3367
msgid "``void *__worker``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3363
msgid ""
"The worker thread function.  All workers belong to a worker_pool - either a "
"per-cpu one or dynamic unbound one.  These workers process all work items "
"regardless of their specific target workqueue.  The only exception is work "
"items which belong to workqueues with a rescuer which will be explained in "
"rescuer_thread()."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3447
msgid "the rescuer thread function"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3453
msgid "``void *__rescuer``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3449
msgid ""
"Workqueue rescuer thread function.  There's one rescuer for each workqueue "
"which has WQ_MEM_RECLAIM set."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3452
msgid ""
"Regular work processing on a pool may block trying to create a new worker "
"which uses GFP_KERNEL allocation which has slight chance of developing into "
"deadlock if some works currently on the same queue need to be processed to "
"satisfy the GFP_KERNEL allocation.  This is the problem rescuer solves."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3458
msgid ""
"When such condition is possible, the pool summons rescuers of all workqueues "
"which have works queued on the pool and let them process those works so that "
"forward progress can be guaranteed."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3462
msgid "This should happen rarely."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3718
msgid "check for flush dependency sanity"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3724
msgid "``struct workqueue_struct *target_wq``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3719
#: kernel/workqueue.c:3844
msgid "workqueue being flushed"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3721
msgid "``struct work_struct *target_work``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3720
msgid "work item being flushed (NULL for workqueue flushes)"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3722
msgid "``bool from_cancel``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3721
msgid "are we called from the work cancel path"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3722
msgid ""
"``current`` is trying to flush the whole **target_wq** or **target_work** on "
"it. If this is not the cancel path (which implies work being flushed is "
"either already running, or will not be at all), check if **target_wq** "
"doesn't have ``WQ_MEM_RECLAIM`` and verify that ``current`` is not "
"reclaiming memory or running on a workqueue which doesn't have "
"``WQ_MEM_RECLAIM`` as that can break forward- progress guarantee leading to "
"a deadlock."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3766
msgid "insert a barrier work"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3767
msgid "pwq to insert barrier into"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3769
msgid "``struct wq_barrier *barr``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3768
msgid "wq_barrier to insert"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3770
msgid "``struct work_struct *target``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3769
msgid "target work to attach **barr** to"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3770
msgid ""
"worker currently executing **target**, NULL if **target** is not executing"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3771
msgid ""
"**barr** is linked to **target** such that **barr** is completed only after "
"**target** finishes execution.  Please note that the ordering guarantee is "
"observed only with respect to **target** and on the local cpu."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3776
msgid ""
"Currently, a queued barrier can't be canceled.  This is because "
"try_to_grab_pending() can't determine whether the work to be grabbed is at "
"the head of the queue and thus can't clear LINKED flag of the previous work "
"while there must be a valid next work after a work with LINKED flag set."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3782
msgid ""
"Note that when **worker** is non-NULL, **target** may be modified underneath "
"us, so we can't reliably determine pwq from **target**."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3843
msgid "prepare pwqs for workqueue flushing"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3846
msgid "``int flush_color``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3845
msgid "new flush color, < 0 for no-op"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3847
msgid "``int work_color``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3846
msgid "new work color, < 0 for no-op"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3847
msgid "Prepare pwqs for workqueue flushing."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3849
msgid ""
"If **flush_color** is non-negative, flush_color on all pwqs should be -1.  "
"If no pwq has in-flight commands at the specified color, all pwq-"
">flush_color's stay at -1 and ``false`` is returned.  If any pwq has in "
"flight commands, its pwq->flush_color is set to **flush_color**, **wq-"
">nr_pwqs_to_flush** is updated accordingly, pwq wakeup logic is armed and "
"``true`` is returned."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3856
msgid ""
"The caller should have initialized **wq->first_flusher** prior to calling "
"this function with non-negative **flush_color**.  If **flush_color** is "
"negative, no flush color update is done and ``false`` is returned."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3861
msgid ""
"If **work_color** is non-negative, all pwqs should have the same work_color "
"which is previous to **work_color** and all will be advanced to "
"**work_color**."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3866
msgid "mutex_lock(wq->mutex)."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3869
msgid ""
"``true`` if **flush_color** >= 0 and there's something to flush.  ``false`` "
"otherwise."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3960
msgid "ensure that any scheduled work has run to completion."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3961
msgid "workqueue to flush"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:3962
msgid ""
"This function sleeps until all work items which were queued on entry have "
"finished execution, but it is not livelocked by new incoming ones."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4117
msgid "drain a workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4118
msgid "workqueue to drain"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4119
msgid ""
"Wait until the workqueue becomes empty.  While draining is in progress, only "
"chain queueing is allowed.  IOW, only currently pending or running work "
"items on **wq** can queue further work items on it.  **wq** is flushed "
"repeatedly until it becomes empty.  The number of flushing is determined by "
"the depth of chaining and should be relatively short.  Whine if it takes too "
"long."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4281
msgid "wait for a work to finish executing the last queueing instance"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4282
msgid "the work to flush"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4283
msgid ""
"Wait until **work** has finished execution.  **work** is guaranteed to be "
"idle on return if it hasn't been requeued since flush started."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4287
#: kernel/workqueue.c:4306
msgid ""
"``true`` if flush_work() waited for the work to finish execution, ``false`` "
"if it was already idle."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4299
msgid "wait for a dwork to finish executing the last queueing"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4300
msgid "the delayed work to flush"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4301
msgid ""
"Delayed timer is cancelled and the pending work is queued for immediate "
"execution.  Like flush_work(), this function only considers the last "
"queueing instance of **dwork**."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4321
msgid "wait for a rwork to finish executing the last queueing"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4322
msgid "the rcu work to flush"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4324
msgid ""
"``true`` if flush_rcu_work() waited for the work to finish execution, "
"``false`` if it was already idle."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4411
msgid "cancel a work and wait for it to finish"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4412
msgid "the work to cancel"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4413
msgid ""
"Cancel **work** and wait for its execution to finish. This function can be "
"used even if the work re-queues itself or migrates to another workqueue. On "
"return from this function, **work** is guaranteed to be not pending or "
"executing on any CPU as long as there aren't racing enqueues."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4418
msgid ""
"cancel_work_sync(:c:type:`delayed_work->work <delayed_work>`) must not be "
"used for delayed_work's. Use cancel_delayed_work_sync() instead."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4421
#: kernel/workqueue.c:4495
msgid ""
"Must be called from a sleepable context if **work** was last queued on a non-"
"BH workqueue. Can also be called from non-hardirq atomic contexts including "
"BH if **work** was last queued on a BH workqueue."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4425
#: kernel/workqueue.c:4499
msgid "Returns ``true`` if **work** was pending, ``false`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4435
msgid "cancel a delayed work"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4436
msgid "delayed_work to cancel"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4437
msgid "Kill off a pending delayed_work."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4440
msgid ""
"``true`` if **dwork** was pending and canceled; ``false`` if it wasn't "
"pending."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4443
msgid ""
"The work callback function may still be running on return, unless it returns "
"``true`` and the work doesn't re-arm itself.  Explicitly flush or use "
"cancel_delayed_work_sync() to wait on it."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4457
msgid "cancel a delayed work and wait for it to finish"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4458
msgid "the delayed work cancel"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4459
msgid "This is cancel_work_sync() for delayed works."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4462
msgid "``true`` if **dwork** was pending, ``false`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4472
msgid "Disable and cancel a work item"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4473
#: kernel/workqueue.c:4491
msgid "work item to disable"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4474
msgid ""
"Disable **work** by incrementing its disable count and cancel it if "
"currently pending. As long as the disable count is non-zero, any attempt to "
"queue **work** will fail and return ``false``. The maximum supported disable "
"depth is 2 to the power of ``WORK_OFFQ_DISABLE_BITS``, currently 65536."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4479
msgid ""
"Can be called from any context. Returns ``true`` if **work** was pending, "
"``false`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4490
msgid "Disable, cancel and drain a work item"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4492
msgid ""
"Similar to disable_work() but also wait for **work** to finish if currently "
"executing."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4509
msgid "Enable a work item"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4510
msgid "work item to enable"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4511
msgid ""
"Undo disable_work[_sync]() by decrementing **work**'s disable count. "
"**work** can only be queued if its disable count is 0."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4514
msgid ""
"Can be called from any context. Returns ``true`` if the disable count "
"reached 0. Otherwise, ``false``."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4536
msgid "Disable and cancel a delayed work item"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4537
#: kernel/workqueue.c:4550
msgid "delayed work item to disable"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4538
msgid "disable_work() for delayed work items."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4549
msgid "Disable, cancel and drain a delayed work item"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4551
msgid "disable_work_sync() for delayed work items."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4562
msgid "Enable a delayed work item"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4563
msgid "delayed work item to enable"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4564
msgid "enable_work() for delayed work items."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4574
msgid "execute a function synchronously on each online CPU"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4580
msgid "``work_func_t func``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4575
msgid "the function to call"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4576
msgid ""
"schedule_on_each_cpu() executes **func** on each online CPU using the system "
"workqueue and blocks until all CPUs have completed. schedule_on_each_cpu() "
"is very slow."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4581
#: kernel/workqueue.c:7395
msgid "0 on success, -errno on failure."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4611
msgid "reliably execute the routine with user context"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4617
msgid "``work_func_t fn``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4612
msgid "the function to execute"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4614
msgid "``struct execute_work *ew``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4613
msgid ""
"guaranteed storage for the execute work structure (must be available when "
"the work executes)"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4615
msgid ""
"Executes the function immediately if process context is available, otherwise "
"schedules the function for delayed execution."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4619
msgid "0 - function was executed 1 - function was scheduled for execution"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4637
msgid "free a workqueue_attrs"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4643
#: kernel/workqueue.c:5226
msgid "``struct workqueue_attrs *attrs``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4638
msgid "workqueue_attrs to free"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4639
msgid "Undo alloc_workqueue_attrs()."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4652
msgid "allocate a workqueue_attrs"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4658
#: kernel/workqueue.c:6033 kernel/workqueue.c:6049 kernel/workqueue.c:6445
#: kernel/workqueue.c:6469 kernel/workqueue.c:6812 kernel/workqueue.c:6840
#: kernel/workqueue.c:6886 kernel/workqueue.c:7743 kernel/workqueue.c:7900
#: kernel/workqueue.c:8024
msgid "``void``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:1
msgid "no arguments"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4653
msgid ""
"Allocate a new workqueue_attrs, initialize with default settings and return "
"it."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4657
msgid "The allocated new workqueue_attr on success. ``NULL`` on failure."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4783
msgid "initialize a newly zalloc'd worker_pool"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4784
msgid "worker_pool to initialize"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4785
msgid ""
"Initialize a newly zalloc'd **pool**.  It also allocates **pool->attrs**."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4788
msgid ""
"0 on success, -errno on failure.  Even on failure, all fields inside "
"**pool** proper are initialized and put_unbound_pool() can be called on "
"**pool** safely to release it."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4949
msgid "put a worker_pool"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4950
msgid "worker_pool to put"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4951
msgid ""
"Put **pool**.  If its refcnt reaches zero, it gets destroyed in RCU safe "
"manner.  get_unbound_pool() calls this function on its failure path and this "
"function should be able to release pools which went through, successfully or "
"not, init_worker_pool()."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:4956
#: kernel/workqueue.c:5035
msgid "Should be called with wq_pool_mutex held."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5028
msgid "get a worker_pool with the specified attributes"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5034
#: kernel/workqueue.c:5411
msgid "``const struct workqueue_attrs *attrs``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5029
msgid "the attributes of the worker_pool to get"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5030
msgid ""
"Obtain a worker_pool which has the same attributes as **attrs**, bump the "
"reference count and return it.  If there already is a matching worker_pool, "
"it will be used; otherwise, this function attempts to create a new one."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5038
msgid ""
"On success, a worker_pool with the same attributes as **attrs**. On failure, "
"``NULL``."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5220
msgid "calculate a wq_attrs' cpumask for a pod"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5221
msgid "the wq_attrs of the default pwq of the target workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5222
msgid "the target CPU"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5223
msgid ""
"Calculate the cpumask a workqueue with **attrs** should use on **pod**. The "
"result is stored in **attrs->__pod_cpumask**."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5226
msgid ""
"If pod affinity is not enabled, **attrs->cpumask** is always used. If "
"enabled and **pod** has online CPUs requested by **attrs**, the returned "
"cpumask is the intersection of the possible CPUs of **pod** and **attrs-"
">cpumask**."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5230
msgid ""
"The caller is responsible for ensuring that the cpumask of **pod** stays "
"stable."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5408
msgid "apply new workqueue_attrs to an unbound workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5410
msgid "the workqueue_attrs to apply, allocated with alloc_workqueue_attrs()"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5411
msgid ""
"Apply **attrs** to an unbound workqueue **wq**. Unless disabled, this "
"function maps a separate pwq to each CPU pod with possibles CPUs in **attrs-"
">cpumask** so that work items are affine to the pod it was issued on. Older "
"pwqs are released as in-flight work items finish. Note that a work item "
"which repeatedly requeues itself back-to-back will stay on its current pwq."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5417
msgid "Performs GFP_KERNEL allocations."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5420
msgid "0 on success and -errno on failure."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5435
msgid "update a pwq slot for CPU hot[un]plug"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5437
msgid "the CPU to update the pwq slot for"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5438
msgid ""
"This function is to be called from ``CPU_DOWN_PREPARE``, ``CPU_ONLINE`` and "
"``CPU_DOWN_FAILED``.  **cpu** is in the same pod of the CPU being "
"hot[un]plugged."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5442
msgid ""
"If pod affinity can't be adjusted due to memory allocation failure, it falls "
"back to **wq->dfl_pwq** which may not be optimal but is always correct."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5445
msgid ""
"Note that when the last allowed CPU of a pod goes offline for a workqueue "
"with a cpumask spanning multiple pods, the workers which were already "
"executing the work items for the workqueue will lose their CPU affinity and "
"may execute on any CPU. This is similar to how per-cpu workqueues behave on "
"CPU_DOWN. If a workqueue user wants strict affinity, it's the user's "
"responsibility to flush the work item from CPU_DOWN_PREPARE."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5627
msgid "update a wq's max_active to the current setting"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5628
#: kernel/workqueue.c:5869 kernel/workqueue.c:5967 kernel/workqueue.c:6060
msgid "target workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5629
msgid ""
"If **wq** isn't freezing, set **wq->max_active** to the saved_max_active and "
"activate inactive work items accordingly. If **wq** is freezing, clear **wq-"
">max_active** to zero."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5868
msgid "safely terminate a workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5870
msgid ""
"Safely destroy a workqueue. All work currently pending will be done first."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5872
msgid ""
"This function does NOT guarantee that non-pending work that has been "
"submitted with queue_delayed_work() and similar functions will be done "
"before destroying the workqueue. The fundamental problem is that, currently, "
"the workqueue has no way of accessing non-pending delayed_work. delayed_work "
"is only linked on the timer-side. All delayed_work must, therefore, be "
"canceled before calling this function."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5879
msgid ""
"TODO: It would be better if the problem described above wouldn't exist and "
"destroy_workqueue() would cleanly cancel all pending and non-pending "
"delayed_work."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5966
msgid "adjust max_active of a workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5968
msgid "new max_active value."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5969
msgid ""
"Set max_active of **wq** to **max_active**. See the alloc_workqueue() "
"function comment."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:5973
msgid "Don't call from IRQ context."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6000
msgid "adjust min_active of an unbound workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6001
msgid "target unbound workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6003
msgid "``int min_active``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6002
msgid "new min_active value"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6003
msgid ""
"Set min_active of an unbound workqueue. Unlike other types of workqueues, an "
"unbound workqueue is not guaranteed to be able to process max_active "
"interdependent work items. Instead, an unbound workqueue is guaranteed to be "
"able to process min_active number of interdependent work items which is "
"``WQ_DFL_MIN_ACTIVE`` by default."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6009
msgid ""
"Use this function to adjust the min_active value between 0 and the current "
"max_active."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6027
msgid "retrieve ``current`` task's work struct"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6028
msgid ""
"Determine if ``current`` task is a workqueue worker and what it's working "
"on. Useful to find out the context that the ``current`` task is running in."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6032
msgid ""
"work struct if ``current`` task is a workqueue worker, ``NULL`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6043
msgid "is ``current`` workqueue rescuer?"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6044
msgid ""
"Determine whether ``current`` is a workqueue rescuer.  Can be used from work "
"functions to determine whether it's being run off the rescuer task."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6048
msgid "``true`` if ``current`` is a workqueue rescuer. ``false`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6058
msgid "test whether a workqueue is congested"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6059
msgid "CPU in question"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6061
msgid ""
"Test whether **wq**'s cpu workqueue for **cpu** is congested.  There is no "
"synchronization around this function and the test result is unreliable and "
"only useful as advisory hints or for debugging."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6065
msgid "If **cpu** is WORK_CPU_UNBOUND, the test is performed on the local CPU."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6067
msgid ""
"With the exception of ordered workqueues, all workqueues have per-cpu "
"pool_workqueues, each with its own congested state. A workqueue being "
"congested on one CPU doesn't mean that the workqueue is contested on any "
"other CPUs."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6073
msgid "``true`` if congested, ``false`` otherwise."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6096
msgid "test whether a work is currently pending or running"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6097
msgid "the work to be tested"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6098
msgid ""
"Test whether **work** is currently pending or running.  There is no "
"synchronization around this function and the test result is unreliable and "
"only useful as advisory hints or for debugging."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6103
msgid "OR'd bitmask of WORK_BUSY_* bits."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6130
msgid "set description for the current work item"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6131
msgid "printf-style format string"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6132
msgid "arguments for the format string"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6133
msgid ""
"This function can be called by a running work function to describe what the "
"work item is about.  If the worker task gets dumped, this information will "
"be printed out together to help debugging.  The description can be at most "
"WORKER_DESC_LEN including the trailing '\\0'."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6153
msgid "print out worker information and description"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6159
msgid "``const char *log_lvl``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6154
msgid "the log level to use when printing"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6155
msgid "target task"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6156
msgid ""
"If **task** is a worker and currently executing a work item, print out the "
"name of the workqueue being serviced and worker description set with "
"set_worker_desc() by the currently executing work item."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6160
msgid ""
"This function can be safely called on any task as long as the task_struct "
"itself is accessible.  While safe, this function isn't synchronized and may "
"print out mixups or garbages of limited length."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6346
msgid "dump state of specified workqueue"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6347
msgid "workqueue whose state will be printed"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6390
msgid "dump state of specified worker pool"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6391
msgid "worker pool whose state will be printed"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6439
msgid "dump workqueue state"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6440
msgid ""
"Called from a sysrq handler and prints out all busy workqueues and pools."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6463
msgid "dump freezable workqueue state"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6464
msgid ""
"Called from try_to_freeze_tasks() and prints out all freezable workqueues "
"still busy."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6588
msgid "rebind all workers of a pool to the associated CPU"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6590
msgid "**pool->cpu** is coming online.  Rebind all workers to the CPU."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6644
msgid "restore cpumask of unbound workers"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6645
msgid "unbound pool of interest"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6646
msgid "the CPU which is coming up"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6647
msgid ""
"An unbound pool may end up with a cpumask which doesn't have any online "
"CPUs.  When a worker of such pool get scheduled, the scheduler resets its "
"cpus_allowed.  If **cpu** is in **pool**'s cpumask which didn't have any "
"online CPU before, cpus_allowed of all its workers should be restored."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6778
msgid "run a function in thread context on a particular cpu"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6779
msgid "the cpu to run on"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6781
msgid "``long (*fn)(void *)``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6780
msgid "the function to run"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6782
msgid "``void *arg``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6781
msgid "the function arg"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6783
msgid "``struct lock_class_key *key``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6782
msgid "The lock class key for lock debugging purposes"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6783
msgid ""
"It is up to the caller to ensure that the cpu doesn't go offline. The caller "
"must not hold any locks which would prevent **fn** from completing."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6787
msgid "The value **fn** returns."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6806
msgid "begin freezing workqueues"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6807
msgid ""
"Start freezing workqueues.  After this function returns, all freezable "
"workqueues will queue new works to their inactive_works list instead of pool-"
">worklist."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6812
#: kernel/workqueue.c:6885
msgid "Grabs and releases wq_pool_mutex, wq->mutex and pool->lock's."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6834
msgid "are freezable workqueues still busy?"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6835
msgid ""
"Check whether freezing is complete.  This function must be called between "
"freeze_workqueues_begin() and thaw_workqueues()."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6839
msgid "Grabs and releases wq_pool_mutex."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6842
msgid ""
"``true`` if some freezable workqueues are still busy.  ``false`` if freezing "
"is complete."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6880
msgid "thaw workqueues"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6881
msgid ""
"Thaw workqueues.  Normal queueing is restored and all collected frozen works "
"are transferred to their respective pool worklists."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6948
msgid "Exclude given CPUs from unbound cpumask"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6954
msgid "``cpumask_var_t exclude_cpumask``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6949
msgid "the cpumask to be excluded from wq_unbound_cpumask"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:6950
msgid ""
"This function can be called from cpuset code to provide a set of isolated "
"CPUs that should be excluded from wq_unbound_cpumask."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:7278
msgid "Set the low-level unbound cpumask"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:7284
msgid "``cpumask_var_t cpumask``"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:7279
msgid "the cpumask to set"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:7280
msgid ""
"The low-level workqueues cpumask is a global cpumask that limits the "
"affinity of all unbound workqueues.  This function check the **cpumask** and "
"apply it to all unbound workqueues and updates all pwqs of them."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:7285
msgid ""
"0       - Success -EINVAL - Invalid **cpumask** -ENOMEM - Failed to allocate "
"memory for attrs or pwqs."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:7383
msgid "make a workqueue visible in sysfs"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:7384
msgid "the workqueue to register"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:7385
msgid ""
"Expose **wq** in sysfs under /sys/bus/workqueue/devices. alloc_workqueue*() "
"automatically calls this function if WQ_SYSFS is set which is the preferred "
"method."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:7389
msgid ""
"Workqueue user should use this function directly iff it wants to apply "
"workqueue_attrs before making the workqueue visible in sysfs; otherwise, "
"apply_workqueue_attrs() may race against userland updating the attributes."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:7450
msgid "undo workqueue_sysfs_register()"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:7451
msgid "the workqueue to unregister"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:7452
msgid ""
"If **wq** is registered to sysfs by workqueue_sysfs_register(), unregister."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:7737
msgid "early init for workqueue subsystem"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:7738
msgid ""
"This is the first step of three-staged workqueue subsystem initialization "
"and invoked as soon as the bare basics - memory allocation, cpumasks and idr "
"are up. It sets up all the data structures and system workqueues and allows "
"early boot code to create workqueues and queue/cancel work items. Actual "
"work item execution starts only after kthreads can be created and scheduled "
"right before early initcalls."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:7894
msgid "bring workqueue subsystem fully online"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:7895
msgid ""
"This is the second step of three-staged workqueue subsystem initialization "
"and invoked as soon as kthreads can be created and scheduled. Workqueues "
"have been created and work items queued on them, but there are no kworkers "
"executing the work items yet. Populate the worker pools with the initial "
"workers and enable future kworker creations."
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:8018
msgid "initialize CPU pods for unbound workqueues"
msgstr ""

#: ../../../core-api/workqueue:789: kernel/workqueue.c:8019
msgid ""
"This is the third step of three-staged workqueue subsystem initialization "
"and invoked after SMP and topology information are fully initialized. It "
"initializes the unbound CPU pods accordingly."
msgstr ""
