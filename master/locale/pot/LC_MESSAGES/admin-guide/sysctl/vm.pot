# SOME DESCRIPTIVE TITLE.
# Copyright (C) The kernel development community
# This file is distributed under the same license as the The Linux Kernel package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: The Linux Kernel master\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-09-29 08:26+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: \n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../admin-guide/sysctl/vm.rst:3
msgid "Documentation for /proc/sys/vm/"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:5
msgid "kernel version 2.6.29"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:7
msgid "Copyright (c) 1998, 1999,  Rik van Riel <riel@nl.linux.org>"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:9
msgid "Copyright (c) 2008         Peter W. Morreale <pmorreale@novell.com>"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:11
msgid "For general info and legal blurb, please look in index.rst."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:15
msgid ""
"This file contains the documentation for the sysctl files in /proc/sys/vm "
"and is valid for Linux kernel version 2.6.29."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:18
msgid ""
"The files in this directory can be used to tune the operation of the virtual "
"memory (VM) subsystem of the Linux kernel and the writeout of dirty data to "
"disk."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:22
msgid ""
"Default values and initialization routines for most of these files can be "
"found in mm/swap.c."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:25
msgid "Currently, these files are in /proc/sys/vm:"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:27 ../../../admin-guide/sysctl/vm.rst:85
msgid "admin_reserve_kbytes"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:28 ../../../admin-guide/sysctl/vm.rst:114
msgid "compact_memory"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:29 ../../../admin-guide/sysctl/vm.rst:122
msgid "compaction_proactiveness"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:30 ../../../admin-guide/sysctl/vm.rst:145
msgid "compact_unevictable_allowed"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:31 ../../../admin-guide/sysctl/vm.rst:157
msgid "defrag_mode"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:32 ../../../admin-guide/sysctl/vm.rst:166
msgid "dirty_background_bytes"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:33 ../../../admin-guide/sysctl/vm.rst:179
msgid "dirty_background_ratio"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:34 ../../../admin-guide/sysctl/vm.rst:189
msgid "dirty_bytes"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:35 ../../../admin-guide/sysctl/vm.rst:205
msgid "dirty_expire_centisecs"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:36 ../../../admin-guide/sysctl/vm.rst:214
msgid "dirty_ratio"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:37 ../../../admin-guide/sysctl/vm.rst:224
msgid "dirtytime_expire_seconds"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:38 ../../../admin-guide/sysctl/vm.rst:236
msgid "dirty_writeback_centisecs"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:39 ../../../admin-guide/sysctl/vm.rst:246
msgid "drop_caches"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:40 ../../../admin-guide/sysctl/vm.rst:288
msgid "enable_soft_offline"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:41 ../../../admin-guide/sysctl/vm.rst:326
msgid "extfrag_threshold"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:42 ../../../admin-guide/sysctl/vm.rst:340
msgid "highmem_is_dirtyable"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:43 ../../../admin-guide/sysctl/vm.rst:360
msgid "hugetlb_shm_group"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:44 ../../../admin-guide/sysctl/vm.rst:367
msgid "laptop_mode"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:45 ../../../admin-guide/sysctl/vm.rst:374
msgid "legacy_va_layout"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:46 ../../../admin-guide/sysctl/vm.rst:381
msgid "lowmem_reserve_ratio"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:47 ../../../admin-guide/sysctl/vm.rst:469
msgid "max_map_count"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:48
msgid "mem_profiling         (only if CONFIG_MEM_ALLOC_PROFILING=y)"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:49 ../../../admin-guide/sysctl/vm.rst:499
msgid "memory_failure_early_kill"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:50 ../../../admin-guide/sysctl/vm.rst:527
msgid "memory_failure_recovery"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:51 ../../../admin-guide/sysctl/vm.rst:537
msgid "min_free_kbytes"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:52 ../../../admin-guide/sysctl/vm.rst:553
msgid "min_slab_ratio"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:53 ../../../admin-guide/sysctl/vm.rst:571
msgid "min_unmapped_ratio"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:54 ../../../admin-guide/sysctl/vm.rst:588
msgid "mmap_min_addr"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:55 ../../../admin-guide/sysctl/vm.rst:601
msgid "mmap_rnd_bits"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:56 ../../../admin-guide/sysctl/vm.rst:614
msgid "mmap_rnd_compat_bits"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:57 ../../../admin-guide/sysctl/vm.rst:628
msgid "nr_hugepages"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:58 ../../../admin-guide/sysctl/vm.rst:673
msgid "nr_hugepages_mempolicy"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:59 ../../../admin-guide/sysctl/vm.rst:682
msgid "nr_overcommit_hugepages"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:60
msgid "nr_trim_pages         (only if CONFIG_MMU=n)"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:61 ../../../admin-guide/sysctl/vm.rst:708
msgid "numa_zonelist_order"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:62 ../../../admin-guide/sysctl/vm.rst:757
msgid "oom_dump_tasks"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:63 ../../../admin-guide/sysctl/vm.rst:779
msgid "oom_kill_allocating_task"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:64 ../../../admin-guide/sysctl/vm.rst:800
msgid "overcommit_kbytes"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:65 ../../../admin-guide/sysctl/vm.rst:811
msgid "overcommit_memory"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:66 ../../../admin-guide/sysctl/vm.rst:836
msgid "overcommit_ratio"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:67 ../../../admin-guide/sysctl/vm.rst:844
msgid "page-cluster"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:68 ../../../admin-guide/sysctl/vm.rst:866
msgid "page_lock_unfairness"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:69 ../../../admin-guide/sysctl/vm.rst:874
msgid "panic_on_oom"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:70 ../../../admin-guide/sysctl/vm.rst:903
msgid "percpu_pagelist_high_fraction"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:71 ../../../admin-guide/sysctl/vm.rst:923
msgid "stat_interval"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:72 ../../../admin-guide/sysctl/vm.rst:930
msgid "stat_refresh"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:73 ../../../admin-guide/sysctl/vm.rst:943
msgid "numa_stat"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:74 ../../../admin-guide/sysctl/vm.rst:960
msgid "swappiness"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:75 ../../../admin-guide/sysctl/vm.rst:985
msgid "unprivileged_userfaultfd"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:76
#: ../../../admin-guide/sysctl/vm.rst:1004
msgid "user_reserve_kbytes"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:77
#: ../../../admin-guide/sysctl/vm.rst:1022
msgid "vfs_cache_pressure"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:78
#: ../../../admin-guide/sysctl/vm.rst:1045
msgid "vfs_cache_pressure_denom"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:79
#: ../../../admin-guide/sysctl/vm.rst:1051
msgid "watermark_boost_factor"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:80
#: ../../../admin-guide/sysctl/vm.rst:1071
msgid "watermark_scale_factor"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:81
#: ../../../admin-guide/sysctl/vm.rst:1089
msgid "zone_reclaim_mode"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:87
msgid ""
"The amount of free memory in the system that should be reserved for users "
"with the capability cap_sys_admin."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:90
msgid "admin_reserve_kbytes defaults to min(3% of free pages, 8MB)"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:92
msgid ""
"That should provide enough for the admin to log in and kill a process, if "
"necessary, under the default overcommit 'guess' mode."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:95
msgid ""
"Systems running under overcommit 'never' should increase this to account for "
"the full Virtual Memory Size of programs used to recover. Otherwise, root "
"may not be able to log in to recover the system."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:99
msgid "How do you calculate a minimum useful reserve?"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:101
msgid "sshd or login + bash (or some other shell) + top (or ps, kill, etc.)"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:103
msgid ""
"For overcommit 'guess', we can sum resident set sizes (RSS). On x86_64 this "
"is about 8MB."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:106
msgid ""
"For overcommit 'never', we can take the max of their virtual sizes (VSZ) and "
"add the sum of their RSS. On x86_64 this is about 128MB."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:110
#: ../../../admin-guide/sysctl/vm.rst:1018
msgid "Changing this takes effect whenever an application requests memory."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:116
msgid ""
"Available only when CONFIG_COMPACTION is set. When 1 is written to the file, "
"all zones are compacted such that free memory is available in contiguous "
"blocks where possible. This can be important for example in the allocation "
"of huge pages although processes will also directly compact memory as "
"required."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:124
msgid ""
"This tunable takes a value in the range [0, 100] with a default value of 20. "
"This tunable determines how aggressively compaction is done in the "
"background. Write of a non zero value to this tunable will immediately "
"trigger the proactive compaction. Setting it to 0 disables proactive "
"compaction."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:129
msgid ""
"Note that compaction has a non-trivial system-wide impact as pages belonging "
"to different processes are moved around, which could also lead to latency "
"spikes in unsuspecting applications. The kernel employs various heuristics "
"to avoid wasting CPU cycles if it detects that proactive compaction is not "
"being effective."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:135
msgid ""
"Setting the value above 80 will, in addition to lowering the acceptable "
"level of fragmentation, make the compaction code more sensitive to increases "
"in fragmentation, i.e. compaction will trigger more often, but reduce "
"fragmentation by a smaller amount. This makes the fragmentation level more "
"stable over time."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:141
msgid ""
"Be careful when setting it to extreme values like 100, as that may cause "
"excessive background compaction activity."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:147
msgid ""
"Available only when CONFIG_COMPACTION is set. When set to 1, compaction is "
"allowed to examine the unevictable lru (mlocked pages) for pages to compact. "
"This should be used on systems where stalls for minor page faults are an "
"acceptable trade for large contiguous free memory.  Set to 0 to prevent "
"compaction from moving pages that are unevictable.  Default value is 1. On "
"CONFIG_PREEMPT_RT the default value is 0 in order to avoid a page fault, due "
"to compaction, which would block the task from becoming active until the "
"fault is resolved."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:159
msgid ""
"When set to 1, the page allocator tries harder to avoid fragmentation and "
"maintain the ability to produce huge pages / higher-order pages."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:162
msgid ""
"It is recommended to enable this right after boot, as fragmentation, once it "
"occurred, can be long-lasting or even permanent."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:168
msgid ""
"Contains the amount of dirty memory at which the background kernel flusher "
"threads will start writeback."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:171
msgid "Note:"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:172
msgid ""
"dirty_background_bytes is the counterpart of dirty_background_ratio. Only "
"one of them may be specified at a time. When one sysctl is written it is "
"immediately taken into account to evaluate the dirty memory limits and the "
"other appears as 0 when read."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:181
msgid ""
"Contains, as a percentage of total available memory that contains free pages "
"and reclaimable pages, the number of pages at which the background kernel "
"flusher threads will start writing out dirty data."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:185
#: ../../../admin-guide/sysctl/vm.rst:220
msgid "The total available memory is not equal to total system memory."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:191
msgid ""
"Contains the amount of dirty memory at which a process generating disk "
"writes will itself start writeback."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:194
msgid ""
"Note: dirty_bytes is the counterpart of dirty_ratio. Only one of them may be "
"specified at a time. When one sysctl is written it is immediately taken into "
"account to evaluate the dirty memory limits and the other appears as 0 when "
"read."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:199
msgid ""
"Note: the minimum value allowed for dirty_bytes is two pages (in bytes); any "
"value lower than this limit will be ignored and the old configuration will "
"be retained."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:207
msgid ""
"This tunable is used to define when dirty data is old enough to be eligible "
"for writeout by the kernel flusher threads.  It is expressed in 100'ths of a "
"second.  Data which has been dirty in-memory for longer than this interval "
"will be written out next time a flusher thread wakes up."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:216
msgid ""
"Contains, as a percentage of total available memory that contains free pages "
"and reclaimable pages, the number of pages at which a process which is "
"generating disk writes will itself start writing out dirty data."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:226
msgid ""
"When a lazytime inode is constantly having its pages dirtied, the inode with "
"an updated timestamp will never get chance to be written out.  And, if the "
"only thing that has happened on the file system is a dirtytime inode caused "
"by an atime update, a worker will be scheduled to make sure that inode "
"eventually gets pushed out to disk.  This tunable is used to define when "
"dirty inode is old enough to be eligible for writeback by the kernel flusher "
"threads. And, it is also used as the interval to wakeup dirtytime_writeback "
"thread."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:238
msgid ""
"The kernel flusher threads will periodically wake up and write `old` data "
"out to disk.  This tunable expresses the interval between those wakeups, in "
"100'ths of a second."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:242
msgid "Setting this to zero disables periodic writeback altogether."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:248
msgid ""
"Writing to this will cause the kernel to drop clean caches, as well as "
"reclaimable slab objects like dentries and inodes.  Once dropped, their "
"memory becomes free."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:252
msgid "To free pagecache::"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:256
msgid "To free reclaimable slab objects (includes dentries and inodes)::"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:260
msgid "To free slab objects and pagecache::"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:264
msgid ""
"This is a non-destructive operation and will not free any dirty objects. To "
"increase the number of objects freed by this operation, the user may run "
"`sync` prior to writing to /proc/sys/vm/drop_caches.  This will minimize the "
"number of dirty objects on the system and create more candidates to be "
"dropped."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:270
msgid ""
"This file is not a means to control the growth of the various kernel caches "
"(inodes, dentries, pagecache, etc...)  These objects are automatically "
"reclaimed by the kernel when memory is needed elsewhere on the system."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:274
msgid ""
"Use of this file can cause performance problems.  Since it discards cached "
"objects, it may cost a significant amount of I/O and CPU to recreate the "
"dropped objects, especially if they were under heavy use.  Because of this, "
"use outside of a testing or debugging environment is not recommended."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:279
msgid ""
"You may see informational messages in your kernel log when this file is "
"used::"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:284
msgid ""
"These are informational only.  They do not mean that anything is wrong with "
"your system.  To disable them, echo 4 (bit 2) into drop_caches."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:289
msgid ""
"Correctable memory errors are very common on servers. Soft-offline is "
"kernel's solution for memory pages having (excessive) corrected memory "
"errors."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:292
msgid ""
"For different types of page, soft-offline has different behaviors / costs."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:294
msgid ""
"For a raw error page, soft-offline migrates the in-use page's content to a "
"new raw page."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:297
msgid ""
"For a page that is part of a transparent hugepage, soft-offline splits the "
"transparent hugepage into raw pages, then migrates only the raw error page. "
"As a result, user is transparently backed by 1 less hugepage, impacting "
"memory access performance."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:302
msgid ""
"For a page that is part of a HugeTLB hugepage, soft-offline first migrates "
"the entire HugeTLB hugepage, during which a free hugepage will be consumed "
"as migration target.  Then the original hugepage is dissolved into raw pages "
"without compensation, reducing the capacity of the HugeTLB pool by 1."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:307
msgid ""
"It is user's call to choose between reliability (staying away from fragile "
"physical memory) vs performance / capacity implications in transparent and "
"HugeTLB cases."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:311
msgid ""
"For all architectures, enable_soft_offline controls whether to soft offline "
"memory pages.  When set to 1, kernel attempts to soft offline the pages "
"whenever it thinks needed.  When set to 0, kernel returns EOPNOTSUPP to the "
"request to soft offline the pages.  Its default value is 1."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:316
msgid ""
"It is worth mentioning that after setting enable_soft_offline to 0, the "
"following requests to soft offline pages will not be performed:"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:319
msgid "Request to soft offline pages from RAS Correctable Errors Collector."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:321
msgid "On ARM, the request to soft offline pages from GHES driver."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:323
msgid ""
"On PARISC, the request to soft offline pages from Page Deallocation Table."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:328
msgid ""
"This parameter affects whether the kernel will compact memory or direct "
"reclaim to satisfy a high-order allocation. The extfrag/extfrag_index file "
"in debugfs shows what the fragmentation index for each order is in each zone "
"in the system. Values tending towards 0 imply allocations would fail due to "
"lack of memory, values towards 1000 imply failures are due to fragmentation "
"and -1 implies that the allocation will succeed as long as watermarks are "
"met."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:335
msgid ""
"The kernel will not compact memory in a zone if the fragmentation index is "
"<= extfrag_threshold. The default value is 500."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:342
msgid "Available only for systems with CONFIG_HIGHMEM enabled (32b systems)."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:344
msgid ""
"This parameter controls whether the high memory is considered for dirty "
"writers throttling.  This is not the case by default which means that only "
"the amount of memory directly visible/usable by the kernel can be dirtied. "
"As a result, on systems with a large amount of memory and lowmem basically "
"depleted writers might be throttled too early and streaming writes can get "
"very slow."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:351
msgid ""
"Changing the value to non zero would allow more memory to be dirtied and "
"thus allow writers to write more data which can be flushed to the storage "
"more effectively. Note this also comes with a risk of pre-mature OOM killer "
"because some writers (e.g. direct block device writes) can only use the low "
"memory and they can fill it up with dirty data without any throttling."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:362
msgid ""
"hugetlb_shm_group contains group id that is allowed to create SysV shared "
"memory segment using hugetlb page."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:369
msgid ""
"laptop_mode is a knob that controls \"laptop mode\". All the things that are "
"controlled by this knob are discussed in Documentation/admin-guide/laptops/"
"laptop-mode.rst."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:376
msgid ""
"If non-zero, this sysctl disables the new 32-bit mmap layout - the kernel "
"will use the legacy (2.4) layout for all processes."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:383
msgid ""
"For some specialised workloads on highmem machines it is dangerous for the "
"kernel to allow process memory to be allocated from the \"lowmem\" zone.  "
"This is because that memory could then be pinned via the mlock() system "
"call, or by unavailability of swapspace."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:388
msgid ""
"And on large highmem machines this lack of reclaimable lowmem memory can be "
"fatal."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:391
msgid ""
"So the Linux page allocator has a mechanism which prevents allocations which "
"*could* use highmem from using too much lowmem.  This means that a certain "
"amount of lowmem is defended from the possibility of being captured into "
"pinned user memory."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:396
msgid ""
"(The same argument applies to the old 16 megabyte ISA DMA region.  This "
"mechanism will also defend that region from allocations which could use "
"highmem or lowmem)."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:400
msgid ""
"The `lowmem_reserve_ratio` tunable determines how aggressive the kernel is "
"in defending these lower zones."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:403
msgid ""
"If you have a machine which uses highmem or ISA DMA and your applications "
"are using mlock(), or if you are running with no swap then you probably "
"should change the lowmem_reserve_ratio setting."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:407
msgid ""
"The lowmem_reserve_ratio is an array. You can see them by reading this file::"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:412
msgid ""
"But, these values are not used directly. The kernel calculates # of "
"protection pages for each zones from them. These are shown as array of "
"protection pages in /proc/zoneinfo like the following. (This is an example "
"of x86-64 box). Each zone has an array of protection pages like this::"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:431
msgid ""
"These protections are added to score to judge whether this zone should be "
"used for page allocation or should be reclaimed."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:434
msgid ""
"In this example, if normal pages (index=2) are required to this DMA zone and "
"watermark[WMARK_HIGH] is used for watermark, the kernel judges this zone "
"should not be used because pages_free(1355) is smaller than watermark + "
"protection[2] (4 + 2004 = 2008). If this protection value is 0, this zone "
"would be used for normal page requirement. If requirement is DMA "
"zone(index=0), protection[0] (=0) is used."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:441
msgid "zone[i]'s protection[j] is calculated by following expression::"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:452
msgid "The default values of lowmem_reserve_ratio[i] are"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:455
msgid "256"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:455
msgid "(if zone[i] means DMA or DMA32 zone)"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:456
msgid "32"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:456
msgid "(others)"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:459
msgid ""
"As above expression, they are reciprocal number of ratio. 256 means 1/256. # "
"of protection pages becomes about \"0.39%\" of total managed pages of higher "
"zones on the node."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:463
msgid ""
"If you would like to protect more pages, smaller values are effective. The "
"minimum value is 1 (1/1 -> 100%). The value less than 1 completely disables "
"protection of the pages."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:471
msgid ""
"This file contains the maximum number of memory map areas a process may "
"have. Memory map areas are used as a side-effect of calling malloc, directly "
"by mmap, mprotect, and madvise, and also when loading shared libraries."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:476
msgid ""
"While most applications need less than a thousand maps, certain programs, "
"particularly malloc debuggers, may consume lots of them, e.g., up to one or "
"two maps per allocation."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:480
msgid "The default value is 65530."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:484
msgid "mem_profiling"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:486
msgid "Enable memory profiling (when CONFIG_MEM_ALLOC_PROFILING=y)"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:488
msgid "1: Enable memory profiling."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:490
msgid "0: Disable memory profiling."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:492
msgid ""
"Enabling memory profiling introduces a small performance overhead for all "
"memory allocations."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:495
msgid ""
"The default value depends on CONFIG_MEM_ALLOC_PROFILING_ENABLED_BY_DEFAULT."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:501
msgid ""
"Control how to kill processes when uncorrected memory error (typically a "
"2bit error in a memory module) is detected in the background by hardware "
"that cannot be handled by the kernel. In some cases (like the page still "
"having a valid copy on disk) the kernel will handle the failure "
"transparently without affecting any applications. But if there is no other "
"up-to-date copy of the data it will kill to prevent any data corruptions "
"from propagating."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:509
msgid ""
"1: Kill all processes that have the corrupted and not reloadable page mapped "
"as soon as the corruption is detected.  Note this is not supported for a few "
"types of pages, like kernel internally allocated data or the swap cache, but "
"works for the majority of user pages."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:514
msgid ""
"0: Only unmap the corrupted page from all processes and only kill a process "
"who tries to access it."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:517
msgid ""
"The kill is done using a catchable SIGBUS with BUS_MCEERR_AO, so processes "
"can handle this if they want to."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:520
msgid ""
"This is only active on architectures/platforms with advanced machine check "
"handling and depends on the hardware capabilities."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:523
msgid ""
"Applications can override this setting individually with the PR_MCE_KILL "
"prctl"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:529
msgid "Enable memory failure recovery (when supported by the platform)"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:531
msgid "1: Attempt recovery."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:533
msgid "0: Always panic on a memory failure."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:539
msgid ""
"This is used to force the Linux VM to keep a minimum number of kilobytes "
"free.  The VM uses this number to compute a watermark[WMARK_MIN] value for "
"each lowmem zone in the system. Each lowmem zone gets a number of reserved "
"free pages based proportionally on its size."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:545
msgid ""
"Some minimal amount of memory is needed to satisfy PF_MEMALLOC allocations; "
"if you set this to lower than 1024KB, your system will become subtly broken, "
"and prone to deadlock under high loads."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:549
msgid "Setting this too high will OOM your machine instantly."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:555
#: ../../../admin-guide/sysctl/vm.rst:573
msgid "This is available only on NUMA kernels."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:557
msgid ""
"A percentage of the total pages in each zone.  On Zone reclaim (fallback "
"from the local zone occurs) slabs will be reclaimed if more than this "
"percentage of pages in a zone are reclaimable slab pages. This insures that "
"the slab growth stays under control even in NUMA systems that rarely perform "
"global reclaim."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:563
msgid "The default is 5 percent."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:565
msgid ""
"Note that slab reclaim is triggered in a per zone / node fashion. The "
"process of reclaiming slab memory is currently not node specific and may not "
"be fast."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:575
msgid ""
"This is a percentage of the total pages in each zone. Zone reclaim will only "
"occur if more than this percentage of pages are in a state that "
"zone_reclaim_mode allows to be reclaimed."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:579
msgid ""
"If zone_reclaim_mode has the value 4 OR'd, then the percentage is compared "
"against all file-backed unmapped pages including swapcache pages and tmpfs "
"files. Otherwise, only unmapped pages backed by normal files but not tmpfs "
"files and similar are considered."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:584
msgid "The default is 1 percent."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:590
msgid ""
"This file indicates the amount of address space  which a user process will "
"be restricted from mmapping.  Since kernel null dereference bugs could "
"accidentally operate based on the information in the first couple of pages "
"of memory userspace processes should not be allowed to write to them.  By "
"default this value is set to 0 and no protections will be enforced by the "
"security module.  Setting this value to something like 64k will allow the "
"vast majority of applications to work correctly and provide defense in depth "
"against future potential kernel bugs."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:603
msgid ""
"This value can be used to select the number of bits to use to determine the "
"random offset to the base address of vma regions resulting from mmap "
"allocations on architectures which support tuning address space "
"randomization.  This value will be bounded by the architecture's minimum and "
"maximum supported values."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:609
msgid ""
"This value can be changed after boot using the /proc/sys/vm/mmap_rnd_bits "
"tunable"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:616
msgid ""
"This value can be used to select the number of bits to use to determine the "
"random offset to the base address of vma regions resulting from mmap "
"allocations for applications run in compatibility mode on architectures "
"which support tuning address space randomization.  This value will be "
"bounded by the architecture's minimum and maximum supported values."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:623
msgid ""
"This value can be changed after boot using the /proc/sys/vm/"
"mmap_rnd_compat_bits tunable"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:630
msgid "Change the minimum size of the hugepage pool."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:632
#: ../../../admin-guide/sysctl/vm.rst:678
#: ../../../admin-guide/sysctl/vm.rst:687
msgid "See Documentation/admin-guide/mm/hugetlbpage.rst"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:636
msgid "hugetlb_optimize_vmemmap"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:638
msgid ""
"This knob is not available when the size of 'struct page' (a structure "
"defined in include/linux/mm_types.h) is not power of two (an unusual system "
"config could result in this)."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:642
msgid ""
"Enable (set to 1) or disable (set to 0) HugeTLB Vmemmap Optimization (HVO)."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:644
msgid ""
"Once enabled, the vmemmap pages of subsequent allocation of HugeTLB pages "
"from buddy allocator will be optimized (7 pages per 2MB HugeTLB page and "
"4095 pages per 1GB HugeTLB page), whereas already allocated HugeTLB pages "
"will not be optimized.  When those optimized HugeTLB pages are freed from "
"the HugeTLB pool to the buddy allocator, the vmemmap pages representing that "
"range needs to be remapped again and the vmemmap pages discarded earlier "
"need to be rellocated again.  If your use case is that HugeTLB pages are "
"allocated 'on the fly' (e.g. never explicitly allocating HugeTLB pages with "
"'nr_hugepages' but only set 'nr_overcommit_hugepages', those overcommitted "
"HugeTLB pages are allocated 'on the fly') instead of being pulled from the "
"HugeTLB pool, you should weigh the benefits of memory savings against the "
"more overhead (~2x slower than before) of allocation or freeing HugeTLB "
"pages between the HugeTLB pool and the buddy allocator.  Another behavior to "
"note is that if the system is under heavy memory pressure, it could prevent "
"the user from freeing HugeTLB pages from the HugeTLB pool to the buddy "
"allocator since the allocation of vmemmap pages could be failed, you have to "
"retry later if your system encounter this situation."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:661
msgid ""
"Once disabled, the vmemmap pages of subsequent allocation of HugeTLB pages "
"from buddy allocator will not be optimized meaning the extra overhead at "
"allocation time from buddy allocator disappears, whereas already optimized "
"HugeTLB pages will not be affected.  If you want to make sure there are no "
"optimized HugeTLB pages, you can set \"nr_hugepages\" to 0 first and then "
"disable this.  Note that writing 0 to nr_hugepages will make any \"in use\" "
"HugeTLB pages become surplus pages.  So, those surplus pages are still "
"optimized until they are no longer in use.  You would need to wait for those "
"surplus pages to be released before there are no optimized pages in the "
"system."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:675
msgid ""
"Change the size of the hugepage pool at run-time on a specific set of NUMA "
"nodes."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:684
msgid ""
"Change the maximum size of the hugepage pool. The maximum is nr_hugepages + "
"nr_overcommit_hugepages."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:691
msgid "nr_trim_pages"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:693
msgid "This is available only on NOMMU kernels."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:695
msgid ""
"This value adjusts the excess page trimming behaviour of power-of-2 aligned "
"NOMMU mmap allocations."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:698
msgid ""
"A value of 0 disables trimming of allocations entirely, while a value of 1 "
"trims excess pages aggressively. Any value >= 1 acts as the watermark where "
"trimming of allocations is initiated."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:702
msgid "The default value is 1."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:704
msgid "See Documentation/admin-guide/mm/nommu-mmap.rst for more information."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:710
msgid ""
"This sysctl is only for NUMA and it is deprecated. Anything but Node order "
"will fail!"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:713
msgid "'where the memory is allocated from' is controlled by zonelists."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:715
msgid ""
"(This documentation ignores ZONE_HIGHMEM/ZONE_DMA32 for simple explanation. "
"you may be able to read ZONE_DMA as ZONE_DMA32...)"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:718
msgid ""
"In non-NUMA case, a zonelist for GFP_KERNEL is ordered as following. "
"ZONE_NORMAL -> ZONE_DMA This means that a memory allocation request for "
"GFP_KERNEL will get memory from ZONE_DMA only when ZONE_NORMAL is not "
"available."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:723
msgid ""
"In NUMA case, you can think of following 2 types of order. Assume 2 node "
"NUMA and below is zonelist of Node(0)'s GFP_KERNEL::"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:729
msgid ""
"Type(A) offers the best locality for processes on Node(0), but ZONE_DMA will "
"be used before ZONE_NORMAL exhaustion. This increases possibility of out-of-"
"memory(OOM) of ZONE_DMA because ZONE_DMA is tend to be small."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:733
msgid ""
"Type(B) cannot offer the best locality but is more robust against OOM of the "
"DMA zone."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:736
msgid "Type(A) is called as \"Node\" order. Type (B) is \"Zone\" order."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:738
msgid ""
"\"Node order\" orders the zonelists by node, then by zone within each node. "
"Specify \"[Nn]ode\" for node order"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:741
msgid ""
"\"Zone Order\" orders the zonelists by zone type, then by node within each "
"zone.  Specify \"[Zz]one\" for zone order."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:744
msgid "Specify \"[Dd]efault\" to request automatic configuration."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:746
msgid ""
"On 32-bit, the Normal zone needs to be preserved for allocations accessible "
"by the kernel, so \"zone\" order will be selected."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:749
msgid ""
"On 64-bit, devices that require DMA32/DMA are relatively rare, so \"node\" "
"order will be selected."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:752
msgid ""
"Default order is recommended unless this is causing problems for your system/"
"application."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:759
msgid ""
"Enables a system-wide task dump (excluding kernel threads) to be produced "
"when the kernel performs an OOM-killing and includes such information as "
"pid, uid, tgid, vm size, rss, pgtables_bytes, swapents, oom_score_adj score, "
"and name.  This is helpful to determine why the OOM killer was invoked, to "
"identify the rogue task that caused it, and to determine why the OOM killer "
"chose the task it did to kill."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:766
msgid ""
"If this is set to zero, this information is suppressed.  On very large "
"systems with thousands of tasks it may not be feasible to dump the memory "
"state information for each one.  Such systems should not be forced to incur "
"a performance penalty in OOM conditions when the information may not be "
"desired."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:772
msgid ""
"If this is set to non-zero, this information is shown whenever the OOM "
"killer actually kills a memory-hogging task."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:775
msgid "The default value is 1 (enabled)."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:781
msgid ""
"This enables or disables killing the OOM-triggering task in out-of-memory "
"situations."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:784
msgid ""
"If this is set to zero, the OOM killer will scan through the entire tasklist "
"and select a task based on heuristics to kill.  This normally selects a "
"rogue memory-hogging task that frees up a large amount of memory when killed."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:789
msgid ""
"If this is set to non-zero, the OOM killer simply kills the task that "
"triggered the out-of-memory condition.  This avoids the expensive tasklist "
"scan."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:793
msgid ""
"If panic_on_oom is selected, it takes precedence over whatever value is used "
"in oom_kill_allocating_task."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:796
#: ../../../admin-guide/sysctl/vm.rst:829
#: ../../../admin-guide/sysctl/vm.rst:893
#: ../../../admin-guide/sysctl/vm.rst:997
msgid "The default value is 0."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:802
msgid ""
"When overcommit_memory is set to 2, the committed address space is not "
"permitted to exceed swap plus this amount of physical RAM. See below."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:805
msgid ""
"Note: overcommit_kbytes is the counterpart of overcommit_ratio. Only one of "
"them may be specified at a time. Setting one disables the other (which then "
"appears as 0 when read)."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:813
msgid "This value contains a flag that enables memory overcommitment."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:815
msgid ""
"When this flag is 0, the kernel compares the userspace memory request size "
"against total memory plus swap and rejects obvious overcommits."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:818
msgid ""
"When this flag is 1, the kernel pretends there is always enough memory until "
"it actually runs out."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:821
msgid ""
"When this flag is 2, the kernel uses a \"never overcommit\" policy that "
"attempts to prevent any overcommit of memory. Note that user_reserve_kbytes "
"affects this policy."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:825
msgid ""
"This feature can be very useful because there are a lot of programs that "
"malloc() huge amounts of memory \"just-in-case\" and don't use much of it."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:831
msgid ""
"See Documentation/mm/overcommit-accounting.rst and mm/util.c::"
"__vm_enough_memory() for more information."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:838
msgid ""
"When overcommit_memory is set to 2, the committed address space is not "
"permitted to exceed swap plus this percentage of physical RAM.  See above."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:846
msgid ""
"page-cluster controls the number of pages up to which consecutive pages are "
"read in from swap in a single attempt. This is the swap counterpart to page "
"cache readahead. The mentioned consecutivity is not in terms of virtual/"
"physical addresses, but consecutive on swap space - that means they were "
"swapped out together."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:852
msgid ""
"It is a logarithmic value - setting it to zero means \"1 page\", setting it "
"to 1 means \"2 pages\", setting it to 2 means \"4 pages\", etc. Zero "
"disables swap readahead completely."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:856
msgid ""
"The default value is three (eight pages at a time).  There may be some small "
"benefits in tuning this to a different value if your workload is swap-"
"intensive."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:860
msgid ""
"Lower values mean lower latencies for initial faults, but at the same time "
"extra faults and I/O delays for following faults if they would have been "
"part of that consecutive pages readahead would have brought in."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:868
msgid ""
"This value determines the number of times that the page lock can be stolen "
"from under a waiter. After the lock is stolen the number of times specified "
"in this file (default is 5), the \"fair lock handoff\" semantics will apply, "
"and the waiter will only be awakened if the lock can be taken."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:876
msgid "This enables or disables panic on out-of-memory feature."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:878
msgid ""
"If this is set to 0, the kernel will kill some rogue process, called "
"oom_killer.  Usually, oom_killer can kill rogue processes and system will "
"survive."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:882
msgid ""
"If this is set to 1, the kernel panics when out-of-memory happens. However, "
"if a process limits using nodes by mempolicy/cpusets, and those nodes become "
"memory exhaustion status, one process may be killed by oom-killer. No panic "
"occurs in this case. Because other nodes' memory may be free. This means "
"system total status may be not fatal yet."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:889
msgid ""
"If this is set to 2, the kernel panics compulsorily even on the above-"
"mentioned. Even oom happens under memory cgroup, the whole system panics."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:895
msgid ""
"1 and 2 are for failover of clustering. Please select either according to "
"your policy of failover."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:898
msgid ""
"panic_on_oom=2+kdump gives you very strong tool to investigate why oom "
"happens. You can get snapshot."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:905
msgid ""
"This is the fraction of pages in each zone that are can be stored to per-cpu "
"page lists. It is an upper boundary that is divided depending on the number "
"of online CPUs. The min value for this is 8 which means that we do not allow "
"more than 1/8th of pages in each zone to be stored on per-cpu page lists. "
"This entry only changes the value of hot per-cpu page lists. A user can "
"specify a number like 100 to allocate 1/100th of each zone between per-cpu "
"lists."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:913
msgid ""
"The batch value of each per-cpu page list remains the same regardless of the "
"value of the high fraction so allocation latencies are unaffected."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:916
msgid ""
"The initial value is zero. Kernel uses this value to set the high pcp->high "
"mark based on the low watermark for the zone and the number of local online "
"CPUs.  If the user writes '0' to this sysctl, it will revert to this default "
"behavior."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:925
msgid ""
"The time interval between which vm statistics are updated.  The default is 1 "
"second."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:932
msgid ""
"Any read or write (by root only) flushes all the per-cpu vm statistics into "
"their global totals, for more accurate reports when testing e.g. cat /proc/"
"sys/vm/stat_refresh /proc/meminfo"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:936
msgid ""
"As a side-effect, it also checks for negative totals (elsewhere reported as "
"0) and \"fails\" with EINVAL if any are found, with a warning in dmesg. (At "
"time of writing, a few stats are known sometimes to be found negative, with "
"no ill effects: errors and warnings on these stats are suppressed.)"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:945
msgid "This interface allows runtime configuration of numa statistics."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:947
msgid ""
"When page allocation performance becomes a bottleneck and you can tolerate "
"some possible tool breakage and decreased numa counter precision, you can "
"do::"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:953
msgid ""
"When page allocation performance is not a bottleneck and you want all "
"tooling to work, you can do::"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:962
msgid ""
"This control is used to define the rough relative IO cost of swapping and "
"filesystem paging, as a value between 0 and 200. At 100, the VM assumes "
"equal IO cost and will thus apply memory pressure to the page cache and swap-"
"backed pages equally; lower values signify more expensive swap IO, higher "
"values indicates cheaper."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:968
msgid ""
"Keep in mind that filesystem IO patterns under memory pressure tend to be "
"more efficient than swap's random IO. An optimal value will require "
"experimentation and will also be workload-dependent."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:972
msgid "The default value is 60."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:974
msgid ""
"For in-memory swap, like zram or zswap, as well as hybrid setups that have "
"swap on faster devices than the filesystem, values beyond 100 can be "
"considered. For example, if the random IO against the swap device is on "
"average 2x faster than IO from the filesystem, swappiness should be 133 (x + "
"2x = 200, 2x = 133.33)."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:980
msgid ""
"At 0, the kernel will not initiate swap until the amount of free and file-"
"backed pages is less than the high watermark in a zone."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:987
msgid ""
"This flag controls the mode in which unprivileged users can use the "
"userfaultfd system calls. Set this to 0 to restrict unprivileged users to "
"handle page faults in user mode only. In this case, users without "
"SYS_CAP_PTRACE must pass UFFD_USER_MODE_ONLY in order for userfaultfd to "
"succeed. Prohibiting use of userfaultfd for handling faults from kernel mode "
"may make certain vulnerabilities more difficult to exploit."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:994
msgid ""
"Set this to 1 to allow unprivileged users to use the userfaultfd system "
"calls without any restrictions."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:999
msgid ""
"Another way to control permissions for userfaultfd is to use /dev/"
"userfaultfd instead of userfaultfd(2). See Documentation/admin-guide/mm/"
"userfaultfd.rst."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:1006
msgid ""
"When overcommit_memory is set to 2, \"never overcommit\" mode, reserve "
"min(3% of current process size, user_reserve_kbytes) of free memory. This is "
"intended to prevent a user from starting a single memory hogging process, "
"such that they cannot recover (kill the hog)."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:1011
msgid ""
"user_reserve_kbytes defaults to min(3% of the current process size, 128MB)."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:1013
msgid ""
"If this is reduced to zero, then the user will be allowed to allocate all "
"free memory with a single process, minus admin_reserve_kbytes. Any "
"subsequent attempts to execute a command will result in \"fork: Cannot "
"allocate memory\"."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:1024
msgid ""
"This percentage value controls the tendency of the kernel to reclaim the "
"memory which is used for caching of directory and inode objects."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:1027
msgid ""
"At the default value of vfs_cache_pressure=vfs_cache_pressure_denom the "
"kernel will attempt to reclaim dentries and inodes at a \"fair\" rate with "
"respect to pagecache and swapcache reclaim.  Decreasing vfs_cache_pressure "
"causes the kernel to prefer to retain dentry and inode caches. When "
"vfs_cache_pressure=0, the kernel will never reclaim dentries and inodes due "
"to memory pressure and this can easily lead to out-of-memory conditions. "
"Increasing vfs_cache_pressure beyond vfs_cache_pressure_denom causes the "
"kernel to prefer to reclaim dentries and inodes."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:1036
msgid ""
"Increasing vfs_cache_pressure significantly beyond vfs_cache_pressure_denom "
"may have negative performance impact. Reclaim code needs to take various "
"locks to find freeable directory and inode objects. When vfs_cache_pressure "
"equals (10 * vfs_cache_pressure_denom), it will look for ten times more "
"freeable objects than there are."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:1042
msgid ""
"Note: This setting should always be used together with "
"vfs_cache_pressure_denom."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:1047
msgid ""
"Defaults to 100 (minimum allowed value). Requires corresponding "
"vfs_cache_pressure setting to take effect."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:1053
msgid ""
"This factor controls the level of reclaim when memory is being fragmented. "
"It defines the percentage of the high watermark of a zone that will be "
"reclaimed if pages of different mobility are being mixed within pageblocks. "
"The intent is that compaction has less work to do in the future and to "
"increase the success rate of future high-order allocations such as SLUB "
"allocations, THP and hugetlbfs pages."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:1060
msgid ""
"To make it sensible with respect to the watermark_scale_factor parameter, "
"the unit is in fractions of 10,000. The default value of 15,000 means that "
"up to 150% of the high watermark will be reclaimed in the event of a "
"pageblock being mixed due to fragmentation. The level of reclaim is "
"determined by the number of fragmentation events that occurred in the recent "
"past. If this value is smaller than a pageblock then a pageblocks worth of "
"pages will be reclaimed (e.g.  2MB on 64-bit x86). A boost factor of 0 will "
"disable the feature."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:1073
msgid ""
"This factor controls the aggressiveness of kswapd. It defines the amount of "
"memory left in a node/system before kswapd is woken up and how much memory "
"needs to be free before kswapd goes back to sleep."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:1077
msgid ""
"The unit is in fractions of 10,000. The default value of 10 means the "
"distances between watermarks are 0.1% of the available memory in the node/"
"system. The maximum value is 3000, or 30% of memory."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:1081
msgid ""
"A high rate of threads entering direct reclaim (allocstall) or kswapd going "
"to sleep prematurely (kswapd_low_wmark_hit_quickly) can indicate that the "
"number of free pages kswapd maintains for latency reasons is too small for "
"the allocation bursts occurring in the system. This knob can then be used to "
"tune kswapd aggressiveness accordingly."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:1091
msgid ""
"Zone_reclaim_mode allows someone to set more or less aggressive approaches "
"to reclaim memory when a zone runs out of memory. If it is set to zero then "
"no zone reclaim occurs. Allocations will be satisfied from other zones / "
"nodes in the system."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:1096
msgid "This is value OR'ed together of"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:1099
msgid "1"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:1099
msgid "Zone reclaim on"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:1100
msgid "2"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:1100
msgid "Zone reclaim writes dirty pages out"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:1101
msgid "4"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:1101
msgid "Zone reclaim swaps pages"
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:1104
msgid ""
"zone_reclaim_mode is disabled by default.  For file servers or workloads "
"that benefit from having their data cached, zone_reclaim_mode should be left "
"disabled as the caching effect is likely to be more important than data "
"locality."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:1109
msgid ""
"Consider enabling one or more zone_reclaim mode bits if it's known that the "
"workload is partitioned such that each partition fits within a NUMA node and "
"that accessing remote memory would cause a measurable performance "
"reduction.  The page allocator will take additional actions before "
"allocating off node pages."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:1115
msgid ""
"Allowing zone reclaim to write out pages stops processes that are writing "
"large amounts of data from dirtying pages on other nodes. Zone reclaim will "
"write out dirty pages if a zone fills up and so effectively throttle the "
"process. This may decrease the performance of a single process since it "
"cannot use all of system memory to buffer the outgoing writes anymore but it "
"preserve the memory on other nodes so that the performance of other "
"processes running on other nodes will not be affected."
msgstr ""

#: ../../../admin-guide/sysctl/vm.rst:1123
msgid ""
"Allowing regular swap effectively restricts allocations to the local node "
"unless explicitly overridden by memory policies or cpuset configurations."
msgstr ""
