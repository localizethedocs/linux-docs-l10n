# SOME DESCRIPTIVE TITLE.
# Copyright (C) The kernel development community
# This file is distributed under the same license as the The Linux Kernel package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: The Linux Kernel master\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-09-29 08:26+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../networking/device_drivers/ethernet/microsoft/netvsc.rst:5
msgid "Hyper-V network driver"
msgstr ""

#: ../../../networking/device_drivers/ethernet/microsoft/netvsc.rst:8
msgid "Compatibility"
msgstr ""

#: ../../../networking/device_drivers/ethernet/microsoft/netvsc.rst:10
msgid ""
"This driver is compatible with Windows Server 2012 R2, 2016 and Windows 10."
msgstr ""

#: ../../../networking/device_drivers/ethernet/microsoft/netvsc.rst:14
msgid "Features"
msgstr ""

#: ../../../networking/device_drivers/ethernet/microsoft/netvsc.rst:17
msgid "Checksum offload"
msgstr ""

#: ../../../networking/device_drivers/ethernet/microsoft/netvsc.rst:18
msgid ""
"The netvsc driver supports checksum offload as long as the Hyper-V host "
"version does. Windows Server 2016 and Azure support checksum offload for TCP "
"and UDP for both IPv4 and IPv6. Windows Server 2012 only supports checksum "
"offload for TCP."
msgstr ""

#: ../../../networking/device_drivers/ethernet/microsoft/netvsc.rst:24
msgid "Receive Side Scaling"
msgstr ""

#: ../../../networking/device_drivers/ethernet/microsoft/netvsc.rst:25
msgid ""
"Hyper-V supports receive side scaling. For TCP & UDP, packets can be "
"distributed among available queues based on IP address and port number."
msgstr ""

#: ../../../networking/device_drivers/ethernet/microsoft/netvsc.rst:29
msgid ""
"For TCP & UDP, we can switch hash level between L3 and L4 by ethtool "
"command. TCP/UDP over IPv4 and v6 can be set differently. The default hash "
"level is L4. We currently only allow switching TX hash level from within the "
"guests."
msgstr ""

#: ../../../networking/device_drivers/ethernet/microsoft/netvsc.rst:34
msgid ""
"On Azure, fragmented UDP packets have high loss rate with L4 hashing. Using "
"L3 hashing is recommended in this case."
msgstr ""

#: ../../../networking/device_drivers/ethernet/microsoft/netvsc.rst:37
msgid "For example, for UDP over IPv4 on eth0:"
msgstr ""

#: ../../../networking/device_drivers/ethernet/microsoft/netvsc.rst:39
msgid "To include UDP port numbers in hashing::"
msgstr ""

#: ../../../networking/device_drivers/ethernet/microsoft/netvsc.rst:43
msgid "To exclude UDP port numbers in hashing::"
msgstr ""

#: ../../../networking/device_drivers/ethernet/microsoft/netvsc.rst:47
msgid "To show UDP hash level::"
msgstr ""

#: ../../../networking/device_drivers/ethernet/microsoft/netvsc.rst:52
msgid "Generic Receive Offload, aka GRO"
msgstr ""

#: ../../../networking/device_drivers/ethernet/microsoft/netvsc.rst:53
msgid ""
"The driver supports GRO and it is enabled by default. GRO coalesces like "
"packets and significantly reduces CPU usage under heavy Rx load."
msgstr ""

#: ../../../networking/device_drivers/ethernet/microsoft/netvsc.rst:58
msgid "Large Receive Offload (LRO), or Receive Side Coalescing (RSC)"
msgstr ""

#: ../../../networking/device_drivers/ethernet/microsoft/netvsc.rst:59
msgid ""
"The driver supports LRO/RSC in the vSwitch feature. It reduces the per "
"packet processing overhead by coalescing multiple TCP segments when "
"possible. The feature is enabled by default on VMs running on Windows Server "
"2019 and later. It may be changed by ethtool command::"
msgstr ""

#: ../../../networking/device_drivers/ethernet/microsoft/netvsc.rst:68
msgid "SR-IOV support"
msgstr ""

#: ../../../networking/device_drivers/ethernet/microsoft/netvsc.rst:69
msgid ""
"Hyper-V supports SR-IOV as a hardware acceleration option. If SR-IOV is "
"enabled in both the vSwitch and the guest configuration, then the Virtual "
"Function (VF) device is passed to the guest as a PCI device. In this case, "
"both a synthetic (netvsc) and VF device are visible in the guest OS and both "
"NIC's have the same MAC address."
msgstr ""

#: ../../../networking/device_drivers/ethernet/microsoft/netvsc.rst:75
msgid ""
"The VF is enslaved by netvsc device.  The netvsc driver will transparently "
"switch the data path to the VF when it is available and up. Network state "
"(addresses, firewall, etc) should be applied only to the netvsc device; the "
"slave device should not be accessed directly in most cases.  The exceptions "
"are if some special queue discipline or flow direction is desired, these "
"should be applied directly to the VF slave device."
msgstr ""

#: ../../../networking/device_drivers/ethernet/microsoft/netvsc.rst:84
msgid "Receive Buffer"
msgstr ""

#: ../../../networking/device_drivers/ethernet/microsoft/netvsc.rst:85
msgid ""
"Packets are received into a receive area which is created when device is "
"probed. The receive area is broken into MTU sized chunks and each may "
"contain one or more packets. The number of receive sections may be changed "
"via ethtool Rx ring parameters."
msgstr ""

#: ../../../networking/device_drivers/ethernet/microsoft/netvsc.rst:90
msgid ""
"There is a similar send buffer which is used to aggregate packets for "
"sending.  The send area is broken into chunks, typically of 6144 bytes, each "
"of section may contain one or more packets. Small packets are usually "
"transmitted via copy to the send buffer. However, if the buffer is "
"temporarily exhausted, or the packet to be transmitted is an LSO packet, the "
"driver will provide the host with pointers to the data from the SKB. This "
"attempts to achieve a balance between the overhead of data copy and the "
"impact of remapping VM memory to be accessible by the host."
msgstr ""

#: ../../../networking/device_drivers/ethernet/microsoft/netvsc.rst:101
msgid "XDP support"
msgstr ""

#: ../../../networking/device_drivers/ethernet/microsoft/netvsc.rst:102
msgid ""
"XDP (eXpress Data Path) is a feature that runs eBPF bytecode at the early "
"stage when packets arrive at a NIC card. The goal is to increase performance "
"for packet processing, reducing the overhead of SKB allocation and other "
"upper network layers."
msgstr ""

#: ../../../networking/device_drivers/ethernet/microsoft/netvsc.rst:107
msgid ""
"hv_netvsc supports XDP in native mode, and transparently sets the XDP "
"program on the associated VF NIC as well."
msgstr ""

#: ../../../networking/device_drivers/ethernet/microsoft/netvsc.rst:110
msgid ""
"Setting / unsetting XDP program on synthetic NIC (netvsc) propagates to VF "
"NIC automatically. Setting / unsetting XDP program on VF NIC directly is not "
"recommended, also not propagated to synthetic NIC, and may be overwritten by "
"setting of synthetic NIC."
msgstr ""

#: ../../../networking/device_drivers/ethernet/microsoft/netvsc.rst:115
msgid ""
"XDP program cannot run with LRO (RSC) enabled, so you need to disable LRO "
"before running XDP::"
msgstr ""

#: ../../../networking/device_drivers/ethernet/microsoft/netvsc.rst:120
msgid "XDP_REDIRECT action is not yet supported."
msgstr ""
